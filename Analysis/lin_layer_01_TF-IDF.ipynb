{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('dbpedia.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 560000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 70000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data_title = train_dataset['title']\n",
    "train_data_content = train_dataset['content']\n",
    "train_labels = train_dataset['label']\n",
    "test_data_title = test_dataset['title']\n",
    "test_data_content = test_dataset['content']\n",
    "test_labels = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_title[0])\n",
    "# print(train_data_content[0])\n",
    "# print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data_title + train_data_content\n",
    "# test_data = test_data_title + test_data_content\n",
    "\n",
    "train_data = [None] * len(train_data_title)\n",
    "for i in range(len(train_data_title)):\n",
    "    train_data[i] = train_data_title[i] + train_data_content[i]\n",
    "\n",
    "test_data = [None] * len(test_data_title)\n",
    "for i in range(len(test_data_title)):\n",
    "    test_data[i] = test_data_title[i] + test_data_content[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def custom_contractions_fix(text):\n",
    "#     # Define custom contractions to expand\n",
    "#     contractions_dict = {\n",
    "#         \"don't\": \"do not\",\n",
    "#         \"doesn't\": \"does not\",\n",
    "#         \"didn't\": \"did not\",\n",
    "#         # Add more contractions as needed\n",
    "#     }\n",
    "    \n",
    "#     # Use a regular expression to find and replace contractions\n",
    "#     contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "#     def replace(match):\n",
    "#         return contractions_dict[match.group(0)]\n",
    "    \n",
    "#     expanded_text = contractions_re.sub(replace, text)\n",
    "#     return expanded_text\n",
    "\n",
    "\n",
    "# def tokenize_text(text):\n",
    "    \n",
    "#     # if there is a word between () then write it twice\n",
    "#     text = re.sub(r'\\((.*?)\\)', r'\\1 \\1', text)\n",
    "    \n",
    "#     # Apply custom contractions expansion\n",
    "#     text = custom_contractions_fix(text)\n",
    "    \n",
    "#     # Replace hyphens with spaces\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     temp = [word for word in tokens if len(word) > 1 and word.isupper()]\n",
    "#     tokens.extend(temp)\n",
    "    \n",
    "#     # Convert to lowercase\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "#     # Remove periods\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # Remove punctuation and stopwords\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(tokenize_text(train_data[10]))\n",
    "\n",
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('dbpedia_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('dbpedia_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('dbpedia_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('dbpedia_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('dbpedia_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('dbpedia_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('dbpedia_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('dbpedia_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36315\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the order of the training data and the testing data along with their labels with a specific seed\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "train_data = []\n",
    "new_train_labels = []\n",
    "\n",
    "test_data = []\n",
    "new_test_labels = []\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    train_data.append(train_tokens[i])\n",
    "    new_train_labels.append(train_labels[i])\n",
    "\n",
    "for i in range(len(test_tokens)):\n",
    "    test_data.append(test_tokens[i])\n",
    "    new_test_labels.append(test_labels[i])\n",
    "\n",
    "# shuffle the data\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(train_data, new_train_labels))\n",
    "random.shuffle(temp)\n",
    "train_data, train_labels = zip(*temp)\n",
    "\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(test_data, new_test_labels))\n",
    "random.shuffle(temp)\n",
    "test_data, test_labels = zip(*temp)\n",
    "\n",
    "# convert them back to the format of train_tokens and test_tokens\n",
    "train_tokens = list(train_data)\n",
    "train_labels = list(train_labels)\n",
    "test_tokens = list(test_data)\n",
    "test_labels = list(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the shuffled data\n",
    "# with open('dbpedia_train_tokens_filtered_shuffled.pkl', 'wb') as file:\n",
    "#     pickle.dump(train_tokens, file)\n",
    "\n",
    "# with open('dbpedia_train_labels_filtered_shuffled.pkl', 'wb') as file:\n",
    "#     pickle.dump(train_labels, file)\n",
    "\n",
    "# with open('dbpedia_test_tokens_filtered_shuffled.pkl', 'wb') as file:\n",
    "#     pickle.dump(test_tokens, file)\n",
    "\n",
    "# with open('dbpedia_test_labels_filtered_shuffled.pkl', 'wb') as file:\n",
    "#     pickle.dump(test_labels, file)\n",
    "\n",
    "# Load the tokenized data\n",
    "with open('dbpedia_train_tokens_filtered_shuffled.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "with open('dbpedia_train_labels_filtered_shuffled.pkl', 'rb') as file:\n",
    "    train_labels = pickle.load(file)\n",
    "\n",
    "with open('dbpedia_test_tokens_filtered_shuffled.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "\n",
    "with open('dbpedia_test_labels_filtered_shuffled.pkl', 'rb') as file:\n",
    "    test_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 4\n",
    "learning_rate = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 14  # Output size is 13 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def calculate_tfidf(tokens, vocab_dict):\n",
    "\n",
    "    n_docs = len(tokens)\n",
    "    vocab_size = len(vocab_dict)\n",
    "    \n",
    "    # Calculate document frequencies (DF) for each word\n",
    "    doc_freq = Counter()\n",
    "    for doc in tokens:\n",
    "        doc_freq.update(set(doc))\n",
    "    \n",
    "    # Precompute IDF values\n",
    "    idf_dict = {}\n",
    "    for word, count in doc_freq.items():\n",
    "        idf_dict[word] = log(n_docs / (count + 1))  # Add 1 to avoid division by zero\n",
    "    \n",
    "    tfidf_matrix = np.zeros((n_docs, vocab_size))\n",
    "    \n",
    "    for i, doc in enumerate(tokens):\n",
    "        # if i % 1000 == 0:\n",
    "        #     print(i)\n",
    "        \n",
    "        total_words_in_doc = len(doc)\n",
    "        term_freq = Counter(doc)  # Calculate term frequency (TF) for the document\n",
    "        \n",
    "        for word, tf in term_freq.items():\n",
    "            if word in vocab_dict:\n",
    "                tfidf_matrix[i][vocab_dict[word]] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "            else:\n",
    "                # Use 'UNK' if the word is not in the vocabulary\n",
    "                tfidf_matrix[i][vocab_dict['UNK']] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Batch [1], Loss: 2.6388494968414307\n",
      "Epoch [1/4], Batch [2], Loss: 2.588744878768921\n",
      "Epoch [1/4], Batch [3], Loss: 2.5391180515289307\n",
      "Epoch [1/4], Batch [4], Loss: 2.489314556121826\n",
      "Epoch [1/4], Batch [5], Loss: 2.4398436546325684\n",
      "Epoch [1/4], Batch [6], Loss: 2.3909718990325928\n",
      "Epoch [1/4], Batch [7], Loss: 2.340740919113159\n",
      "Epoch [1/4], Batch [8], Loss: 2.292851448059082\n",
      "Epoch [1/4], Batch [9], Loss: 2.2467446327209473\n",
      "Epoch [1/4], Batch [10], Loss: 2.1976256370544434\n",
      "Epoch [1/4], Batch [11], Loss: 2.152862310409546\n",
      "Epoch [1/4], Batch [12], Loss: 2.104461431503296\n",
      "Epoch [1/4], Batch [13], Loss: 2.0598862171173096\n",
      "Epoch [1/4], Batch [14], Loss: 2.0156192779541016\n",
      "Epoch [1/4], Batch [15], Loss: 1.971560001373291\n",
      "Epoch [1/4], Batch [16], Loss: 1.9302208423614502\n",
      "Epoch [1/4], Batch [17], Loss: 1.881575584411621\n",
      "Epoch [1/4], Batch [18], Loss: 1.8424794673919678\n",
      "Epoch [1/4], Batch [19], Loss: 1.8020232915878296\n",
      "Epoch [1/4], Batch [20], Loss: 1.7590304613113403\n",
      "Epoch [1/4], Batch [21], Loss: 1.719362497329712\n",
      "Epoch [1/4], Batch [22], Loss: 1.6777750253677368\n",
      "Epoch [1/4], Batch [23], Loss: 1.6413228511810303\n",
      "Epoch [1/4], Batch [24], Loss: 1.6042553186416626\n",
      "Epoch [1/4], Batch [25], Loss: 1.5693068504333496\n",
      "Epoch [1/4], Batch [26], Loss: 1.532394289970398\n",
      "Epoch [1/4], Batch [27], Loss: 1.4901710748672485\n",
      "Epoch [1/4], Batch [28], Loss: 1.4614349603652954\n",
      "Epoch [1/4], Batch [29], Loss: 1.4342238903045654\n",
      "Epoch [1/4], Batch [30], Loss: 1.3939276933670044\n",
      "Epoch [1/4], Batch [31], Loss: 1.3613741397857666\n",
      "Epoch [1/4], Batch [32], Loss: 1.3349438905715942\n",
      "Epoch [1/4], Batch [33], Loss: 1.3050225973129272\n",
      "Epoch [1/4], Batch [34], Loss: 1.278414011001587\n",
      "Epoch [1/4], Batch [35], Loss: 1.2490507364273071\n",
      "Epoch [1/4], Batch [36], Loss: 1.2125468254089355\n",
      "Epoch [1/4], Batch [37], Loss: 1.1926368474960327\n",
      "Epoch [1/4], Batch [38], Loss: 1.1633743047714233\n",
      "Epoch [1/4], Batch [39], Loss: 1.1397534608840942\n",
      "Epoch [1/4], Batch [40], Loss: 1.1116324663162231\n",
      "Epoch [1/4], Batch [41], Loss: 1.0976744890213013\n",
      "Epoch [1/4], Batch [42], Loss: 1.0678719282150269\n",
      "Epoch [1/4], Batch [43], Loss: 1.048572063446045\n",
      "Epoch [1/4], Batch [44], Loss: 1.0273796319961548\n",
      "Epoch [1/4], Batch [45], Loss: 1.0081877708435059\n",
      "Epoch [1/4], Batch [46], Loss: 0.9819740056991577\n",
      "Epoch [1/4], Batch [47], Loss: 0.9632156491279602\n",
      "Epoch [1/4], Batch [48], Loss: 0.9412524700164795\n",
      "Epoch [1/4], Batch [49], Loss: 0.9291039109230042\n",
      "Epoch [1/4], Batch [50], Loss: 0.9037865996360779\n",
      "Epoch [1/4], Batch [51], Loss: 0.8854095935821533\n",
      "Epoch [1/4], Batch [52], Loss: 0.8671903610229492\n",
      "Epoch [1/4], Batch [53], Loss: 0.8558973670005798\n",
      "Epoch [1/4], Batch [54], Loss: 0.8343793749809265\n",
      "Epoch [1/4], Batch [55], Loss: 0.8217577934265137\n",
      "Epoch [1/4], Batch [56], Loss: 0.8140150308609009\n",
      "Accuracy of the network on the 560000 train inputs: 91.39660714285715 %\n",
      "Epoch [2/4], Batch [1], Loss: 0.74906325340271\n",
      "Epoch [2/4], Batch [2], Loss: 0.7328136563301086\n",
      "Epoch [2/4], Batch [3], Loss: 0.7288074493408203\n",
      "Epoch [2/4], Batch [4], Loss: 0.7225713133811951\n",
      "Epoch [2/4], Batch [5], Loss: 0.7102342844009399\n",
      "Epoch [2/4], Batch [6], Loss: 0.7033451199531555\n",
      "Epoch [2/4], Batch [7], Loss: 0.6826821565628052\n",
      "Epoch [2/4], Batch [8], Loss: 0.6738258004188538\n",
      "Epoch [2/4], Batch [9], Loss: 0.6650406718254089\n",
      "Epoch [2/4], Batch [10], Loss: 0.6553964614868164\n",
      "Epoch [2/4], Batch [11], Loss: 0.6505982279777527\n",
      "Epoch [2/4], Batch [12], Loss: 0.6341260671615601\n",
      "Epoch [2/4], Batch [13], Loss: 0.6245161890983582\n",
      "Epoch [2/4], Batch [14], Loss: 0.6192013621330261\n",
      "Epoch [2/4], Batch [15], Loss: 0.609265148639679\n",
      "Epoch [2/4], Batch [16], Loss: 0.6057287454605103\n",
      "Epoch [2/4], Batch [17], Loss: 0.5870667099952698\n",
      "Epoch [2/4], Batch [18], Loss: 0.5843817591667175\n",
      "Epoch [2/4], Batch [19], Loss: 0.5769019722938538\n",
      "Epoch [2/4], Batch [20], Loss: 0.5701543092727661\n",
      "Epoch [2/4], Batch [21], Loss: 0.5542439818382263\n",
      "Epoch [2/4], Batch [22], Loss: 0.5486037731170654\n",
      "Epoch [2/4], Batch [23], Loss: 0.5430533289909363\n",
      "Epoch [2/4], Batch [24], Loss: 0.5366788506507874\n",
      "Epoch [2/4], Batch [25], Loss: 0.5357354879379272\n",
      "Epoch [2/4], Batch [26], Loss: 0.5266579389572144\n",
      "Epoch [2/4], Batch [27], Loss: 0.5091878175735474\n",
      "Epoch [2/4], Batch [28], Loss: 0.5116223096847534\n",
      "Epoch [2/4], Batch [29], Loss: 0.5148177742958069\n",
      "Epoch [2/4], Batch [30], Loss: 0.4979703724384308\n",
      "Epoch [2/4], Batch [31], Loss: 0.4910154342651367\n",
      "Epoch [2/4], Batch [32], Loss: 0.49004101753234863\n",
      "Epoch [2/4], Batch [33], Loss: 0.4812003970146179\n",
      "Epoch [2/4], Batch [34], Loss: 0.4806877076625824\n",
      "Epoch [2/4], Batch [35], Loss: 0.4757639765739441\n",
      "Epoch [2/4], Batch [36], Loss: 0.46070653200149536\n",
      "Epoch [2/4], Batch [37], Loss: 0.4655172824859619\n",
      "Epoch [2/4], Batch [38], Loss: 0.45312896370887756\n",
      "Epoch [2/4], Batch [39], Loss: 0.45352640748023987\n",
      "Epoch [2/4], Batch [40], Loss: 0.4444744288921356\n",
      "Epoch [2/4], Batch [41], Loss: 0.4483311176300049\n",
      "Epoch [2/4], Batch [42], Loss: 0.4381219148635864\n",
      "Epoch [2/4], Batch [43], Loss: 0.4375445246696472\n",
      "Epoch [2/4], Batch [44], Loss: 0.4369126856327057\n",
      "Epoch [2/4], Batch [45], Loss: 0.4313786029815674\n",
      "Epoch [2/4], Batch [46], Loss: 0.4207611382007599\n",
      "Epoch [2/4], Batch [47], Loss: 0.41835081577301025\n",
      "Epoch [2/4], Batch [48], Loss: 0.4118134677410126\n",
      "Epoch [2/4], Batch [49], Loss: 0.41310933232307434\n",
      "Epoch [2/4], Batch [50], Loss: 0.4035061001777649\n",
      "Epoch [2/4], Batch [51], Loss: 0.4018288850784302\n",
      "Epoch [2/4], Batch [52], Loss: 0.39372801780700684\n",
      "Epoch [2/4], Batch [53], Loss: 0.3953836262226105\n",
      "Epoch [2/4], Batch [54], Loss: 0.3878541588783264\n",
      "Epoch [2/4], Batch [55], Loss: 0.38719308376312256\n",
      "Epoch [2/4], Batch [56], Loss: 0.39080560207366943\n",
      "Accuracy of the network on the 560000 train inputs: 95.81214285714286 %\n",
      "Epoch [3/4], Batch [1], Loss: 0.35873404145240784\n",
      "Epoch [3/4], Batch [2], Loss: 0.35052335262298584\n",
      "Epoch [3/4], Batch [3], Loss: 0.353732705116272\n",
      "Epoch [3/4], Batch [4], Loss: 0.35753610730171204\n",
      "Epoch [3/4], Batch [5], Loss: 0.35283830761909485\n",
      "Epoch [3/4], Batch [6], Loss: 0.35483866930007935\n",
      "Epoch [3/4], Batch [7], Loss: 0.3436010181903839\n",
      "Epoch [3/4], Batch [8], Loss: 0.3426531255245209\n",
      "Epoch [3/4], Batch [9], Loss: 0.34115710854530334\n",
      "Epoch [3/4], Batch [10], Loss: 0.3401408791542053\n",
      "Epoch [3/4], Batch [11], Loss: 0.34177762269973755\n",
      "Epoch [3/4], Batch [12], Loss: 0.3335500657558441\n",
      "Epoch [3/4], Batch [13], Loss: 0.33014434576034546\n",
      "Epoch [3/4], Batch [14], Loss: 0.3315025568008423\n",
      "Epoch [3/4], Batch [15], Loss: 0.32829049229621887\n",
      "Epoch [3/4], Batch [16], Loss: 0.3309430181980133\n",
      "Epoch [3/4], Batch [17], Loss: 0.3195266127586365\n",
      "Epoch [3/4], Batch [18], Loss: 0.32174989581108093\n",
      "Epoch [3/4], Batch [19], Loss: 0.3206562101840973\n",
      "Epoch [3/4], Batch [20], Loss: 0.32051193714141846\n",
      "Epoch [3/4], Batch [21], Loss: 0.3079787790775299\n",
      "Epoch [3/4], Batch [22], Loss: 0.30875706672668457\n",
      "Epoch [3/4], Batch [23], Loss: 0.3083442449569702\n",
      "Epoch [3/4], Batch [24], Loss: 0.3071712851524353\n",
      "Epoch [3/4], Batch [25], Loss: 0.31230562925338745\n",
      "Epoch [3/4], Batch [26], Loss: 0.3075794577598572\n",
      "Epoch [3/4], Batch [27], Loss: 0.2950727939605713\n",
      "Epoch [3/4], Batch [28], Loss: 0.30171361565589905\n",
      "Epoch [3/4], Batch [29], Loss: 0.30710774660110474\n",
      "Epoch [3/4], Batch [30], Loss: 0.2957666516304016\n",
      "Epoch [3/4], Batch [31], Loss: 0.2929500341415405\n",
      "Epoch [3/4], Batch [32], Loss: 0.2946309745311737\n",
      "Epoch [3/4], Batch [33], Loss: 0.2884908616542816\n",
      "Epoch [3/4], Batch [34], Loss: 0.291252464056015\n",
      "Epoch [3/4], Batch [35], Loss: 0.2910432517528534\n",
      "Epoch [3/4], Batch [36], Loss: 0.28027647733688354\n",
      "Epoch [3/4], Batch [37], Loss: 0.28899988532066345\n",
      "Epoch [3/4], Batch [38], Loss: 0.2779342532157898\n",
      "Epoch [3/4], Batch [39], Loss: 0.28250566124916077\n",
      "Epoch [3/4], Batch [40], Loss: 0.27655738592147827\n",
      "Epoch [3/4], Batch [41], Loss: 0.28213536739349365\n",
      "Epoch [3/4], Batch [42], Loss: 0.27553144097328186\n",
      "Epoch [3/4], Batch [43], Loss: 0.27759525179862976\n",
      "Epoch [3/4], Batch [44], Loss: 0.28126659989356995\n",
      "Epoch [3/4], Batch [45], Loss: 0.2768060564994812\n",
      "Epoch [3/4], Batch [46], Loss: 0.26879656314849854\n",
      "Epoch [3/4], Batch [47], Loss: 0.2684272229671478\n",
      "Epoch [3/4], Batch [48], Loss: 0.26504889130592346\n",
      "Epoch [3/4], Batch [49], Loss: 0.26835301518440247\n",
      "Epoch [3/4], Batch [50], Loss: 0.26192161440849304\n",
      "Epoch [3/4], Batch [51], Loss: 0.2630065381526947\n",
      "Epoch [3/4], Batch [52], Loss: 0.2567655146121979\n",
      "Epoch [3/4], Batch [53], Loss: 0.2596731185913086\n",
      "Epoch [3/4], Batch [54], Loss: 0.2559107542037964\n",
      "Epoch [3/4], Batch [55], Loss: 0.2568957507610321\n",
      "Epoch [3/4], Batch [56], Loss: 0.2620140314102173\n",
      "Accuracy of the network on the 560000 train inputs: 96.76928571428572 %\n",
      "Epoch [4/4], Batch [1], Loss: 0.2400333434343338\n",
      "Epoch [4/4], Batch [2], Loss: 0.234178826212883\n",
      "Epoch [4/4], Batch [3], Loss: 0.23723481595516205\n",
      "Epoch [4/4], Batch [4], Loss: 0.24234627187252045\n",
      "Epoch [4/4], Batch [5], Loss: 0.2388060837984085\n",
      "Epoch [4/4], Batch [6], Loss: 0.24250991642475128\n",
      "Epoch [4/4], Batch [7], Loss: 0.23324687778949738\n",
      "Epoch [4/4], Batch [8], Loss: 0.2339843064546585\n",
      "Epoch [4/4], Batch [9], Loss: 0.23357710242271423\n",
      "Epoch [4/4], Batch [10], Loss: 0.23408305644989014\n",
      "Epoch [4/4], Batch [11], Loss: 0.2367585450410843\n",
      "Epoch [4/4], Batch [12], Loss: 0.230460062623024\n",
      "Epoch [4/4], Batch [13], Loss: 0.22864164412021637\n",
      "Epoch [4/4], Batch [14], Loss: 0.23079143464565277\n",
      "Epoch [4/4], Batch [15], Loss: 0.22895239293575287\n",
      "Epoch [4/4], Batch [16], Loss: 0.23256030678749084\n",
      "Epoch [4/4], Batch [17], Loss: 0.2233932912349701\n",
      "Epoch [4/4], Batch [18], Loss: 0.22593334317207336\n",
      "Epoch [4/4], Batch [19], Loss: 0.2264149934053421\n",
      "Epoch [4/4], Batch [20], Loss: 0.22728624939918518\n",
      "Epoch [4/4], Batch [21], Loss: 0.21616820991039276\n",
      "Epoch [4/4], Batch [22], Loss: 0.21822111308574677\n",
      "Epoch [4/4], Batch [23], Loss: 0.21889208257198334\n",
      "Epoch [4/4], Batch [24], Loss: 0.21914492547512054\n",
      "Epoch [4/4], Batch [25], Loss: 0.2255224883556366\n",
      "Epoch [4/4], Batch [26], Loss: 0.22203005850315094\n",
      "Epoch [4/4], Batch [27], Loss: 0.21141009032726288\n",
      "Epoch [4/4], Batch [28], Loss: 0.2183746099472046\n",
      "Epoch [4/4], Batch [29], Loss: 0.2231319099664688\n",
      "Epoch [4/4], Batch [30], Loss: 0.2141951471567154\n",
      "Epoch [4/4], Batch [31], Loss: 0.2123311311006546\n",
      "Epoch [4/4], Batch [32], Loss: 0.21452228724956512\n",
      "Epoch [4/4], Batch [33], Loss: 0.2089373767375946\n",
      "Epoch [4/4], Batch [34], Loss: 0.2121027410030365\n",
      "Epoch [4/4], Batch [35], Loss: 0.21364212036132812\n",
      "Epoch [4/4], Batch [36], Loss: 0.2042616456747055\n",
      "Epoch [4/4], Batch [37], Loss: 0.21400325000286102\n",
      "Epoch [4/4], Batch [38], Loss: 0.20304834842681885\n",
      "Epoch [4/4], Batch [39], Loss: 0.20858171582221985\n",
      "Epoch [4/4], Batch [40], Loss: 0.20366398990154266\n",
      "Epoch [4/4], Batch [41], Loss: 0.2093883752822876\n",
      "Epoch [4/4], Batch [42], Loss: 0.20417815446853638\n",
      "Epoch [4/4], Batch [43], Loss: 0.20639841258525848\n",
      "Epoch [4/4], Batch [44], Loss: 0.21181997656822205\n",
      "Epoch [4/4], Batch [45], Loss: 0.20729374885559082\n",
      "Epoch [4/4], Batch [46], Loss: 0.2001933604478836\n",
      "Epoch [4/4], Batch [47], Loss: 0.20003646612167358\n",
      "Epoch [4/4], Batch [48], Loss: 0.1980445384979248\n",
      "Epoch [4/4], Batch [49], Loss: 0.20170508325099945\n",
      "Epoch [4/4], Batch [50], Loss: 0.1965157389640808\n",
      "Epoch [4/4], Batch [51], Loss: 0.1981334686279297\n",
      "Epoch [4/4], Batch [52], Loss: 0.19293954968452454\n",
      "Epoch [4/4], Batch [53], Loss: 0.1958405077457428\n",
      "Epoch [4/4], Batch [54], Loss: 0.19362656772136688\n",
      "Epoch [4/4], Batch [55], Loss: 0.1950473040342331\n",
      "Epoch [4/4], Batch [56], Loss: 0.20030038058757782\n",
      "Accuracy of the network on the 560000 train inputs: 97.33017857142858 %\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "        # print(len(inputs))\n",
    "        # print(len(vocab))\n",
    "        \n",
    "        # make bow vector for inputs\n",
    "        # bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "        # # print(bow.shape)\n",
    "        # for j in range(batch_size):\n",
    "        #     for token in inputs[j]:\n",
    "\n",
    "        #         bow[j][vocab_dict[token]] += 1\n",
    "\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "\n",
    "        \n",
    "        # convert bow to tensor\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "        # Convert labels to LongTensors\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        # Print the loss for this batch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}')\n",
    "\n",
    "    print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "# torch.save(model.state_dict(), 'dbpedia_model_tfidf.ckpt')\n",
    "\n",
    "# Load the model\n",
    "model = BoWClassifier(input_size, output_size).to(device)\n",
    "model.load_state_dict(torch.load('dbpedia_model_tfidf.ckpt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 70000 test inputs: 7.615714285714286 %\n",
      "Confusion Matrix:\n",
      "[[ 522  299  390  142  225  276  364   83   97  273  201  274 1007  847]\n",
      " [ 562  307  299  206  225  505  154  142   84  269  197  184 1035  831]\n",
      " [ 558  567  508  178  136  200  320  132  123  138  160  269  814  897]\n",
      " [1114  475  581  193  173  293  241  162  193  122   97  205  578  573]\n",
      " [ 659  688  488  170  118  213  197  290  130  277  136  285  969  380]\n",
      " [ 523  338  898  127  119  265  280   72   64  319  125  317 1116  437]\n",
      " [ 486  295  453  174  196  229  375  164   66  119  138  334 1484  487]\n",
      " [ 597 1465  474  184  311  116  137   33   63   53  396  201  504  466]\n",
      " [ 573  473  273   72  366  133  319  133  219  310  197  211  740  981]\n",
      " [ 514  563  387  761   97  257  305  137  132  258  204  232  641  512]\n",
      " [ 763  266  371  259  117  527  205  153  161  357  225  290  827  479]\n",
      " [ 516  950  280  201  133  411  415  220   76  144  121  304  582  647]\n",
      " [ 493  418  454  259  155  246  269   85  105  158  246  246 1210  656]\n",
      " [ 589  425  733  275  236  280  189  101   56  191  192  310  629  794]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.10      0.08      5000\n",
      "           1       0.04      0.06      0.05      5000\n",
      "           2       0.08      0.10      0.09      5000\n",
      "           3       0.06      0.04      0.05      5000\n",
      "           4       0.05      0.02      0.03      5000\n",
      "           5       0.07      0.05      0.06      5000\n",
      "           6       0.10      0.07      0.09      5000\n",
      "           7       0.02      0.01      0.01      5000\n",
      "           8       0.14      0.04      0.07      5000\n",
      "           9       0.09      0.05      0.06      5000\n",
      "          10       0.09      0.04      0.06      5000\n",
      "          11       0.08      0.06      0.07      5000\n",
      "          12       0.10      0.24      0.14      5000\n",
      "          13       0.09      0.16      0.11      5000\n",
      "\n",
      "    accuracy                           0.08     70000\n",
      "   macro avg       0.08      0.08      0.07     70000\n",
      "weighted avg       0.08      0.08      0.07     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(0, len(test_tokens), batch_size):\n",
    "        inputs = test_tokens[i:i + batch_size]\n",
    "        # bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "\n",
    "        # for j in range(batch_size):\n",
    "        #     for token in inputs[j]:\n",
    "        #         if token in vocab_dict:\n",
    "        #             bow[j][vocab_dict[token]] += 1\n",
    "        #         else:\n",
    "        #             bow[j][vocab_dict['UNK']] += 1\n",
    "\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        labels = test_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the {total} test inputs: {accuracy} %')\n",
    "\n",
    "    # Calculate confusion matrix, precision, recall, and F1 scores\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    classification_rep = classification_report(true_labels, predictions, target_names=[str(i) for i in range(14)])\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    # print(\"F1 Micro:\", f1_micro)\n",
    "    # print(\"F1 Macro:\", f1_macro)\n",
    "    # print(\"F1 Weighted:\", f1_weighted)\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassifications: 2714\n"
     ]
    }
   ],
   "source": [
    "# get the misclassifications and print few of them\n",
    "misclassified = []\n",
    "for i in range(len(true_labels)):\n",
    "    if true_labels[i] != predictions[i]:\n",
    "        misclassified.append(i)\n",
    "\n",
    "print(\"Number of misclassifications:\", len(misclassified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified example:\n",
      "True label: 4\n",
      "Predicted label: 2\n",
      "Text: ['kamal', 'kamal', 'born', 'born', 'renowned', 'social', 'worker']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 4\n",
      "Predicted label: 0\n",
      "Text: ['native', 'spain', 'spanish', 'governor', 'general', 'philippines', 'december', 'march', 'among', 'archipelago', 'account', 'ability', 'public', 'welfare', 'especially', 'active', 'building', 'providing', 'means', 'communication', 'bring', 'inland', 'maritime', 'provinces', 'communication']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 8\n",
      "Predicted label: 13\n",
      "Text: ['buddha', 'image', 'buddha', 'image', 'record', 'base', 'bronze', 'image', 'buddha', 'recording', 'wife', 'assigned', 'fifth', 'ce', 'ce']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 0\n",
      "Predicted label: 1\n",
      "Text: ['network', 'science', 'center', 'network', 'science', 'center', 'founded', 'october', 'professor', 'indiana', 'university', 'center', 'mission', 'advance', 'tools', 'study', 'social', 'science', 'physics', 'networks']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 12\n",
      "Predicted label: 13\n",
      "Text: ['children', 'children', 'science', 'fiction', 'film', 'sequel', 'version', 'village', 'group', 'children', 'similar', 'powers', 'original', 'opposite', 'interpretation', 'children', 'good', 'pure', 'form', 'human', 'instead', 'evil', 'alien']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 12\n",
      "Predicted label: 1\n",
      "Text: ['four', 'girls', 'white', 'four', 'girls', 'white', 'drama', 'directed', 'simon', 'starring', 'florence', 'rice', 'four', 'nursing', 'students', 'enrolled', 'three', 'year', 'training', 'course']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 2\n",
      "Predicted label: 11\n",
      "Text: ['scott', 'scott', 'british', 'american', 'singer', 'known', 'singing', 'english', 'versions', 'theme', 'songs', 'series', 'mostly', 'songs', 'originally', 'sung', 'mostly', 'songs', 'originally', 'sung', 'scott', 'guest', 'singer', 'los', 'angeles', 'concert']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 0\n",
      "Predicted label: 11\n",
      "Text: ['mark', 'vii', 'limited', 'mark', 'vii', 'limited', 'mark', 'seven', 'limited', 'mark', 'seven', 'limited', 'production', 'company', 'actor', 'producer', 'director', 'jack', 'webb', 'active', 'webb', 'death', 'vii', 'vii']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 12\n",
      "Predicted label: 11\n",
      "Text: ['remix', 'remix', 'official', 'english', 'language', 'version', 'film', 'produced', 'edited', 'hollywood', 'director', 'brett', 'international', 'version', 'minutes', 'long', 'opposed', 'hindi', 'version', 'minutes', 'long', 'new', 'version', 'original', 'hindi', 'songs', 'except', 'title', 'track', 'fire', 'alternative', 'english', 'version', 'heard', 'end', 'titles']\n",
      "\n",
      "Misclassified example:\n",
      "True label: 11\n",
      "Predicted label: 2\n",
      "Text: ['robot', 'robot', 'sixth', 'album', 'musician', 'mc', 'stated', 'worked', 'weird', 'al', 'brothers', 'nick', 'mike', 'kennedy', 'james', 'daniel', 'time', 'finn', 'flash', 'pierre', 'simple', 'plan', 'mc', 'bat', 'commander', 'suburban', 'worm', 'quartet', 'gabriel', 'brett', 'anderson', 'mc', 'blonde', 'jesse', 'size', 'parry', 'jonathan', 'sebastian', 'reynolds', 'b', 'mc', 'mc', 'mc']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Misclassified example:\")\n",
    "    print(\"True label:\", true_labels[misclassified[i]])\n",
    "    print(\"Predicted label:\", predictions[misclassified[i]])\n",
    "    print(\"Text:\", test_data[misclassified[i]])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/d7g63tw141g81ynhx4f_hngr0000gn/T/ipykernel_26630/1777643572.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 70000 test inputs: 96.46 %\n",
      "Length 24 - Accuracy: 95.78% (Correct: 2295, Total: 2396)\n",
      "Length 16 - Accuracy: 96.68% (Correct: 2479, Total: 2564)\n",
      "Length 19 - Accuracy: 96.47% (Correct: 2375, Total: 2462)\n",
      "Length 9 - Accuracy: 97.35% (Correct: 2090, Total: 2147)\n",
      "Length 8 - Accuracy: 97.11% (Correct: 1986, Total: 2045)\n",
      "Length 25 - Accuracy: 95.89% (Correct: 2311, Total: 2410)\n",
      "Length 10 - Accuracy: 97.04% (Correct: 2099, Total: 2163)\n",
      "Length 23 - Accuracy: 95.83% (Correct: 2435, Total: 2541)\n",
      "Length 32 - Accuracy: 96.65% (Correct: 1268, Total: 1312)\n",
      "Length 22 - Accuracy: 95.50% (Correct: 2376, Total: 2488)\n",
      "Length 31 - Accuracy: 95.47% (Correct: 1517, Total: 1589)\n",
      "Length 27 - Accuracy: 96.15% (Correct: 2196, Total: 2284)\n",
      "Length 21 - Accuracy: 96.18% (Correct: 2520, Total: 2620)\n",
      "Length 33 - Accuracy: 97.07% (Correct: 1028, Total: 1059)\n",
      "Length 35 - Accuracy: 96.27% (Correct: 620, Total: 644)\n",
      "Length 17 - Accuracy: 96.54% (Correct: 2347, Total: 2431)\n",
      "Length 14 - Accuracy: 96.85% (Correct: 2400, Total: 2478)\n",
      "Length 7 - Accuracy: 97.16% (Correct: 2118, Total: 2180)\n",
      "Length 13 - Accuracy: 96.64% (Correct: 2301, Total: 2381)\n",
      "Length 26 - Accuracy: 95.63% (Correct: 2211, Total: 2312)\n",
      "Length 11 - Accuracy: 97.10% (Correct: 2240, Total: 2307)\n",
      "Length 15 - Accuracy: 96.56% (Correct: 2525, Total: 2615)\n",
      "Length 12 - Accuracy: 96.54% (Correct: 2206, Total: 2285)\n",
      "Length 3 - Accuracy: 97.17% (Correct: 1029, Total: 1059)\n",
      "Length 20 - Accuracy: 96.67% (Correct: 2468, Total: 2553)\n",
      "Length 37 - Accuracy: 93.62% (Correct: 323, Total: 345)\n",
      "Length 18 - Accuracy: 96.80% (Correct: 2331, Total: 2408)\n",
      "Length 6 - Accuracy: 97.75% (Correct: 1955, Total: 2000)\n",
      "Length 4 - Accuracy: 98.12% (Correct: 1725, Total: 1758)\n",
      "Length 30 - Accuracy: 95.44% (Correct: 1715, Total: 1797)\n",
      "Length 34 - Accuracy: 96.26% (Correct: 797, Total: 828)\n",
      "Length 29 - Accuracy: 96.25% (Correct: 1953, Total: 2029)\n",
      "Length 36 - Accuracy: 95.30% (Correct: 446, Total: 468)\n",
      "Length 28 - Accuracy: 95.51% (Correct: 2063, Total: 2160)\n",
      "Length 38 - Accuracy: 96.83% (Correct: 244, Total: 252)\n",
      "Length 5 - Accuracy: 97.64% (Correct: 1779, Total: 1822)\n",
      "Length 39 - Accuracy: 96.49% (Correct: 165, Total: 171)\n",
      "Length 2 - Accuracy: 93.06% (Correct: 295, Total: 317)\n",
      "Length 40 - Accuracy: 92.05% (Correct: 81, Total: 88)\n",
      "Length 43 - Accuracy: 96.67% (Correct: 29, Total: 30)\n",
      "Length 42 - Accuracy: 94.87% (Correct: 37, Total: 39)\n",
      "Length 44 - Accuracy: 95.00% (Correct: 19, Total: 20)\n",
      "Length 41 - Accuracy: 98.00% (Correct: 49, Total: 50)\n",
      "Length 62 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 1 - Accuracy: 71.79% (Correct: 28, Total: 39)\n",
      "Length 66 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 56 - Accuracy: 100.00% (Correct: 2, Total: 2)\n",
      "Length 55 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 52 - Accuracy: 50.00% (Correct: 1, Total: 2)\n",
      "Length 46 - Accuracy: 87.50% (Correct: 7, Total: 8)\n",
      "Length 47 - Accuracy: 84.62% (Correct: 11, Total: 13)\n",
      "Length 73 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 45 - Accuracy: 100.00% (Correct: 9, Total: 9)\n",
      "Length 103 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 53 - Accuracy: 100.00% (Correct: 2, Total: 2)\n",
      "Length 49 - Accuracy: 100.00% (Correct: 4, Total: 4)\n",
      "Length 61 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 65 - Accuracy: 0.00% (Correct: 0, Total: 1)\n",
      "Length 50 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 79 - Accuracy: 0.00% (Correct: 0, Total: 1)\n",
      "Length 54 - Accuracy: 100.00% (Correct: 2, Total: 2)\n",
      "Length 59 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 58 - Accuracy: 100.00% (Correct: 1, Total: 1)\n",
      "Length 64 - Accuracy: 100.00% (Correct: 1, Total: 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store length-wise statistics\n",
    "length_correct_total = {}\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(0, len(test_tokens), batch_size):\n",
    "        inputs = test_tokens[i:i + batch_size]\n",
    "        bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            for token in inputs[j]:\n",
    "                if token in vocab_dict:\n",
    "                    bow[j][vocab_dict[token]] += 1\n",
    "                else:\n",
    "                    bow[j][vocab_dict['UNK']] += 1\n",
    "\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        labels = test_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Update length-wise statistics\n",
    "        for j in range(len(inputs)):\n",
    "            length = int(torch.sum(inputs[j] != 0).item())\n",
    "            # print(length)\n",
    "            if length not in length_correct_total:\n",
    "                length_correct_total[length] = [0, 0]\n",
    "            if predicted[j] == labels[j]:\n",
    "                length_correct_total[length][0] += 1\n",
    "            length_correct_total[length][1] += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the {total} test inputs: {accuracy} %')\n",
    "\n",
    "# Analyze length-wise statistics\n",
    "for length, (correct_predictions, total_predictions) in length_correct_total.items():\n",
    "    accuracy = correct_predictions / total_predictions * 100 if total_predictions > 0 else 0\n",
    "    print(f\"Length {length} - Accuracy: {accuracy:.2f}% (Correct: {correct_predictions}, Total: {total_predictions})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 0 - Accuracy: 0.00% (Correct: 0, Total: 0)\n",
      "Length 10 - Accuracy: 97.26% (Correct: 15104, Total: 15530)\n",
      "Length 20 - Accuracy: 96.68% (Correct: 23672, Total: 24484)\n",
      "Length 30 - Accuracy: 95.82% (Correct: 22075, Total: 23037)\n",
      "Length 40 - Accuracy: 96.05% (Correct: 6489, Total: 6756)\n",
      "Length 50 - Accuracy: 95.40% (Correct: 166, Total: 174)\n",
      "Length 60 - Accuracy: 90.91% (Correct: 10, Total: 11)\n",
      "Length 70 - Accuracy: 80.00% (Correct: 4, Total: 5)\n",
      "Length 80 - Accuracy: 50.00% (Correct: 1, Total: 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "length_correct_total_binned = {bin: [0, 0] for bin in bins}\n",
    "for lenght, (correct_predictions, total_predictions) in length_correct_total.items():\n",
    "    for bin in bins:\n",
    "        if lenght <= bin:\n",
    "            length_correct_total_binned[bin][0] += correct_predictions\n",
    "            length_correct_total_binned[bin][1] += total_predictions\n",
    "            break\n",
    "\n",
    "for bin, (correct_predictions, total_predictions) in length_correct_total_binned.items():\n",
    "    accuracy = correct_predictions / total_predictions * 100 if total_predictions > 0 else 0\n",
    "    print(f\"Length {bin} - Accuracy: {accuracy:.2f}% (Correct: {correct_predictions}, Total: {total_predictions})\")\n",
    "\n",
    "# plot length-wise statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [length for length in length_correct_total_binned.keys()]\n",
    "accuracies = [correct_predictions / total_predictions * 100 if total_predictions > 0 else 0 for _, (correct_predictions, total_predictions) in length_correct_total_binned.items()]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(lengths, accuracies, width=3)\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Length\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 24499 (\\N{CJK UNIFIED IDEOGRAPH-5FB3}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 24029 (\\N{CJK UNIFIED IDEOGRAPH-5DDD}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 23567 (\\N{CJK UNIFIED IDEOGRAPH-5C0F}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 26519 (\\N{CJK UNIFIED IDEOGRAPH-6797}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 26666 (\\N{CJK UNIFIED IDEOGRAPH-682A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 24335 (\\N{CJK UNIFIED IDEOGRAPH-5F0F}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 20250 (\\N{CJK UNIFIED IDEOGRAPH-4F1A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 31038 (\\N{CJK UNIFIED IDEOGRAPH-793E}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 30000 (\\N{CJK UNIFIED IDEOGRAPH-7530}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 20013 (\\N{CJK UNIFIED IDEOGRAPH-4E2D}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 37428 (\\N{CJK UNIFIED IDEOGRAPH-9234}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 26408 (\\N{CJK UNIFIED IDEOGRAPH-6728}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 26494 (\\N{CJK UNIFIED IDEOGRAPH-677E}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 26449 (\\N{CJK UNIFIED IDEOGRAPH-6751}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 20234 (\\N{CJK UNIFIED IDEOGRAPH-4F0A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 34276 (\\N{CJK UNIFIED IDEOGRAPH-85E4}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 20304 (\\N{CJK UNIFIED IDEOGRAPH-4F50}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 27211 (\\N{CJK UNIFIED IDEOGRAPH-6A4B}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 1729 (\\N{ARABIC LETTER HEH GOAL}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Matplotlib currently does not support Arabic natively.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 2451 (\\N{BENGALI LETTER O}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Matplotlib currently does not support Bengali natively.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 54788 (\\N{HANGUL SYLLABLE HYEON}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 45824 (\\N{HANGUL SYLLABLE DAE}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 21127 (\\N{CJK UNIFIED IDEOGRAPH-5287}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 22580 (\\N{CJK UNIFIED IDEOGRAPH-5834}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 29256 (\\N{CJK UNIFIED IDEOGRAPH-7248}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12489 (\\N{KATAKANA LETTER DO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12521 (\\N{KATAKANA LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12468 (\\N{KATAKANA LETTER GO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12531 (\\N{KATAKANA LETTER N}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12508 (\\N{KATAKANA LETTER BO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12540 (\\N{KATAKANA-HIRAGANA PROLONGED SOUND MARK}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12523 (\\N{KATAKANA LETTER RU}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 30007 (\\N{CJK UNIFIED IDEOGRAPH-7537}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12399 (\\N{HIRAGANA LETTER HA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12388 (\\N{HIRAGANA LETTER TU}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12425 (\\N{HIRAGANA LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12356 (\\N{HIRAGANA LETTER I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:84: UserWarning: Glyph 12424 (\\N{HIRAGANA LETTER YO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 24499 (\\N{CJK UNIFIED IDEOGRAPH-5FB3}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 24029 (\\N{CJK UNIFIED IDEOGRAPH-5DDD}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 23567 (\\N{CJK UNIFIED IDEOGRAPH-5C0F}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 26519 (\\N{CJK UNIFIED IDEOGRAPH-6797}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 26666 (\\N{CJK UNIFIED IDEOGRAPH-682A}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 24335 (\\N{CJK UNIFIED IDEOGRAPH-5F0F}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 20250 (\\N{CJK UNIFIED IDEOGRAPH-4F1A}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 31038 (\\N{CJK UNIFIED IDEOGRAPH-793E}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 30000 (\\N{CJK UNIFIED IDEOGRAPH-7530}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 20013 (\\N{CJK UNIFIED IDEOGRAPH-4E2D}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 37428 (\\N{CJK UNIFIED IDEOGRAPH-9234}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 26408 (\\N{CJK UNIFIED IDEOGRAPH-6728}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 26494 (\\N{CJK UNIFIED IDEOGRAPH-677E}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 26449 (\\N{CJK UNIFIED IDEOGRAPH-6751}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 20234 (\\N{CJK UNIFIED IDEOGRAPH-4F0A}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 34276 (\\N{CJK UNIFIED IDEOGRAPH-85E4}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 20304 (\\N{CJK UNIFIED IDEOGRAPH-4F50}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 27211 (\\N{CJK UNIFIED IDEOGRAPH-6A4B}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 1729 (\\N{ARABIC LETTER HEH GOAL}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Matplotlib currently does not support Arabic natively.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 2451 (\\N{BENGALI LETTER O}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Matplotlib currently does not support Bengali natively.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 54788 (\\N{HANGUL SYLLABLE HYEON}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 45824 (\\N{HANGUL SYLLABLE DAE}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 21127 (\\N{CJK UNIFIED IDEOGRAPH-5287}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 22580 (\\N{CJK UNIFIED IDEOGRAPH-5834}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 29256 (\\N{CJK UNIFIED IDEOGRAPH-7248}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12489 (\\N{KATAKANA LETTER DO}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12521 (\\N{KATAKANA LETTER RA}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12468 (\\N{KATAKANA LETTER GO}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12531 (\\N{KATAKANA LETTER N}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12508 (\\N{KATAKANA LETTER BO}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12540 (\\N{KATAKANA-HIRAGANA PROLONGED SOUND MARK}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12523 (\\N{KATAKANA LETTER RU}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 30007 (\\N{CJK UNIFIED IDEOGRAPH-7537}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12399 (\\N{HIRAGANA LETTER HA}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12388 (\\N{HIRAGANA LETTER TU}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12425 (\\N{HIRAGANA LETTER RA}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12356 (\\N{HIRAGANA LETTER I}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n",
      "/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:671: UserWarning: Glyph 12424 (\\N{HIRAGANA LETTER YO}) missing from current font.\n",
      "  bboxes = [l.get_window_extent() for l in labels]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m misclassified_index \u001b[39m=\u001b[39m misclassified[\u001b[39m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m \u001b[39m# Visualize the impact of TF-IDF on classification for the selected example\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m visualize_tfidf_impact(model, test_data, test_tokens, true_labels, predictions, misclassified_index, vocab_dict)\n",
      "Cell \u001b[0;32mIn[35], line 24\u001b[0m, in \u001b[0;36mvisualize_tfidf_impact\u001b[0;34m(model, test_data, test_tokens, true_labels, predictions, index, vocab_dict)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Plot TF-IDF values\u001b[39;00m\n\u001b[1;32m     23\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m sns\u001b[39m.\u001b[39;49mheatmap(tfidf_representation, cmap\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYlGnBu\u001b[39;49m\u001b[39m\"\u001b[39;49m, annot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, xticklabels\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(vocab_dict\u001b[39m.\u001b[39;49mkeys()), yticklabels\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mTF-IDF\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     25\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mTF-IDF Representation\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Plot predicted probabilities\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/seaborn/matrix.py:459\u001b[0m, in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m square:\n\u001b[1;32m    458\u001b[0m     ax\u001b[39m.\u001b[39mset_aspect(\u001b[39m\"\u001b[39m\u001b[39mequal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 459\u001b[0m plotter\u001b[39m.\u001b[39;49mplot(ax, cbar_ax, kwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m ax\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/seaborn/matrix.py:342\u001b[0m, in \u001b[0;36m_HeatMapper.plot\u001b[0;34m(self, ax, cax, kws)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m# Possibly rotate them if they overlap\u001b[39;00m\n\u001b[1;32m    340\u001b[0m _draw_figure(ax\u001b[39m.\u001b[39mfigure)\n\u001b[0;32m--> 342\u001b[0m \u001b[39mif\u001b[39;00m axis_ticklabels_overlap(xtl):\n\u001b[1;32m    343\u001b[0m     plt\u001b[39m.\u001b[39msetp(xtl, rotation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m axis_ticklabels_overlap(ytl):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:672\u001b[0m, in \u001b[0;36maxis_ticklabels_overlap\u001b[0;34m(labels)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     bboxes \u001b[39m=\u001b[39m [l\u001b[39m.\u001b[39mget_window_extent() \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m labels]\n\u001b[0;32m--> 672\u001b[0m     overlaps \u001b[39m=\u001b[39m [b\u001b[39m.\u001b[39;49mcount_overlaps(bboxes) \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m bboxes]\n\u001b[1;32m    673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(overlaps) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    674\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[39m# Issue on macos backend raises an error in the above code\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/seaborn/utils.py:672\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     bboxes \u001b[39m=\u001b[39m [l\u001b[39m.\u001b[39mget_window_extent() \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m labels]\n\u001b[0;32m--> 672\u001b[0m     overlaps \u001b[39m=\u001b[39m [b\u001b[39m.\u001b[39;49mcount_overlaps(bboxes) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m bboxes]\n\u001b[1;32m    673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(overlaps) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    674\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[39m# Issue on macos backend raises an error in the above code\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/transforms.py:605\u001b[0m, in \u001b[0;36mBboxBase.count_overlaps\u001b[0;34m(self, bboxes)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount_overlaps\u001b[39m(\u001b[39mself\u001b[39m, bboxes):\n\u001b[1;32m    597\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m    Count the number of bounding boxes that overlap this one.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39m    bboxes : sequence of `.BboxBase`\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m     \u001b[39mreturn\u001b[39;00m count_bboxes_overlapping_bbox(\n\u001b[0;32m--> 605\u001b[0m         \u001b[39mself\u001b[39m, np\u001b[39m.\u001b[39matleast_3d([np\u001b[39m.\u001b[39;49marray(x) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m bboxes]))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/transforms.py:605\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount_overlaps\u001b[39m(\u001b[39mself\u001b[39m, bboxes):\n\u001b[1;32m    597\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m    Count the number of bounding boxes that overlap this one.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39m    bboxes : sequence of `.BboxBase`\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m     \u001b[39mreturn\u001b[39;00m count_bboxes_overlapping_bbox(\n\u001b[0;32m--> 605\u001b[0m         \u001b[39mself\u001b[39m, np\u001b[39m.\u001b[39matleast_3d([np\u001b[39m.\u001b[39marray(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m bboxes]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 24499 (\\N{CJK UNIFIED IDEOGRAPH-5FB3}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 24029 (\\N{CJK UNIFIED IDEOGRAPH-5DDD}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 23567 (\\N{CJK UNIFIED IDEOGRAPH-5C0F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 26519 (\\N{CJK UNIFIED IDEOGRAPH-6797}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 26666 (\\N{CJK UNIFIED IDEOGRAPH-682A}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 24335 (\\N{CJK UNIFIED IDEOGRAPH-5F0F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 20250 (\\N{CJK UNIFIED IDEOGRAPH-4F1A}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 31038 (\\N{CJK UNIFIED IDEOGRAPH-793E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 30000 (\\N{CJK UNIFIED IDEOGRAPH-7530}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 20013 (\\N{CJK UNIFIED IDEOGRAPH-4E2D}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 37428 (\\N{CJK UNIFIED IDEOGRAPH-9234}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 26408 (\\N{CJK UNIFIED IDEOGRAPH-6728}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 26494 (\\N{CJK UNIFIED IDEOGRAPH-677E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 26449 (\\N{CJK UNIFIED IDEOGRAPH-6751}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 20234 (\\N{CJK UNIFIED IDEOGRAPH-4F0A}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 34276 (\\N{CJK UNIFIED IDEOGRAPH-85E4}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 20304 (\\N{CJK UNIFIED IDEOGRAPH-4F50}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 39640 (\\N{CJK UNIFIED IDEOGRAPH-9AD8}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 27211 (\\N{CJK UNIFIED IDEOGRAPH-6A4B}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 1729 (\\N{ARABIC LETTER HEH GOAL}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Matplotlib currently does not support Arabic natively.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 2451 (\\N{BENGALI LETTER O}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Matplotlib currently does not support Bengali natively.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54788 (\\N{HANGUL SYLLABLE HYEON}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 45824 (\\N{HANGUL SYLLABLE DAE}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 21127 (\\N{CJK UNIFIED IDEOGRAPH-5287}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 22580 (\\N{CJK UNIFIED IDEOGRAPH-5834}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 29256 (\\N{CJK UNIFIED IDEOGRAPH-7248}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12489 (\\N{KATAKANA LETTER DO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12521 (\\N{KATAKANA LETTER RA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12468 (\\N{KATAKANA LETTER GO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12531 (\\N{KATAKANA LETTER N}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12508 (\\N{KATAKANA LETTER BO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12540 (\\N{KATAKANA-HIRAGANA PROLONGED SOUND MARK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12523 (\\N{KATAKANA LETTER RU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 30007 (\\N{CJK UNIFIED IDEOGRAPH-7537}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12399 (\\N{HIRAGANA LETTER HA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12388 (\\N{HIRAGANA LETTER TU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12425 (\\N{HIRAGANA LETTER RA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12356 (\\N{HIRAGANA LETTER I}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/revanthgundam/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12424 (\\N{HIRAGANA LETTER YO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_tfidf_impact(model, test_data, test_tokens, true_labels, predictions, index, vocab_dict):\n",
    "    # Get the input sequence and true/predicted labels\n",
    "    input_sequence = test_data[index]\n",
    "    true_label = true_labels[index]\n",
    "    predicted_label = predictions[index]\n",
    "\n",
    "    # Get the TF-IDF representation of the input sequence\n",
    "    tfidf_representation = calculate_tfidf([test_tokens[index]], vocab_dict)\n",
    "\n",
    "    # Convert TF-IDF representation to a tensor\n",
    "    tfidf_input = torch.tensor(tfidf_representation, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Get the output probabilities from the model\n",
    "    output_probs = model(tfidf_input).softmax(dim=1).detach().cpu().numpy()\n",
    "\n",
    "    # Plot the impact of TF-IDF on classification\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot TF-IDF values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(tfidf_representation, cmap=\"YlGnBu\", annot=True, xticklabels=list(vocab_dict.keys()), yticklabels=['TF-IDF'])\n",
    "    plt.title('TF-IDF Representation')\n",
    "    \n",
    "    # Plot predicted probabilities\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(output_probs.shape[1]), output_probs.squeeze(), color='green', alpha=0.7)\n",
    "    plt.title(f'Predicted Probabilities (True: {true_label}, Predicted: {predicted_label})')\n",
    "    plt.xlabel('Class Index')\n",
    "    plt.ylabel('Probability')\n",
    "\n",
    "    plt.suptitle('Impact of TF-IDF on Classification')\n",
    "    plt.show()\n",
    "\n",
    "# Choose an index of an example\n",
    "misclassified_index = misclassified[0]\n",
    "\n",
    "# Visualize the impact of TF-IDF on classification for the selected example\n",
    "visualize_tfidf_impact(model, test_data, test_tokens, true_labels, predictions, misclassified_index, vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.linear.weight.detach().cpu().numpy()\n",
    "\n",
    "# Match each weight with its corresponding word in the vocabulary\n",
    "word_weights = dict(zip(vocab, weights[0]))\n",
    "\n",
    "sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 10 words\n",
    "for word, weight in sorted_words[:10]:\n",
    "    print(f'Word: {word:10} | Weight: {weight:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
