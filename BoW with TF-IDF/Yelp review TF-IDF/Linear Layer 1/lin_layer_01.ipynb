{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "test_data = test_dataset['text']\n",
    "test_labels = test_dataset['label']\n",
    "\n",
    "# Convert labels to lists (optional)\n",
    "# train_labels = train_labels.tolist()\n",
    "# test_labels = test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re  # Import the regular expressions module\n",
    "\n",
    "# # use tokenizer to remove punctuation\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     # Replace \"!\" with \"exm\" using regular expressions\n",
    "#     text = re.sub(r'!', ' exm', text)\n",
    "    \n",
    "#     expanded_words = []\n",
    "#     for word in text.split():\n",
    "#         # using contractions.fix to expand the shortened words\n",
    "#         expanded_words.append(contractions.fix(word))   \n",
    "    \n",
    "#     expanded_text = ' '.join(expanded_words)\n",
    "#     # print(expanded_text)\n",
    "    \n",
    "#     text = expanded_text    \n",
    "    \n",
    "#     # replace a-b with a and b\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     # temp = [word if (len(word) > 1 and word.isupper()) else None for word in tokens]\n",
    "#     temp = []\n",
    "#     for word in tokens:\n",
    "#         if len(word) > 1 and word.isupper():\n",
    "#             temp.append(word)\n",
    "#     tokens.extend(x for x in temp if x)\n",
    "    \n",
    "#     # convert to lower case\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "#     # dr. = dr and st. = st and so on\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # remove punctuation\n",
    "#     # tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# # print(train_data[4])\n",
    "# # tokenize_text(train_data[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('yelp_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('yelp_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45287\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_train_tokens = train_tokens\n",
    "\n",
    "# # Build the BoW representation manually\n",
    "# # Create a vocabulary by collecting unique words from the training data\n",
    "# vocab_reduced = set()\n",
    "# for tokens in reduced_train_tokens:\n",
    "#     vocab_reduced.update(tokens)\n",
    "\n",
    "# # Create a dictionary to map words to indices in the vocabulary\n",
    "# vocab_dict_reduced = {}\n",
    "# for i, word in enumerate(vocab_reduced):\n",
    "#     vocab_dict_reduced[word] = i\n",
    "\n",
    "# print(len(vocab_dict_reduced))\n",
    "\n",
    "# train_bow = np.zeros((len(reduced_train_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow train\")\n",
    "# # Convert text to BoW vectors\n",
    "# for i, tokens in enumerate(reduced_train_tokens):\n",
    "#     if i % 5000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         train_bow[i][vocab_dict_reduced[token]] += 1\n",
    "\n",
    "\n",
    "\n",
    "# reduced_test_tokens = test_tokens[:5000]\n",
    "# test_bow = np.zeros((len(reduced_test_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow test\")\n",
    "\n",
    "# for i, tokens in enumerate(reduced_test_tokens):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         if token in vocab_dict_reduced:\n",
    "#             test_bow[i][vocab_dict_reduced[token]] += 1\n",
    "#         else:    # if there is an unknown word, add it to the UNK column \n",
    "#             test_bow[i][vocab_dict_reduced['UNK']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 4\n",
    "learning_rate = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 5  # Output size is 5 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def calculate_tfidf(tokens, vocab_dict):\n",
    "\n",
    "    n_docs = len(tokens)\n",
    "    vocab_size = len(vocab_dict)\n",
    "    \n",
    "    # Calculate document frequencies (DF) for each word\n",
    "    doc_freq = Counter()\n",
    "    for doc in tokens:\n",
    "        doc_freq.update(set(doc))\n",
    "    \n",
    "    # Precompute IDF values\n",
    "    idf_dict = {}\n",
    "    for word, count in doc_freq.items():\n",
    "        idf_dict[word] = log(n_docs / (count + 1))  # Add 1 to avoid division by zero\n",
    "    \n",
    "    tfidf_matrix = np.zeros((n_docs, vocab_size))\n",
    "    \n",
    "    for i, doc in enumerate(tokens):\n",
    "        # if i % 1000 == 0:\n",
    "        #     print(i)\n",
    "        \n",
    "        total_words_in_doc = len(doc)\n",
    "        term_freq = Counter(doc)  # Calculate term frequency (TF) for the document\n",
    "        \n",
    "        for word, tf in term_freq.items():\n",
    "            if word in vocab_dict:\n",
    "                tfidf_matrix[i][vocab_dict[word]] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "            else:\n",
    "                # Use 'UNK' if the word is not in the vocabulary\n",
    "                tfidf_matrix[i][vocab_dict['UNK']] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Batch [1], Loss: 1.6093493700027466\n",
      "Epoch [1/4], Batch [2], Loss: 1.6014455556869507\n",
      "Epoch [1/4], Batch [3], Loss: 1.5965968370437622\n",
      "Epoch [1/4], Batch [4], Loss: 1.5862528085708618\n",
      "Epoch [1/4], Batch [5], Loss: 1.5796576738357544\n",
      "Epoch [1/4], Batch [6], Loss: 1.5670703649520874\n",
      "Epoch [1/4], Batch [7], Loss: 1.5609441995620728\n",
      "Epoch [1/4], Batch [8], Loss: 1.5536936521530151\n",
      "Epoch [1/4], Batch [9], Loss: 1.552130937576294\n",
      "Epoch [1/4], Batch [10], Loss: 1.5381931066513062\n",
      "Epoch [1/4], Batch [11], Loss: 1.5426872968673706\n",
      "Epoch [1/4], Batch [12], Loss: 1.5283254384994507\n",
      "Epoch [1/4], Batch [13], Loss: 1.5296149253845215\n",
      "Epoch [1/4], Batch [14], Loss: 1.5211189985275269\n",
      "Epoch [1/4], Batch [15], Loss: 1.5048354864120483\n",
      "Epoch [1/4], Batch [16], Loss: 1.5399950742721558\n",
      "Epoch [1/4], Batch [17], Loss: 1.5265430212020874\n",
      "Epoch [1/4], Batch [18], Loss: 1.5238316059112549\n",
      "Epoch [1/4], Batch [19], Loss: 1.5263962745666504\n",
      "Epoch [1/4], Batch [20], Loss: 1.5102314949035645\n",
      "Epoch [1/4], Batch [21], Loss: 1.502278208732605\n",
      "Epoch [1/4], Batch [22], Loss: 1.5039066076278687\n",
      "Epoch [1/4], Batch [23], Loss: 1.4939318895339966\n",
      "Epoch [1/4], Batch [24], Loss: 1.4763391017913818\n",
      "Epoch [1/4], Batch [25], Loss: 1.4441813230514526\n",
      "Epoch [1/4], Batch [26], Loss: 1.4390666484832764\n",
      "Epoch [1/4], Batch [27], Loss: 1.4405511617660522\n",
      "Epoch [1/4], Batch [28], Loss: 1.4598432779312134\n",
      "Epoch [1/4], Batch [29], Loss: 1.459486722946167\n",
      "Epoch [1/4], Batch [30], Loss: 1.460471272468567\n",
      "Epoch [1/4], Batch [31], Loss: 1.4430737495422363\n",
      "Epoch [1/4], Batch [32], Loss: 1.4363435506820679\n",
      "Epoch [1/4], Batch [33], Loss: 1.4419766664505005\n",
      "Epoch [1/4], Batch [34], Loss: 1.4471805095672607\n",
      "Epoch [1/4], Batch [35], Loss: 1.4218626022338867\n",
      "Epoch [1/4], Batch [36], Loss: 1.435215711593628\n",
      "Epoch [1/4], Batch [37], Loss: 1.427420735359192\n",
      "Epoch [1/4], Batch [38], Loss: 1.4262783527374268\n",
      "Epoch [1/4], Batch [39], Loss: 1.4108062982559204\n",
      "Epoch [1/4], Batch [40], Loss: 1.4081252813339233\n",
      "Epoch [1/4], Batch [41], Loss: 1.3908562660217285\n",
      "Epoch [1/4], Batch [42], Loss: 1.3917967081069946\n",
      "Epoch [1/4], Batch [43], Loss: 1.3775572776794434\n",
      "Epoch [1/4], Batch [44], Loss: 1.3414092063903809\n",
      "Epoch [1/4], Batch [45], Loss: 1.3585861921310425\n",
      "Epoch [1/4], Batch [46], Loss: 1.3955563306808472\n",
      "Epoch [1/4], Batch [47], Loss: 1.393206238746643\n",
      "Epoch [1/4], Batch [48], Loss: 1.362899661064148\n",
      "Epoch [1/4], Batch [49], Loss: 1.3585644960403442\n",
      "Epoch [1/4], Batch [50], Loss: 1.3856306076049805\n",
      "Epoch [1/4], Batch [51], Loss: 1.3514589071273804\n",
      "Epoch [1/4], Batch [52], Loss: 1.338318943977356\n",
      "Epoch [1/4], Batch [53], Loss: 1.333371877670288\n",
      "Epoch [1/4], Batch [54], Loss: 1.3342865705490112\n",
      "Epoch [1/4], Batch [55], Loss: 1.3253388404846191\n",
      "Epoch [1/4], Batch [56], Loss: 1.3188340663909912\n",
      "Epoch [1/4], Batch [57], Loss: 1.308005928993225\n",
      "Epoch [1/4], Batch [58], Loss: 1.3206405639648438\n",
      "Epoch [1/4], Batch [59], Loss: 1.310420036315918\n",
      "Epoch [1/4], Batch [60], Loss: 1.2986118793487549\n",
      "Epoch [1/4], Batch [61], Loss: 1.2759966850280762\n",
      "Epoch [1/4], Batch [62], Loss: 1.2714483737945557\n",
      "Epoch [1/4], Batch [63], Loss: 1.2738311290740967\n",
      "Epoch [1/4], Batch [64], Loss: 1.2617759704589844\n",
      "Epoch [1/4], Batch [65], Loss: 1.256807804107666\n",
      "Accuracy of the network on the 650000 train inputs: 46.32830769230769 %\n",
      "Epoch [2/4], Batch [1], Loss: 1.3138772249221802\n",
      "Epoch [2/4], Batch [2], Loss: 1.3096535205841064\n",
      "Epoch [2/4], Batch [3], Loss: 1.2728395462036133\n",
      "Epoch [2/4], Batch [4], Loss: 1.2735614776611328\n",
      "Epoch [2/4], Batch [5], Loss: 1.2652499675750732\n",
      "Epoch [2/4], Batch [6], Loss: 1.2671819925308228\n",
      "Epoch [2/4], Batch [7], Loss: 1.2545071840286255\n",
      "Epoch [2/4], Batch [8], Loss: 1.2807354927062988\n",
      "Epoch [2/4], Batch [9], Loss: 1.294307827949524\n",
      "Epoch [2/4], Batch [10], Loss: 1.25290048122406\n",
      "Epoch [2/4], Batch [11], Loss: 1.2731434106826782\n",
      "Epoch [2/4], Batch [12], Loss: 1.263359785079956\n",
      "Epoch [2/4], Batch [13], Loss: 1.2973496913909912\n",
      "Epoch [2/4], Batch [14], Loss: 1.2831870317459106\n",
      "Epoch [2/4], Batch [15], Loss: 1.240339994430542\n",
      "Epoch [2/4], Batch [16], Loss: 1.319960355758667\n",
      "Epoch [2/4], Batch [17], Loss: 1.3223748207092285\n",
      "Epoch [2/4], Batch [18], Loss: 1.31869637966156\n",
      "Epoch [2/4], Batch [19], Loss: 1.3134300708770752\n",
      "Epoch [2/4], Batch [20], Loss: 1.29917573928833\n",
      "Epoch [2/4], Batch [21], Loss: 1.2831079959869385\n",
      "Epoch [2/4], Batch [22], Loss: 1.2914221286773682\n",
      "Epoch [2/4], Batch [23], Loss: 1.2791383266448975\n",
      "Epoch [2/4], Batch [24], Loss: 1.2687444686889648\n",
      "Epoch [2/4], Batch [25], Loss: 1.1975265741348267\n",
      "Epoch [2/4], Batch [26], Loss: 1.193390130996704\n",
      "Epoch [2/4], Batch [27], Loss: 1.202033519744873\n",
      "Epoch [2/4], Batch [28], Loss: 1.2596324682235718\n",
      "Epoch [2/4], Batch [29], Loss: 1.2639715671539307\n",
      "Epoch [2/4], Batch [30], Loss: 1.2615364789962769\n",
      "Epoch [2/4], Batch [31], Loss: 1.2463746070861816\n",
      "Epoch [2/4], Batch [32], Loss: 1.2388075590133667\n",
      "Epoch [2/4], Batch [33], Loss: 1.2513022422790527\n",
      "Epoch [2/4], Batch [34], Loss: 1.2688379287719727\n",
      "Epoch [2/4], Batch [35], Loss: 1.2252150774002075\n",
      "Epoch [2/4], Batch [36], Loss: 1.254886269569397\n",
      "Epoch [2/4], Batch [37], Loss: 1.2432568073272705\n",
      "Epoch [2/4], Batch [38], Loss: 1.2474701404571533\n",
      "Epoch [2/4], Batch [39], Loss: 1.2271935939788818\n",
      "Epoch [2/4], Batch [40], Loss: 1.2263559103012085\n",
      "Epoch [2/4], Batch [41], Loss: 1.203831434249878\n",
      "Epoch [2/4], Batch [42], Loss: 1.2087175846099854\n",
      "Epoch [2/4], Batch [43], Loss: 1.1921404600143433\n",
      "Epoch [2/4], Batch [44], Loss: 1.1281144618988037\n",
      "Epoch [2/4], Batch [45], Loss: 1.1666908264160156\n",
      "Epoch [2/4], Batch [46], Loss: 1.2307758331298828\n",
      "Epoch [2/4], Batch [47], Loss: 1.2274136543273926\n",
      "Epoch [2/4], Batch [48], Loss: 1.1876994371414185\n",
      "Epoch [2/4], Batch [49], Loss: 1.1841424703598022\n",
      "Epoch [2/4], Batch [50], Loss: 1.233775019645691\n",
      "Epoch [2/4], Batch [51], Loss: 1.1832146644592285\n",
      "Epoch [2/4], Batch [52], Loss: 1.1643732786178589\n",
      "Epoch [2/4], Batch [53], Loss: 1.1631557941436768\n",
      "Epoch [2/4], Batch [54], Loss: 1.1684271097183228\n",
      "Epoch [2/4], Batch [55], Loss: 1.1601327657699585\n",
      "Epoch [2/4], Batch [56], Loss: 1.1534982919692993\n",
      "Epoch [2/4], Batch [57], Loss: 1.1428457498550415\n",
      "Epoch [2/4], Batch [58], Loss: 1.1629736423492432\n",
      "Epoch [2/4], Batch [59], Loss: 1.1520708799362183\n",
      "Epoch [2/4], Batch [60], Loss: 1.14296293258667\n",
      "Epoch [2/4], Batch [61], Loss: 1.1155427694320679\n",
      "Epoch [2/4], Batch [62], Loss: 1.114578127861023\n",
      "Epoch [2/4], Batch [63], Loss: 1.1204407215118408\n",
      "Epoch [2/4], Batch [64], Loss: 1.1089332103729248\n",
      "Epoch [2/4], Batch [65], Loss: 1.1041375398635864\n",
      "Accuracy of the network on the 650000 train inputs: 54.64507692307692 %\n",
      "Epoch [3/4], Batch [1], Loss: 1.1821707487106323\n",
      "Epoch [3/4], Batch [2], Loss: 1.1826773881912231\n",
      "Epoch [3/4], Batch [3], Loss: 1.1429084539413452\n",
      "Epoch [3/4], Batch [4], Loss: 1.1436349153518677\n",
      "Epoch [3/4], Batch [5], Loss: 1.1344739198684692\n",
      "Epoch [3/4], Batch [6], Loss: 1.141966462135315\n",
      "Epoch [3/4], Batch [7], Loss: 1.1265007257461548\n",
      "Epoch [3/4], Batch [8], Loss: 1.1573123931884766\n",
      "Epoch [3/4], Batch [9], Loss: 1.1733825206756592\n",
      "Epoch [3/4], Batch [10], Loss: 1.1350936889648438\n",
      "Epoch [3/4], Batch [11], Loss: 1.1552644968032837\n",
      "Epoch [3/4], Batch [12], Loss: 1.1457325220108032\n",
      "Epoch [3/4], Batch [13], Loss: 1.187571406364441\n",
      "Epoch [3/4], Batch [14], Loss: 1.1743284463882446\n",
      "Epoch [3/4], Batch [15], Loss: 1.1274524927139282\n",
      "Epoch [3/4], Batch [16], Loss: 1.2159528732299805\n",
      "Epoch [3/4], Batch [17], Loss: 1.2191003561019897\n",
      "Epoch [3/4], Batch [18], Loss: 1.216831088066101\n",
      "Epoch [3/4], Batch [19], Loss: 1.2130484580993652\n",
      "Epoch [3/4], Batch [20], Loss: 1.1988298892974854\n",
      "Epoch [3/4], Batch [21], Loss: 1.1805753707885742\n",
      "Epoch [3/4], Batch [22], Loss: 1.1914050579071045\n",
      "Epoch [3/4], Batch [23], Loss: 1.178489327430725\n",
      "Epoch [3/4], Batch [24], Loss: 1.1725916862487793\n",
      "Epoch [3/4], Batch [25], Loss: 1.0922578573226929\n",
      "Epoch [3/4], Batch [26], Loss: 1.0882588624954224\n",
      "Epoch [3/4], Batch [27], Loss: 1.0982966423034668\n",
      "Epoch [3/4], Batch [28], Loss: 1.1654468774795532\n",
      "Epoch [3/4], Batch [29], Loss: 1.1700408458709717\n",
      "Epoch [3/4], Batch [30], Loss: 1.1643474102020264\n",
      "Epoch [3/4], Batch [31], Loss: 1.1534000635147095\n",
      "Epoch [3/4], Batch [32], Loss: 1.1461433172225952\n",
      "Epoch [3/4], Batch [33], Loss: 1.1591585874557495\n",
      "Epoch [3/4], Batch [34], Loss: 1.1811567544937134\n",
      "Epoch [3/4], Batch [35], Loss: 1.1311273574829102\n",
      "Epoch [3/4], Batch [36], Loss: 1.1653848886489868\n",
      "Epoch [3/4], Batch [37], Loss: 1.1510344743728638\n",
      "Epoch [3/4], Batch [38], Loss: 1.1584676504135132\n",
      "Epoch [3/4], Batch [39], Loss: 1.1384286880493164\n",
      "Epoch [3/4], Batch [40], Loss: 1.1369293928146362\n",
      "Epoch [3/4], Batch [41], Loss: 1.1144603490829468\n",
      "Epoch [3/4], Batch [42], Loss: 1.1201833486557007\n",
      "Epoch [3/4], Batch [43], Loss: 1.103962779045105\n",
      "Epoch [3/4], Batch [44], Loss: 1.0349512100219727\n",
      "Epoch [3/4], Batch [45], Loss: 1.076035499572754\n",
      "Epoch [3/4], Batch [46], Loss: 1.146247148513794\n",
      "Epoch [3/4], Batch [47], Loss: 1.1420010328292847\n",
      "Epoch [3/4], Batch [48], Loss: 1.101226806640625\n",
      "Epoch [3/4], Batch [49], Loss: 1.098054051399231\n",
      "Epoch [3/4], Batch [50], Loss: 1.155272126197815\n",
      "Epoch [3/4], Batch [51], Loss: 1.099957823753357\n",
      "Epoch [3/4], Batch [52], Loss: 1.078007459640503\n",
      "Epoch [3/4], Batch [53], Loss: 1.0790555477142334\n",
      "Epoch [3/4], Batch [54], Loss: 1.085495114326477\n",
      "Epoch [3/4], Batch [55], Loss: 1.0775729417800903\n",
      "Epoch [3/4], Batch [56], Loss: 1.0701595544815063\n",
      "Epoch [3/4], Batch [57], Loss: 1.0596041679382324\n",
      "Epoch [3/4], Batch [58], Loss: 1.0816535949707031\n",
      "Epoch [3/4], Batch [59], Loss: 1.071189045906067\n",
      "Epoch [3/4], Batch [60], Loss: 1.0636955499649048\n",
      "Epoch [3/4], Batch [61], Loss: 1.034173607826233\n",
      "Epoch [3/4], Batch [62], Loss: 1.0344561338424683\n",
      "Epoch [3/4], Batch [63], Loss: 1.0416369438171387\n",
      "Epoch [3/4], Batch [64], Loss: 1.0301004648208618\n",
      "Epoch [3/4], Batch [65], Loss: 1.02407705783844\n",
      "Accuracy of the network on the 650000 train inputs: 57.33292307692308 %\n",
      "Epoch [4/4], Batch [1], Loss: 1.1111215353012085\n",
      "Epoch [4/4], Batch [2], Loss: 1.1139346361160278\n",
      "Epoch [4/4], Batch [3], Loss: 1.0731985569000244\n",
      "Epoch [4/4], Batch [4], Loss: 1.0745272636413574\n",
      "Epoch [4/4], Batch [5], Loss: 1.0641385316848755\n",
      "Epoch [4/4], Batch [6], Loss: 1.0750324726104736\n",
      "Epoch [4/4], Batch [7], Loss: 1.057269811630249\n",
      "Epoch [4/4], Batch [8], Loss: 1.089128017425537\n",
      "Epoch [4/4], Batch [9], Loss: 1.1056644916534424\n",
      "Epoch [4/4], Batch [10], Loss: 1.0710687637329102\n",
      "Epoch [4/4], Batch [11], Loss: 1.0892248153686523\n",
      "Epoch [4/4], Batch [12], Loss: 1.0802181959152222\n",
      "Epoch [4/4], Batch [13], Loss: 1.125260591506958\n",
      "Epoch [4/4], Batch [14], Loss: 1.1116081476211548\n",
      "Epoch [4/4], Batch [15], Loss: 1.0644748210906982\n",
      "Epoch [4/4], Batch [16], Loss: 1.155372142791748\n",
      "Epoch [4/4], Batch [17], Loss: 1.158206820487976\n",
      "Epoch [4/4], Batch [18], Loss: 1.1559199094772339\n",
      "Epoch [4/4], Batch [19], Loss: 1.1539400815963745\n",
      "Epoch [4/4], Batch [20], Loss: 1.1396695375442505\n",
      "Epoch [4/4], Batch [21], Loss: 1.1206903457641602\n",
      "Epoch [4/4], Batch [22], Loss: 1.1324622631072998\n",
      "Epoch [4/4], Batch [23], Loss: 1.1194648742675781\n",
      "Epoch [4/4], Batch [24], Loss: 1.1160023212432861\n",
      "Epoch [4/4], Batch [25], Loss: 1.0336387157440186\n",
      "Epoch [4/4], Batch [26], Loss: 1.0297293663024902\n",
      "Epoch [4/4], Batch [27], Loss: 1.0404390096664429\n",
      "Epoch [4/4], Batch [28], Loss: 1.110033631324768\n",
      "Epoch [4/4], Batch [29], Loss: 1.114165186882019\n",
      "Epoch [4/4], Batch [30], Loss: 1.1064478158950806\n",
      "Epoch [4/4], Batch [31], Loss: 1.098013997077942\n",
      "Epoch [4/4], Batch [32], Loss: 1.0913479328155518\n",
      "Epoch [4/4], Batch [33], Loss: 1.1040464639663696\n",
      "Epoch [4/4], Batch [34], Loss: 1.1267226934432983\n",
      "Epoch [4/4], Batch [35], Loss: 1.0752121210098267\n",
      "Epoch [4/4], Batch [36], Loss: 1.110426902770996\n",
      "Epoch [4/4], Batch [37], Loss: 1.095133662223816\n",
      "Epoch [4/4], Batch [38], Loss: 1.1045740842819214\n",
      "Epoch [4/4], Batch [39], Loss: 1.0851962566375732\n",
      "Epoch [4/4], Batch [40], Loss: 1.0824456214904785\n",
      "Epoch [4/4], Batch [41], Loss: 1.0608357191085815\n",
      "Epoch [4/4], Batch [42], Loss: 1.0669145584106445\n",
      "Epoch [4/4], Batch [43], Loss: 1.0513713359832764\n",
      "Epoch [4/4], Batch [44], Loss: 0.9819655418395996\n",
      "Epoch [4/4], Batch [45], Loss: 1.0219545364379883\n",
      "Epoch [4/4], Batch [46], Loss: 1.0936907529830933\n",
      "Epoch [4/4], Batch [47], Loss: 1.0881381034851074\n",
      "Epoch [4/4], Batch [48], Loss: 1.0481526851654053\n",
      "Epoch [4/4], Batch [49], Loss: 1.0457258224487305\n",
      "Epoch [4/4], Batch [50], Loss: 1.106001615524292\n",
      "Epoch [4/4], Batch [51], Loss: 1.0492037534713745\n",
      "Epoch [4/4], Batch [52], Loss: 1.0252902507781982\n",
      "Epoch [4/4], Batch [53], Loss: 1.0275784730911255\n",
      "Epoch [4/4], Batch [54], Loss: 1.0346442461013794\n",
      "Epoch [4/4], Batch [55], Loss: 1.0267581939697266\n",
      "Epoch [4/4], Batch [56], Loss: 1.0189701318740845\n",
      "Epoch [4/4], Batch [57], Loss: 1.008269190788269\n",
      "Epoch [4/4], Batch [58], Loss: 1.0308736562728882\n",
      "Epoch [4/4], Batch [59], Loss: 1.021061658859253\n",
      "Epoch [4/4], Batch [60], Loss: 1.0143753290176392\n",
      "Epoch [4/4], Batch [61], Loss: 0.9839591979980469\n",
      "Epoch [4/4], Batch [62], Loss: 0.9845923781394958\n",
      "Epoch [4/4], Batch [63], Loss: 0.9925581812858582\n",
      "Epoch [4/4], Batch [64], Loss: 0.9809631109237671\n",
      "Epoch [4/4], Batch [65], Loss: 0.9738015532493591\n",
      "Accuracy of the network on the 650000 train inputs: 58.84138461538461 %\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "        # print(len(inputs))\n",
    "        # print(len(vocab))\n",
    "        \n",
    "        # make bow vector for inputs\n",
    "        # bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "        # print(bow.shape)\n",
    "        # for j in range(batch_size):\n",
    "        #     for token in inputs[j]:\n",
    "\n",
    "        #         bow[j][vocab_dict[token]] += 1\n",
    "\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "        \n",
    "        # convert bow to tensor\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "        # Convert labels to LongTensors\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        # Print the loss for this batch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}')\n",
    "\n",
    "    print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the above model in its entirety in pkl file\n",
    "torch.save(model, 'bow_model.pkl')\n",
    "\n",
    "# load the model\n",
    "model = torch.load('bow_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50000 test inputs: 55.654 %\n",
      "Confusion Matrix:\n",
      "[[7396 1948  237   73  346]\n",
      " [2648 4757 1827  294  474]\n",
      " [ 838 2153 4189 1768 1052]\n",
      " [ 321  473 1719 3886 3601]\n",
      " [ 324  132  304 1641 7599]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69     10000\n",
      "           1       0.50      0.48      0.49     10000\n",
      "           2       0.51      0.42      0.46     10000\n",
      "           3       0.51      0.39      0.44     10000\n",
      "           4       0.58      0.76      0.66     10000\n",
      "\n",
      "    accuracy                           0.56     50000\n",
      "   macro avg       0.55      0.56      0.55     50000\n",
      "weighted avg       0.55      0.56      0.55     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(0, len(test_tokens), batch_size):\n",
    "        inputs = test_tokens[i:i + batch_size]\n",
    "        # bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "\n",
    "        # for j in range(batch_size):\n",
    "        #     for token in inputs[j]:\n",
    "        #         if token in vocab_dict:\n",
    "        #             bow[j][vocab_dict[token]] += 1\n",
    "        #         else:\n",
    "        #             bow[j][vocab_dict['UNK']] += 1\n",
    "\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        labels = test_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the {total} test inputs: {accuracy} %')\n",
    "\n",
    "    # Calculate confusion matrix, precision, recall, and F1 scores\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    classification_rep = classification_report(true_labels, predictions, target_names=[str(i) for i in range(5)])\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    # print(\"F1 Micro:\", f1_micro)\n",
    "    # print(\"F1 Macro:\", f1_macro)\n",
    "    # print(\"F1 Weighted:\", f1_weighted)\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
