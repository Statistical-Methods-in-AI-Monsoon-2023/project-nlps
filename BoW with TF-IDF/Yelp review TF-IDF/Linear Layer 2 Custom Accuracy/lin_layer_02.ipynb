{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "test_data = test_dataset['text']\n",
    "test_labels = test_dataset['label']\n",
    "\n",
    "# Convert labels to lists (optional)\n",
    "# train_labels = train_labels.tolist()\n",
    "# test_labels = test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re  # Import the regular expressions module\n",
    "\n",
    "# # use tokenizer to remove punctuation\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     # Replace \"!\" with \"exm\" using regular expressions\n",
    "#     text = re.sub(r'!', ' exm', text)\n",
    "    \n",
    "#     expanded_words = []\n",
    "#     for word in text.split():\n",
    "#         # using contractions.fix to expand the shortened words\n",
    "#         expanded_words.append(contractions.fix(word))   \n",
    "    \n",
    "#     expanded_text = ' '.join(expanded_words)\n",
    "#     # print(expanded_text)\n",
    "    \n",
    "#     text = expanded_text    \n",
    "    \n",
    "#     # replace a-b with a and b\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     # temp = [word if (len(word) > 1 and word.isupper()) else None for word in tokens]\n",
    "#     temp = []\n",
    "#     for word in tokens:\n",
    "#         if len(word) > 1 and word.isupper():\n",
    "#             temp.append(word)\n",
    "#     tokens.extend(x for x in temp if x)\n",
    "    \n",
    "#     # convert to lower case\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "#     # dr. = dr and st. = st and so on\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # remove punctuation\n",
    "#     # tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# # print(train_data[4])\n",
    "# # tokenize_text(train_data[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('yelp_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('yelp_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45287\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_train_tokens = train_tokens\n",
    "\n",
    "# # Build the BoW representation manually\n",
    "# # Create a vocabulary by collecting unique words from the training data\n",
    "# vocab_reduced = set()\n",
    "# for tokens in reduced_train_tokens:\n",
    "#     vocab_reduced.update(tokens)\n",
    "\n",
    "# # Create a dictionary to map words to indices in the vocabulary\n",
    "# vocab_dict_reduced = {}\n",
    "# for i, word in enumerate(vocab_reduced):\n",
    "#     vocab_dict_reduced[word] = i\n",
    "\n",
    "# print(len(vocab_dict_reduced))\n",
    "\n",
    "# train_bow = np.zeros((len(reduced_train_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow train\")\n",
    "# # Convert text to BoW vectors\n",
    "# for i, tokens in enumerate(reduced_train_tokens):\n",
    "#     if i % 5000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         train_bow[i][vocab_dict_reduced[token]] += 1\n",
    "\n",
    "\n",
    "\n",
    "# reduced_test_tokens = test_tokens[:5000]\n",
    "# test_bow = np.zeros((len(reduced_test_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow test\")\n",
    "\n",
    "# for i, tokens in enumerate(reduced_test_tokens):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         if token in vocab_dict_reduced:\n",
    "#             test_bow[i][vocab_dict_reduced[token]] += 1\n",
    "#         else:    # if there is an unknown word, add it to the UNK column \n",
    "#             test_bow[i][vocab_dict_reduced['UNK']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 4\n",
    "learning_rate = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 5  # Output size is 5 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def calculate_tfidf(tokens, vocab_dict):\n",
    "\n",
    "    n_docs = len(tokens)\n",
    "    vocab_size = len(vocab_dict)\n",
    "    \n",
    "    # Calculate document frequencies (DF) for each word\n",
    "    doc_freq = Counter()\n",
    "    for doc in tokens:\n",
    "        doc_freq.update(set(doc))\n",
    "    \n",
    "    # Precompute IDF values\n",
    "    idf_dict = {}\n",
    "    for word, count in doc_freq.items():\n",
    "        idf_dict[word] = log(n_docs / (count + 1))  # Add 1 to avoid division by zero\n",
    "    \n",
    "    tfidf_matrix = np.zeros((n_docs, vocab_size))\n",
    "    \n",
    "    for i, doc in enumerate(tokens):\n",
    "        # if i % 1000 == 0:\n",
    "        #     print(i)\n",
    "        \n",
    "        total_words_in_doc = len(doc)\n",
    "        term_freq = Counter(doc)  # Calculate term frequency (TF) for the document\n",
    "        \n",
    "        for word, tf in term_freq.items():\n",
    "            if word in vocab_dict:\n",
    "                tfidf_matrix[i][vocab_dict[word]] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "            else:\n",
    "                # Use 'UNK' if the word is not in the vocabulary\n",
    "                tfidf_matrix[i][vocab_dict['UNK']] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Batch [1], Loss: 1.6095019578933716, Accuracy: 59.525 %\n",
      "Epoch [1/4], Batch [2], Loss: 1.6015169620513916, Accuracy: 67.13375 %\n",
      "Epoch [1/4], Batch [3], Loss: 1.596676230430603, Accuracy: 69.76333333333334 %\n",
      "Epoch [1/4], Batch [4], Loss: 1.5864652395248413, Accuracy: 71.860625 %\n",
      "Epoch [1/4], Batch [5], Loss: 1.5796678066253662, Accuracy: 73.0135 %\n",
      "Epoch [1/4], Batch [6], Loss: 1.5672848224639893, Accuracy: 74.05291666666666 %\n",
      "Epoch [1/4], Batch [7], Loss: 1.5609720945358276, Accuracy: 74.55392857142857 %\n",
      "Epoch [1/4], Batch [8], Loss: 1.5537269115447998, Accuracy: 74.905625 %\n",
      "Epoch [1/4], Batch [9], Loss: 1.5520808696746826, Accuracy: 75.0525 %\n",
      "Epoch [1/4], Batch [10], Loss: 1.5382966995239258, Accuracy: 75.01125 %\n",
      "Epoch [1/4], Batch [11], Loss: 1.542673110961914, Accuracy: 74.92840909090908 %\n",
      "Epoch [1/4], Batch [12], Loss: 1.5284136533737183, Accuracy: 74.8725 %\n",
      "Epoch [1/4], Batch [13], Loss: 1.5295809507369995, Accuracy: 74.74423076923077 %\n",
      "Epoch [1/4], Batch [14], Loss: 1.5211021900177002, Accuracy: 74.68642857142858 %\n",
      "Epoch [1/4], Batch [15], Loss: 1.5049209594726562, Accuracy: 74.69816666666667 %\n",
      "Epoch [1/4], Batch [16], Loss: 1.539913296699524, Accuracy: 74.61640625 %\n",
      "Epoch [1/4], Batch [17], Loss: 1.5265214443206787, Accuracy: 74.50617647058823 %\n",
      "Epoch [1/4], Batch [18], Loss: 1.5236878395080566, Accuracy: 74.56930555555556 %\n",
      "Epoch [1/4], Batch [19], Loss: 1.5264140367507935, Accuracy: 74.5875 %\n",
      "Epoch [1/4], Batch [20], Loss: 1.510179877281189, Accuracy: 74.821625 %\n",
      "Epoch [1/4], Batch [21], Loss: 1.5022716522216797, Accuracy: 75.04869047619047 %\n",
      "Epoch [1/4], Batch [22], Loss: 1.5039281845092773, Accuracy: 75.27659090909091 %\n",
      "Epoch [1/4], Batch [23], Loss: 1.4939842224121094, Accuracy: 75.50239130434782 %\n",
      "Epoch [1/4], Batch [24], Loss: 1.4763516187667847, Accuracy: 75.76604166666667 %\n",
      "Epoch [1/4], Batch [25], Loss: 1.4442195892333984, Accuracy: 75.925 %\n",
      "Epoch [1/4], Batch [26], Loss: 1.4391218423843384, Accuracy: 76.07211538461539 %\n",
      "Epoch [1/4], Batch [27], Loss: 1.440600872039795, Accuracy: 76.23018518518519 %\n",
      "Epoch [1/4], Batch [28], Loss: 1.4599183797836304, Accuracy: 76.44383928571429 %\n",
      "Epoch [1/4], Batch [29], Loss: 1.4594320058822632, Accuracy: 76.6578448275862 %\n",
      "Epoch [1/4], Batch [30], Loss: 1.4604793787002563, Accuracy: 76.83633333333333 %\n",
      "Epoch [1/4], Batch [31], Loss: 1.443102240562439, Accuracy: 77.03685483870967 %\n",
      "Epoch [1/4], Batch [32], Loss: 1.436476469039917, Accuracy: 77.226953125 %\n",
      "Epoch [1/4], Batch [33], Loss: 1.4420926570892334, Accuracy: 77.38257575757575 %\n",
      "Epoch [1/4], Batch [34], Loss: 1.4471310377120972, Accuracy: 77.52720588235294 %\n",
      "Epoch [1/4], Batch [35], Loss: 1.4219305515289307, Accuracy: 77.69528571428572 %\n",
      "Epoch [1/4], Batch [36], Loss: 1.4353116750717163, Accuracy: 77.86069444444445 %\n",
      "Epoch [1/4], Batch [37], Loss: 1.4274210929870605, Accuracy: 77.9943918918919 %\n",
      "Epoch [1/4], Batch [38], Loss: 1.4262336492538452, Accuracy: 78.13013157894737 %\n",
      "Epoch [1/4], Batch [39], Loss: 1.4107789993286133, Accuracy: 78.27858974358975 %\n",
      "Epoch [1/4], Batch [40], Loss: 1.4080936908721924, Accuracy: 78.4118125 %\n",
      "Epoch [1/4], Batch [41], Loss: 1.3907463550567627, Accuracy: 78.54548780487805 %\n",
      "Epoch [1/4], Batch [42], Loss: 1.3917853832244873, Accuracy: 78.66375 %\n",
      "Epoch [1/4], Batch [43], Loss: 1.3775291442871094, Accuracy: 78.79191860465116 %\n",
      "Epoch [1/4], Batch [44], Loss: 1.3413512706756592, Accuracy: 78.8996590909091 %\n",
      "Epoch [1/4], Batch [45], Loss: 1.3586965799331665, Accuracy: 79.01361111111112 %\n",
      "Epoch [1/4], Batch [46], Loss: 1.3955680131912231, Accuracy: 79.11755434782609 %\n",
      "Epoch [1/4], Batch [47], Loss: 1.3932081460952759, Accuracy: 79.19675531914893 %\n",
      "Epoch [1/4], Batch [48], Loss: 1.36287260055542, Accuracy: 79.29354166666667 %\n",
      "Epoch [1/4], Batch [49], Loss: 1.358674168586731, Accuracy: 79.39423469387755 %\n",
      "Epoch [1/4], Batch [50], Loss: 1.3858137130737305, Accuracy: 79.47905 %\n",
      "Epoch [1/4], Batch [51], Loss: 1.3515591621398926, Accuracy: 79.57392156862745 %\n",
      "Epoch [1/4], Batch [52], Loss: 1.3382740020751953, Accuracy: 79.66480769230769 %\n",
      "Epoch [1/4], Batch [53], Loss: 1.3332840204238892, Accuracy: 79.75061320754718 %\n",
      "Epoch [1/4], Batch [54], Loss: 1.3342747688293457, Accuracy: 79.83773148148148 %\n",
      "Epoch [1/4], Batch [55], Loss: 1.325256586074829, Accuracy: 79.91640909090908 %\n",
      "Epoch [1/4], Batch [56], Loss: 1.3189160823822021, Accuracy: 79.98991071428571 %\n",
      "Epoch [1/4], Batch [57], Loss: 1.3080034255981445, Accuracy: 80.07013157894737 %\n",
      "Epoch [1/4], Batch [58], Loss: 1.3205857276916504, Accuracy: 80.13478448275862 %\n",
      "Epoch [1/4], Batch [59], Loss: 1.3104790449142456, Accuracy: 80.20555084745763 %\n",
      "Epoch [1/4], Batch [60], Loss: 1.2986533641815186, Accuracy: 80.277875 %\n",
      "Epoch [1/4], Batch [61], Loss: 1.2761002779006958, Accuracy: 80.35102459016393 %\n",
      "Epoch [1/4], Batch [62], Loss: 1.2716211080551147, Accuracy: 80.41580645161291 %\n",
      "Epoch [1/4], Batch [63], Loss: 1.2737399339675903, Accuracy: 80.48218253968254 %\n",
      "Epoch [1/4], Batch [64], Loss: 1.261828899383545, Accuracy: 80.53984375 %\n",
      "Epoch [1/4], Batch [65], Loss: 1.2568632364273071, Accuracy: 80.60007692307693 %\n",
      "Accuracy of the network on the 650000 train inputs: 80.60007692307693 %\n",
      "Epoch [2/4], Batch [1], Loss: 1.3138476610183716, Accuracy: 81.37 %\n",
      "Epoch [2/4], Batch [2], Loss: 1.3094708919525146, Accuracy: 81.08375 %\n",
      "Epoch [2/4], Batch [3], Loss: 1.2729384899139404, Accuracy: 81.21833333333333 %\n",
      "Epoch [2/4], Batch [4], Loss: 1.2736095190048218, Accuracy: 81.384375 %\n",
      "Epoch [2/4], Batch [5], Loss: 1.2652286291122437, Accuracy: 81.5075 %\n",
      "Epoch [2/4], Batch [6], Loss: 1.2672502994537354, Accuracy: 81.64375 %\n",
      "Epoch [2/4], Batch [7], Loss: 1.2544928789138794, Accuracy: 81.865 %\n",
      "Epoch [2/4], Batch [8], Loss: 1.2806676626205444, Accuracy: 81.9578125 %\n",
      "Epoch [2/4], Batch [9], Loss: 1.2943980693817139, Accuracy: 82.03527777777778 %\n",
      "Epoch [2/4], Batch [10], Loss: 1.2528735399246216, Accuracy: 82.13125 %\n",
      "Epoch [2/4], Batch [11], Loss: 1.2731728553771973, Accuracy: 82.25613636363636 %\n",
      "Epoch [2/4], Batch [12], Loss: 1.2634702920913696, Accuracy: 82.32125 %\n",
      "Epoch [2/4], Batch [13], Loss: 1.2973030805587769, Accuracy: 82.35403846153847 %\n",
      "Epoch [2/4], Batch [14], Loss: 1.2833555936813354, Accuracy: 82.445 %\n",
      "Epoch [2/4], Batch [15], Loss: 1.2404139041900635, Accuracy: 82.55883333333334 %\n",
      "Epoch [2/4], Batch [16], Loss: 1.3199762105941772, Accuracy: 82.62359375 %\n",
      "Epoch [2/4], Batch [17], Loss: 1.3223856687545776, Accuracy: 82.6654411764706 %\n",
      "Epoch [2/4], Batch [18], Loss: 1.3184752464294434, Accuracy: 82.74458333333334 %\n",
      "Epoch [2/4], Batch [19], Loss: 1.3134980201721191, Accuracy: 82.79144736842105 %\n",
      "Epoch [2/4], Batch [20], Loss: 1.2992960214614868, Accuracy: 82.89 %\n",
      "Epoch [2/4], Batch [21], Loss: 1.2830852270126343, Accuracy: 82.9752380952381 %\n",
      "Epoch [2/4], Batch [22], Loss: 1.2914509773254395, Accuracy: 83.05545454545455 %\n",
      "Epoch [2/4], Batch [23], Loss: 1.2791376113891602, Accuracy: 83.1270652173913 %\n",
      "Epoch [2/4], Batch [24], Loss: 1.2688264846801758, Accuracy: 83.20333333333333 %\n",
      "Epoch [2/4], Batch [25], Loss: 1.197609782218933, Accuracy: 83.2496 %\n",
      "Epoch [2/4], Batch [26], Loss: 1.193390965461731, Accuracy: 83.29384615384615 %\n",
      "Epoch [2/4], Batch [27], Loss: 1.2020903825759888, Accuracy: 83.34657407407407 %\n",
      "Epoch [2/4], Batch [28], Loss: 1.2597637176513672, Accuracy: 83.40857142857143 %\n",
      "Epoch [2/4], Batch [29], Loss: 1.263763666152954, Accuracy: 83.4598275862069 %\n",
      "Epoch [2/4], Batch [30], Loss: 1.2614887952804565, Accuracy: 83.50883333333333 %\n",
      "Epoch [2/4], Batch [31], Loss: 1.2463873624801636, Accuracy: 83.57 %\n",
      "Epoch [2/4], Batch [32], Loss: 1.2389904260635376, Accuracy: 83.623984375 %\n",
      "Epoch [2/4], Batch [33], Loss: 1.251441478729248, Accuracy: 83.66287878787878 %\n",
      "Epoch [2/4], Batch [34], Loss: 1.2687946557998657, Accuracy: 83.69139705882353 %\n",
      "Epoch [2/4], Batch [35], Loss: 1.225236415863037, Accuracy: 83.74164285714286 %\n",
      "Epoch [2/4], Batch [36], Loss: 1.2550336122512817, Accuracy: 83.78798611111111 %\n",
      "Epoch [2/4], Batch [37], Loss: 1.2431755065917969, Accuracy: 83.81925675675676 %\n",
      "Epoch [2/4], Batch [38], Loss: 1.2474182844161987, Accuracy: 83.8542105263158 %\n",
      "Epoch [2/4], Batch [39], Loss: 1.2270219326019287, Accuracy: 83.89897435897436 %\n",
      "Epoch [2/4], Batch [40], Loss: 1.2262468338012695, Accuracy: 83.9388125 %\n",
      "Epoch [2/4], Batch [41], Loss: 1.2035940885543823, Accuracy: 83.98420731707317 %\n",
      "Epoch [2/4], Batch [42], Loss: 1.2086293697357178, Accuracy: 84.01946428571429 %\n",
      "Epoch [2/4], Batch [43], Loss: 1.1920592784881592, Accuracy: 84.06505813953488 %\n",
      "Epoch [2/4], Batch [44], Loss: 1.1279935836791992, Accuracy: 84.10028409090908 %\n",
      "Epoch [2/4], Batch [45], Loss: 1.1667712926864624, Accuracy: 84.14583333333333 %\n",
      "Epoch [2/4], Batch [46], Loss: 1.2308502197265625, Accuracy: 84.16858695652174 %\n",
      "Epoch [2/4], Batch [47], Loss: 1.2274631261825562, Accuracy: 84.17856382978724 %\n",
      "Epoch [2/4], Batch [48], Loss: 1.1876810789108276, Accuracy: 84.20791666666666 %\n",
      "Epoch [2/4], Batch [49], Loss: 1.1843210458755493, Accuracy: 84.24163265306123 %\n",
      "Epoch [2/4], Batch [50], Loss: 1.2340587377548218, Accuracy: 84.25655 %\n",
      "Epoch [2/4], Batch [51], Loss: 1.1834006309509277, Accuracy: 84.2888725490196 %\n",
      "Epoch [2/4], Batch [52], Loss: 1.1643116474151611, Accuracy: 84.31980769230769 %\n",
      "Epoch [2/4], Batch [53], Loss: 1.1630295515060425, Accuracy: 84.34905660377359 %\n",
      "Epoch [2/4], Batch [54], Loss: 1.1683944463729858, Accuracy: 84.3776388888889 %\n",
      "Epoch [2/4], Batch [55], Loss: 1.1600582599639893, Accuracy: 84.4069090909091 %\n",
      "Epoch [2/4], Batch [56], Loss: 1.153634786605835, Accuracy: 84.43285714285715 %\n",
      "Epoch [2/4], Batch [57], Loss: 1.1428091526031494, Accuracy: 84.46666666666667 %\n",
      "Epoch [2/4], Batch [58], Loss: 1.1629408597946167, Accuracy: 84.48913793103448 %\n",
      "Epoch [2/4], Batch [59], Loss: 1.1520992517471313, Accuracy: 84.51741525423729 %\n",
      "Epoch [2/4], Batch [60], Loss: 1.1430343389511108, Accuracy: 84.54691666666666 %\n",
      "Epoch [2/4], Batch [61], Loss: 1.1157113313674927, Accuracy: 84.58069672131147 %\n",
      "Epoch [2/4], Batch [62], Loss: 1.1148571968078613, Accuracy: 84.61084677419355 %\n",
      "Epoch [2/4], Batch [63], Loss: 1.1202876567840576, Accuracy: 84.64261904761905 %\n",
      "Epoch [2/4], Batch [64], Loss: 1.10894775390625, Accuracy: 84.6739453125 %\n",
      "Epoch [2/4], Batch [65], Loss: 1.1041938066482544, Accuracy: 84.70726923076923 %\n",
      "Accuracy of the network on the 650000 train inputs: 84.70726923076923 %\n",
      "Epoch [3/4], Batch [1], Loss: 1.1821318864822388, Accuracy: 84.7375 %\n",
      "Epoch [3/4], Batch [2], Loss: 1.1824023723602295, Accuracy: 84.39125 %\n",
      "Epoch [3/4], Batch [3], Loss: 1.1429980993270874, Accuracy: 84.3575 %\n",
      "Epoch [3/4], Batch [4], Loss: 1.1436954736709595, Accuracy: 84.384375 %\n",
      "Epoch [3/4], Batch [5], Loss: 1.134430170059204, Accuracy: 84.443 %\n",
      "Epoch [3/4], Batch [6], Loss: 1.142073631286621, Accuracy: 84.47375 %\n",
      "Epoch [3/4], Batch [7], Loss: 1.126497507095337, Accuracy: 84.59214285714286 %\n",
      "Epoch [3/4], Batch [8], Loss: 1.157226324081421, Accuracy: 84.6403125 %\n",
      "Epoch [3/4], Batch [9], Loss: 1.1735183000564575, Accuracy: 84.64444444444445 %\n",
      "Epoch [3/4], Batch [10], Loss: 1.135033130645752, Accuracy: 84.65575 %\n",
      "Epoch [3/4], Batch [11], Loss: 1.1552705764770508, Accuracy: 84.69568181818182 %\n",
      "Epoch [3/4], Batch [12], Loss: 1.14583158493042, Accuracy: 84.71333333333334 %\n",
      "Epoch [3/4], Batch [13], Loss: 1.1875439882278442, Accuracy: 84.72596153846153 %\n",
      "Epoch [3/4], Batch [14], Loss: 1.174600601196289, Accuracy: 84.75821428571429 %\n",
      "Epoch [3/4], Batch [15], Loss: 1.1275372505187988, Accuracy: 84.80816666666666 %\n",
      "Epoch [3/4], Batch [16], Loss: 1.2160162925720215, Accuracy: 84.824375 %\n",
      "Epoch [3/4], Batch [17], Loss: 1.2191145420074463, Accuracy: 84.84367647058824 %\n",
      "Epoch [3/4], Batch [18], Loss: 1.216450810432434, Accuracy: 84.88055555555556 %\n",
      "Epoch [3/4], Batch [19], Loss: 1.2131210565567017, Accuracy: 84.8875 %\n",
      "Epoch [3/4], Batch [20], Loss: 1.1990123987197876, Accuracy: 84.940125 %\n",
      "Epoch [3/4], Batch [21], Loss: 1.1805533170700073, Accuracy: 84.98607142857144 %\n",
      "Epoch [3/4], Batch [22], Loss: 1.1914005279541016, Accuracy: 85.02386363636364 %\n",
      "Epoch [3/4], Batch [23], Loss: 1.1784553527832031, Accuracy: 85.05521739130435 %\n",
      "Epoch [3/4], Batch [24], Loss: 1.1727004051208496, Accuracy: 85.08677083333333 %\n",
      "Epoch [3/4], Batch [25], Loss: 1.0923690795898438, Accuracy: 85.122 %\n",
      "Epoch [3/4], Batch [26], Loss: 1.088222861289978, Accuracy: 85.155 %\n",
      "Epoch [3/4], Batch [27], Loss: 1.0983226299285889, Accuracy: 85.1850925925926 %\n",
      "Epoch [3/4], Batch [28], Loss: 1.1655782461166382, Accuracy: 85.21785714285714 %\n",
      "Epoch [3/4], Batch [29], Loss: 1.1697429418563843, Accuracy: 85.23870689655172 %\n",
      "Epoch [3/4], Batch [30], Loss: 1.1642720699310303, Accuracy: 85.25741666666667 %\n",
      "Epoch [3/4], Batch [31], Loss: 1.1534219980239868, Accuracy: 85.28983870967743 %\n",
      "Epoch [3/4], Batch [32], Loss: 1.1463466882705688, Accuracy: 85.316015625 %\n",
      "Epoch [3/4], Batch [33], Loss: 1.159285068511963, Accuracy: 85.335 %\n",
      "Epoch [3/4], Batch [34], Loss: 1.1811023950576782, Accuracy: 85.34279411764706 %\n",
      "Epoch [3/4], Batch [35], Loss: 1.1311421394348145, Accuracy: 85.37342857142858 %\n",
      "Epoch [3/4], Batch [36], Loss: 1.1655718088150024, Accuracy: 85.39659722222223 %\n",
      "Epoch [3/4], Batch [37], Loss: 1.1508902311325073, Accuracy: 85.41385135135135 %\n",
      "Epoch [3/4], Batch [38], Loss: 1.1584149599075317, Accuracy: 85.43171052631578 %\n",
      "Epoch [3/4], Batch [39], Loss: 1.1381646394729614, Accuracy: 85.45320512820513 %\n",
      "Epoch [3/4], Batch [40], Loss: 1.1367545127868652, Accuracy: 85.4776875 %\n",
      "Epoch [3/4], Batch [41], Loss: 1.1141345500946045, Accuracy: 85.50396341463414 %\n",
      "Epoch [3/4], Batch [42], Loss: 1.120052456855774, Accuracy: 85.52583333333334 %\n",
      "Epoch [3/4], Batch [43], Loss: 1.1038177013397217, Accuracy: 85.55540697674418 %\n",
      "Epoch [3/4], Batch [44], Loss: 1.0347809791564941, Accuracy: 85.58005681818182 %\n",
      "Epoch [3/4], Batch [45], Loss: 1.0760959386825562, Accuracy: 85.61427777777777 %\n",
      "Epoch [3/4], Batch [46], Loss: 1.1463782787322998, Accuracy: 85.6270652173913 %\n",
      "Epoch [3/4], Batch [47], Loss: 1.142087459564209, Accuracy: 85.63287234042554 %\n",
      "Epoch [3/4], Batch [48], Loss: 1.1012128591537476, Accuracy: 85.64880208333334 %\n",
      "Epoch [3/4], Batch [49], Loss: 1.098253607749939, Accuracy: 85.67341836734694 %\n",
      "Epoch [3/4], Batch [50], Loss: 1.155604600906372, Accuracy: 85.6789 %\n",
      "Epoch [3/4], Batch [51], Loss: 1.1001993417739868, Accuracy: 85.6986274509804 %\n",
      "Epoch [3/4], Batch [52], Loss: 1.077939510345459, Accuracy: 85.72125 %\n",
      "Epoch [3/4], Batch [53], Loss: 1.0789096355438232, Accuracy: 85.7433962264151 %\n",
      "Epoch [3/4], Batch [54], Loss: 1.085466742515564, Accuracy: 85.76171296296296 %\n",
      "Epoch [3/4], Batch [55], Loss: 1.077518105506897, Accuracy: 85.78013636363636 %\n",
      "Epoch [3/4], Batch [56], Loss: 1.0703306198120117, Accuracy: 85.79705357142858 %\n",
      "Epoch [3/4], Batch [57], Loss: 1.0595520734786987, Accuracy: 85.82070175438596 %\n",
      "Epoch [3/4], Batch [58], Loss: 1.0816375017166138, Accuracy: 85.83418103448275 %\n",
      "Epoch [3/4], Batch [59], Loss: 1.071168303489685, Accuracy: 85.85398305084746 %\n",
      "Epoch [3/4], Batch [60], Loss: 1.0637997388839722, Accuracy: 85.87595833333333 %\n",
      "Epoch [3/4], Batch [61], Loss: 1.0343968868255615, Accuracy: 85.90348360655737 %\n",
      "Epoch [3/4], Batch [62], Loss: 1.034825086593628, Accuracy: 85.92895161290322 %\n",
      "Epoch [3/4], Batch [63], Loss: 1.0414741039276123, Accuracy: 85.95333333333333 %\n",
      "Epoch [3/4], Batch [64], Loss: 1.0300893783569336, Accuracy: 85.9796484375 %\n",
      "Epoch [3/4], Batch [65], Loss: 1.0241259336471558, Accuracy: 86.00884615384615 %\n",
      "Accuracy of the network on the 650000 train inputs: 86.00884615384615 %\n",
      "Epoch [4/4], Batch [1], Loss: 1.1110844612121582, Accuracy: 85.8925 %\n",
      "Epoch [4/4], Batch [2], Loss: 1.1135822534561157, Accuracy: 85.52625 %\n",
      "Epoch [4/4], Batch [3], Loss: 1.0732601881027222, Accuracy: 85.57416666666667 %\n",
      "Epoch [4/4], Batch [4], Loss: 1.074601411819458, Accuracy: 85.61375 %\n",
      "Epoch [4/4], Batch [5], Loss: 1.0640898942947388, Accuracy: 85.659 %\n",
      "Epoch [4/4], Batch [6], Loss: 1.0751827955245972, Accuracy: 85.63458333333334 %\n",
      "Epoch [4/4], Batch [7], Loss: 1.057266354560852, Accuracy: 85.7325 %\n",
      "Epoch [4/4], Batch [8], Loss: 1.089033842086792, Accuracy: 85.771875 %\n",
      "Epoch [4/4], Batch [9], Loss: 1.1058349609375, Accuracy: 85.77055555555556 %\n",
      "Epoch [4/4], Batch [10], Loss: 1.0709950923919678, Accuracy: 85.7645 %\n",
      "Epoch [4/4], Batch [11], Loss: 1.0892064571380615, Accuracy: 85.7875 %\n",
      "Epoch [4/4], Batch [12], Loss: 1.0803078413009644, Accuracy: 85.791875 %\n",
      "Epoch [4/4], Batch [13], Loss: 1.1252682209014893, Accuracy: 85.78653846153846 %\n",
      "Epoch [4/4], Batch [14], Loss: 1.1119499206542969, Accuracy: 85.79053571428571 %\n",
      "Epoch [4/4], Batch [15], Loss: 1.064573049545288, Accuracy: 85.818 %\n",
      "Epoch [4/4], Batch [16], Loss: 1.155480980873108, Accuracy: 85.81546875 %\n",
      "Epoch [4/4], Batch [17], Loss: 1.1582428216934204, Accuracy: 85.82176470588236 %\n",
      "Epoch [4/4], Batch [18], Loss: 1.1554218530654907, Accuracy: 85.83930555555555 %\n",
      "Epoch [4/4], Batch [19], Loss: 1.1540138721466064, Accuracy: 85.83486842105263 %\n",
      "Epoch [4/4], Batch [20], Loss: 1.1398950815200806, Accuracy: 85.86225 %\n",
      "Epoch [4/4], Batch [21], Loss: 1.1206731796264648, Accuracy: 85.89369047619047 %\n",
      "Epoch [4/4], Batch [22], Loss: 1.1324249505996704, Accuracy: 85.91693181818182 %\n",
      "Epoch [4/4], Batch [23], Loss: 1.1194018125534058, Accuracy: 85.93706521739131 %\n",
      "Epoch [4/4], Batch [24], Loss: 1.1161359548568726, Accuracy: 85.95354166666667 %\n",
      "Epoch [4/4], Batch [25], Loss: 1.0337694883346558, Accuracy: 85.9836 %\n",
      "Epoch [4/4], Batch [26], Loss: 1.0296627283096313, Accuracy: 86.01490384615384 %\n",
      "Epoch [4/4], Batch [27], Loss: 1.0404387712478638, Accuracy: 86.04185185185185 %\n",
      "Epoch [4/4], Batch [28], Loss: 1.1101605892181396, Accuracy: 86.063125 %\n",
      "Epoch [4/4], Batch [29], Loss: 1.1138136386871338, Accuracy: 86.07284482758621 %\n",
      "Epoch [4/4], Batch [30], Loss: 1.1063501834869385, Accuracy: 86.08283333333334 %\n",
      "Epoch [4/4], Batch [31], Loss: 1.0980467796325684, Accuracy: 86.10548387096775 %\n",
      "Epoch [4/4], Batch [32], Loss: 1.091564655303955, Accuracy: 86.123046875 %\n",
      "Epoch [4/4], Batch [33], Loss: 1.1041539907455444, Accuracy: 86.13704545454546 %\n",
      "Epoch [4/4], Batch [34], Loss: 1.126661777496338, Accuracy: 86.13676470588236 %\n",
      "Epoch [4/4], Batch [35], Loss: 1.075227975845337, Accuracy: 86.16285714285715 %\n",
      "Epoch [4/4], Batch [36], Loss: 1.110648512840271, Accuracy: 86.18020833333334 %\n",
      "Epoch [4/4], Batch [37], Loss: 1.0949352979660034, Accuracy: 86.19047297297297 %\n",
      "Epoch [4/4], Batch [38], Loss: 1.1045247316360474, Accuracy: 86.20394736842105 %\n",
      "Epoch [4/4], Batch [39], Loss: 1.0848721265792847, Accuracy: 86.21833333333333 %\n",
      "Epoch [4/4], Batch [40], Loss: 1.0822194814682007, Accuracy: 86.2381875 %\n",
      "Epoch [4/4], Batch [41], Loss: 1.0604482889175415, Accuracy: 86.25835365853659 %\n",
      "Epoch [4/4], Batch [42], Loss: 1.0667634010314941, Accuracy: 86.27446428571429 %\n",
      "Epoch [4/4], Batch [43], Loss: 1.051176905632019, Accuracy: 86.30110465116279 %\n",
      "Epoch [4/4], Batch [44], Loss: 0.9817700386047363, Accuracy: 86.324375 %\n",
      "Epoch [4/4], Batch [45], Loss: 1.0220096111297607, Accuracy: 86.3536111111111 %\n",
      "Epoch [4/4], Batch [46], Loss: 1.0938661098480225, Accuracy: 86.36440217391305 %\n",
      "Epoch [4/4], Batch [47], Loss: 1.0882501602172852, Accuracy: 86.37090425531915 %\n",
      "Epoch [4/4], Batch [48], Loss: 1.0481421947479248, Accuracy: 86.38776041666667 %\n",
      "Epoch [4/4], Batch [49], Loss: 1.0459343194961548, Accuracy: 86.40811224489796 %\n",
      "Epoch [4/4], Batch [50], Loss: 1.1063646078109741, Accuracy: 86.40885 %\n",
      "Epoch [4/4], Batch [51], Loss: 1.0494886636734009, Accuracy: 86.42622549019607 %\n",
      "Epoch [4/4], Batch [52], Loss: 1.025223970413208, Accuracy: 86.44586538461539 %\n",
      "Epoch [4/4], Batch [53], Loss: 1.0274267196655273, Accuracy: 86.46509433962264 %\n",
      "Epoch [4/4], Batch [54], Loss: 1.034631609916687, Accuracy: 86.47935185185185 %\n",
      "Epoch [4/4], Batch [55], Loss: 1.0267244577407837, Accuracy: 86.4945 %\n",
      "Epoch [4/4], Batch [56], Loss: 1.0191763639450073, Accuracy: 86.50995535714286 %\n",
      "Epoch [4/4], Batch [57], Loss: 1.0082151889801025, Accuracy: 86.52956140350877 %\n",
      "Epoch [4/4], Batch [58], Loss: 1.0308711528778076, Accuracy: 86.54202586206897 %\n",
      "Epoch [4/4], Batch [59], Loss: 1.021002173423767, Accuracy: 86.55796610169492 %\n",
      "Epoch [4/4], Batch [60], Loss: 1.0145139694213867, Accuracy: 86.57525 %\n",
      "Epoch [4/4], Batch [61], Loss: 0.9842305779457092, Accuracy: 86.60012295081967 %\n",
      "Epoch [4/4], Batch [62], Loss: 0.9850297570228577, Accuracy: 86.62391129032258 %\n",
      "Epoch [4/4], Batch [63], Loss: 0.9924068450927734, Accuracy: 86.6456746031746 %\n",
      "Epoch [4/4], Batch [64], Loss: 0.9809315204620361, Accuracy: 86.6689453125 %\n",
      "Epoch [4/4], Batch [65], Loss: 0.9738448262214661, Accuracy: 86.69553846153846 %\n",
      "Accuracy of the network on the 650000 train inputs: 86.69553846153846 %\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "        \n",
    "        # convert bow to tensor\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "        # Convert labels to LongTensors\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        for j in range(batch_size):\n",
    "            diff = abs(predicted[j] - labels[j])\n",
    "            if diff == 0:\n",
    "                correct += 1\n",
    "            elif diff == 1:\n",
    "                correct += 0.75\n",
    "            elif diff == 2:\n",
    "                correct += 0.5\n",
    "            elif diff == 3:\n",
    "                correct += 0.25\n",
    "\n",
    "\n",
    "        # Print the loss for this batch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}, Accuracy: {100 * correct / total} %')\n",
    "\n",
    "    print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "print('Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the above model in its entirety in pkl file\n",
    "torch.save(model, 'bow_tfidf_final_model.pkl')\n",
    "\n",
    "# load the model\n",
    "model = torch.load('bow_tfidf_final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal Accuracy of the network on the 50000 test inputs: 85.3045 %\n",
      "Confusion Matrix:\n",
      "[[7397 1944  239   72  348]\n",
      " [2649 4763 1818  295  475]\n",
      " [ 835 2152 4185 1777 1051]\n",
      " [ 322  473 1728 3885 3592]\n",
      " [ 324  132  306 1642 7596]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69     10000\n",
      "           1       0.50      0.48      0.49     10000\n",
      "           2       0.51      0.42      0.46     10000\n",
      "           3       0.51      0.39      0.44     10000\n",
      "           4       0.58      0.76      0.66     10000\n",
      "\n",
      "    accuracy                           0.56     50000\n",
      "   macro avg       0.55      0.56      0.55     50000\n",
      "weighted avg       0.55      0.56      0.55     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(0, len(test_tokens), batch_size):\n",
    "        inputs = test_tokens[i:i + batch_size]\n",
    "\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        labels = test_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        for j in range(batch_size):\n",
    "            diff = abs(predicted[j] - labels[j])\n",
    "            if diff == 0:\n",
    "                correct += 1\n",
    "            elif diff == 1:\n",
    "                correct += 0.75\n",
    "            elif diff == 2:\n",
    "                correct += 0.5\n",
    "            elif diff == 3:\n",
    "                correct += 0.25\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy_ordinal = 100 * correct / total\n",
    "    print(f'Ordinal Accuracy of the network on the {total} test inputs: {accuracy_ordinal} %')\n",
    "\n",
    "    # Calculate confusion matrix, precision, recall, and F1 scores\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    classification_rep = classification_report(true_labels, predictions, target_names=[str(i) for i in range(5)])\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    # print(\"F1 Micro:\", f1_micro)\n",
    "    # print(\"F1 Macro:\", f1_macro)\n",
    "    # print(\"F1 Weighted:\", f1_weighted)\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
