{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"amazon_polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('amazon.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('amazon.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 3600000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 400000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data_title = train_dataset['title']\n",
    "train_data_content = train_dataset['content']\n",
    "train_labels = train_dataset['label']\n",
    "test_data_title = test_dataset['title']\n",
    "test_data_content = test_dataset['content']\n",
    "test_labels = test_dataset['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stuning even for the non-gamer\n",
      "This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_data_title[0])\n",
    "print(train_data_content[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data_title + train_data_content\n",
    "# test_data = test_data_title + test_data_content\n",
    "\n",
    "train_data = [None] * len(train_data_title)\n",
    "for i in range(len(train_data_title)):\n",
    "    train_data[i] = train_data_title[i] + train_data_content[i] + train_data_title[i]\n",
    "\n",
    "test_data = [None] * len(test_data_title)\n",
    "for i in range(len(test_data_title)):\n",
    "    test_data[i] = test_data_title[i] + test_data_content[i] + test_data_title[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def custom_contractions_fix(text):\n",
    "#     # Define custom contractions to expand\n",
    "#     contractions_dict = {\n",
    "#         \"don't\": \"do not\",\n",
    "#         \"doesn't\": \"does not\",\n",
    "#         \"didn't\": \"did not\",\n",
    "#         # Add more contractions as needed\n",
    "#     }\n",
    "    \n",
    "#     # Use a regular expression to find and replace contractions\n",
    "#     contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "#     def replace(match):\n",
    "#         return contractions_dict[match.group(0)]\n",
    "    \n",
    "#     expanded_text = contractions_re.sub(replace, text)\n",
    "#     return expanded_text\n",
    "\n",
    "\n",
    "# def tokenize_text(text):\n",
    "    \n",
    "#     # if there is a word between () then write it twice\n",
    "#     text = re.sub(r'\\((.*?)\\)', r'\\1 \\1', text)\n",
    "    \n",
    "#     # Apply custom contractions expansion\n",
    "#     text = custom_contractions_fix(text)\n",
    "    \n",
    "#     # Replace hyphens with spaces\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     temp = [word for word in tokens if len(word) > 1 and word.isupper()]\n",
    "#     tokens.extend(temp)\n",
    "    \n",
    "#     # Convert to lowercase\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "#     # Remove periods\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # Remove punctuation and stopwords\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut entire train and test tokens and labels to 5 lakh and 50k\n",
    "train_data = train_data[:500000]\n",
    "train_labels = train_labels[:500000]\n",
    "test_data = test_data[:50000]\n",
    "test_labels = test_labels[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(tokenize_text(train_data[10]))\n",
    "\n",
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('amazon_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('amazon_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('amazon_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('amazon_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('amazon_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('amazon_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('amazon_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('amazon_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29617\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the order of the training data and the testing data along with their labels with a specific seed\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "train_data = []\n",
    "new_train_labels = []\n",
    "\n",
    "test_data = []\n",
    "new_test_labels = []\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    train_data.append(train_tokens[i])\n",
    "    new_train_labels.append(train_labels[i])\n",
    "\n",
    "for i in range(len(test_tokens)):\n",
    "    test_data.append(test_tokens[i])\n",
    "    new_test_labels.append(test_labels[i])\n",
    "\n",
    "# shuffle the data\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(train_data, new_train_labels))\n",
    "random.shuffle(temp)\n",
    "train_data, train_labels = zip(*temp)\n",
    "\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(test_data, new_test_labels))\n",
    "random.shuffle(temp)\n",
    "test_data, test_labels = zip(*temp)\n",
    "\n",
    "# convert them back to the format of train_tokens and test_tokens\n",
    "train_tokens = list(train_data)\n",
    "train_labels = list(train_labels)\n",
    "test_tokens = list(test_data)\n",
    "test_labels = list(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def calculate_tfidf(tokens, vocab_dict):\n",
    "\n",
    "    n_docs = len(tokens)\n",
    "    vocab_size = len(vocab_dict)\n",
    "    \n",
    "    # Calculate document frequencies (DF) for each word\n",
    "    doc_freq = Counter()\n",
    "    for doc in tokens:\n",
    "        doc_freq.update(set(doc))\n",
    "    \n",
    "    # Precompute IDF values\n",
    "    idf_dict = {}\n",
    "    for word, count in doc_freq.items():\n",
    "        idf_dict[word] = log(n_docs / (count + 1))  # Add 1 to avoid division by zero\n",
    "    \n",
    "    tfidf_matrix = np.zeros((n_docs, vocab_size))\n",
    "    \n",
    "    for i, doc in enumerate(tokens):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        total_words_in_doc = len(doc)\n",
    "        term_freq = Counter(doc)  # Calculate term frequency (TF) for the document\n",
    "        \n",
    "        for word, tf in term_freq.items():\n",
    "            if word in vocab_dict:\n",
    "                tfidf_matrix[i][vocab_dict[word]] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "            else:\n",
    "                # Use 'UNK' if the word is not in the vocabulary\n",
    "                tfidf_matrix[i][vocab_dict['UNK']] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28777\n",
      "bow train\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n"
     ]
    }
   ],
   "source": [
    "reduced_train_tokens = train_tokens[:40000]\n",
    "\n",
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab_reduced = set()\n",
    "for tokens in reduced_train_tokens:\n",
    "    vocab_reduced.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict_reduced = {}\n",
    "for i, word in enumerate(vocab_reduced):\n",
    "    vocab_dict_reduced[word] = i\n",
    "\n",
    "print(len(vocab_dict_reduced))\n",
    "\n",
    "# train_bow = np.zeros((len(reduced_train_tokens), len(vocab_reduced)))\n",
    "\n",
    "print(\"bow train\")\n",
    "# Convert text to BoW vectors\n",
    "# for i, tokens in enumerate(reduced_train_tokens):\n",
    "#     if i % 5000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         train_bow[i][vocab_dict_reduced[token]] += 1\n",
    "\n",
    "train_tfidf = calculate_tfidf(reduced_train_tokens, vocab_dict_reduced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow test\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "reduced_test_tokens = test_tokens[:5000]\n",
    "# test_bow = np.zeros((len(reduced_test_tokens), len(vocab_reduced)))\n",
    "\n",
    "print(\"bow test\")\n",
    "\n",
    "# for i, tokens in enumerate(reduced_test_tokens):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         if token in vocab_dict_reduced:\n",
    "#             test_bow[i][vocab_dict_reduced[token]] += 1\n",
    "#         else:    # if there is an unknown word, add it to the UNK column \n",
    "#             test_bow[i][vocab_dict_reduced['UNK']] += 1\n",
    "\n",
    "test_tfidf = calculate_tfidf(reduced_test_tokens, vocab_dict_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8686\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87      2472\n",
      "           1       0.87      0.87      0.87      2528\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "classifier = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Fit the classifier to the training data with a progress bar\n",
    "# with tqdm(total=len(train_tfidf[:20000])) as pbar:\n",
    "classifier.fit(train_tfidf[:40000], train_labels[:40000])\n",
    "    # pbar.update(len(train_tfidf[:20000]))\n",
    "\n",
    "# Predict the labels of the test data with a progress bar\n",
    "# with tqdm(total=len(test_tfidf[:5000])) as pbar:\n",
    "y_pred = classifier.predict(test_tfidf[:5000])\n",
    "    # pbar.update(len(test_tfidf[:5000]))\n",
    "\n",
    "# Calculate accuracy and other classification metrics\n",
    "print('Accuracy: {}'.format(accuracy_score(test_labels[:5000], y_pred)))\n",
    "print(classification_report(test_labels[:5000], y_pred))\n",
    "\n",
    "# save the classifier\n",
    "with open('amazon_classifier_log_reg_06_200itr.pkl', 'wb') as file:\n",
    "    pickle.dump(classifier, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
