{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('dbpedia.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 560000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 70000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data_title = train_dataset['title']\n",
    "train_data_content = train_dataset['content']\n",
    "train_labels = train_dataset['label']\n",
    "test_data_title = test_dataset['title']\n",
    "test_data_content = test_dataset['content']\n",
    "test_labels = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_title[0])\n",
    "# print(train_data_content[0])\n",
    "# print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data_title + train_data_content\n",
    "# test_data = test_data_title + test_data_content\n",
    "\n",
    "train_data = [None] * len(train_data_title)\n",
    "for i in range(len(train_data_title)):\n",
    "    train_data[i] = train_data_title[i] + train_data_content[i]\n",
    "\n",
    "test_data = [None] * len(test_data_title)\n",
    "for i in range(len(test_data_title)):\n",
    "    test_data[i] = test_data_title[i] + test_data_content[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def custom_contractions_fix(text):\n",
    "#     # Define custom contractions to expand\n",
    "#     contractions_dict = {\n",
    "#         \"don't\": \"do not\",\n",
    "#         \"doesn't\": \"does not\",\n",
    "#         \"didn't\": \"did not\",\n",
    "#         # Add more contractions as needed\n",
    "#     }\n",
    "    \n",
    "#     # Use a regular expression to find and replace contractions\n",
    "#     contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "#     def replace(match):\n",
    "#         return contractions_dict[match.group(0)]\n",
    "    \n",
    "#     expanded_text = contractions_re.sub(replace, text)\n",
    "#     return expanded_text\n",
    "\n",
    "\n",
    "# def tokenize_text(text):\n",
    "    \n",
    "#     # if there is a word between () then write it twice\n",
    "#     text = re.sub(r'\\((.*?)\\)', r'\\1 \\1', text)\n",
    "    \n",
    "#     # Apply custom contractions expansion\n",
    "#     text = custom_contractions_fix(text)\n",
    "    \n",
    "#     # Replace hyphens with spaces\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     temp = [word for word in tokens if len(word) > 1 and word.isupper()]\n",
    "#     tokens.extend(temp)\n",
    "    \n",
    "#     # Convert to lowercase\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "#     # Remove periods\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # Remove punctuation and stopwords\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(tokenize_text(train_data[10]))\n",
    "\n",
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('dbpedia_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('dbpedia_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('dbpedia_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('dbpedia_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('dbpedia_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('dbpedia_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('dbpedia_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('dbpedia_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36315\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the order of the training data and the testing data along with their labels with a specific seed\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "train_data = []\n",
    "new_train_labels = []\n",
    "\n",
    "test_data = []\n",
    "new_test_labels = []\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    train_data.append(train_tokens[i])\n",
    "    new_train_labels.append(train_labels[i])\n",
    "\n",
    "for i in range(len(test_tokens)):\n",
    "    test_data.append(test_tokens[i])\n",
    "    new_test_labels.append(test_labels[i])\n",
    "\n",
    "# shuffle the data\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(train_data, new_train_labels))\n",
    "random.shuffle(temp)\n",
    "train_data, train_labels = zip(*temp)\n",
    "\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(test_data, new_test_labels))\n",
    "random.shuffle(temp)\n",
    "test_data, test_labels = zip(*temp)\n",
    "\n",
    "# convert them back to the format of train_tokens and test_tokens\n",
    "train_tokens = list(train_data)\n",
    "train_labels = list(train_labels)\n",
    "test_tokens = list(test_data)\n",
    "test_labels = list(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 4\n",
    "learning_rate = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 14  # Output size is 13 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def calculate_tfidf(tokens, vocab_dict):\n",
    "\n",
    "    n_docs = len(tokens)\n",
    "    vocab_size = len(vocab_dict)\n",
    "    \n",
    "    # Calculate document frequencies (DF) for each word\n",
    "    doc_freq = Counter()\n",
    "    for doc in tokens:\n",
    "        doc_freq.update(set(doc))\n",
    "    \n",
    "    # Precompute IDF values\n",
    "    idf_dict = {}\n",
    "    for word, count in doc_freq.items():\n",
    "        idf_dict[word] = log(n_docs / (count + 1))  # Add 1 to avoid division by zero\n",
    "    \n",
    "    tfidf_matrix = np.zeros((n_docs, vocab_size))\n",
    "    \n",
    "    for i, doc in enumerate(tokens):\n",
    "        # if i % 1000 == 0:\n",
    "        #     print(i)\n",
    "        \n",
    "        total_words_in_doc = len(doc)\n",
    "        term_freq = Counter(doc)  # Calculate term frequency (TF) for the document\n",
    "        \n",
    "        for word, tf in term_freq.items():\n",
    "            if word in vocab_dict:\n",
    "                tfidf_matrix[i][vocab_dict[word]] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "            else:\n",
    "                # Use 'UNK' if the word is not in the vocabulary\n",
    "                tfidf_matrix[i][vocab_dict['UNK']] = (tf / total_words_in_doc) * idf_dict[word]\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Batch [1], Loss: 2.6389148235321045\n",
      "Epoch [1/4], Batch [2], Loss: 2.588900566101074\n",
      "Epoch [1/4], Batch [3], Loss: 2.5391645431518555\n",
      "Epoch [1/4], Batch [4], Loss: 2.4892818927764893\n",
      "Epoch [1/4], Batch [5], Loss: 2.439829111099243\n",
      "Epoch [1/4], Batch [6], Loss: 2.3911490440368652\n",
      "Epoch [1/4], Batch [7], Loss: 2.3406126499176025\n",
      "Epoch [1/4], Batch [8], Loss: 2.2928597927093506\n",
      "Epoch [1/4], Batch [9], Loss: 2.2468421459198\n",
      "Epoch [1/4], Batch [10], Loss: 2.1975719928741455\n",
      "Epoch [1/4], Batch [11], Loss: 2.1528661251068115\n",
      "Epoch [1/4], Batch [12], Loss: 2.1045045852661133\n",
      "Epoch [1/4], Batch [13], Loss: 2.0598368644714355\n",
      "Epoch [1/4], Batch [14], Loss: 2.015774726867676\n",
      "Epoch [1/4], Batch [15], Loss: 1.9716567993164062\n",
      "Epoch [1/4], Batch [16], Loss: 1.9302722215652466\n",
      "Epoch [1/4], Batch [17], Loss: 1.881532073020935\n",
      "Epoch [1/4], Batch [18], Loss: 1.8424574136734009\n",
      "Epoch [1/4], Batch [19], Loss: 1.8021113872528076\n",
      "Epoch [1/4], Batch [20], Loss: 1.7588152885437012\n",
      "Epoch [1/4], Batch [21], Loss: 1.7193080186843872\n",
      "Epoch [1/4], Batch [22], Loss: 1.6777340173721313\n",
      "Epoch [1/4], Batch [23], Loss: 1.6411316394805908\n",
      "Epoch [1/4], Batch [24], Loss: 1.6041932106018066\n",
      "Epoch [1/4], Batch [25], Loss: 1.569167971611023\n",
      "Epoch [1/4], Batch [26], Loss: 1.5323582887649536\n",
      "Epoch [1/4], Batch [27], Loss: 1.4899462461471558\n",
      "Epoch [1/4], Batch [28], Loss: 1.461463212966919\n",
      "Epoch [1/4], Batch [29], Loss: 1.4341517686843872\n",
      "Epoch [1/4], Batch [30], Loss: 1.3937522172927856\n",
      "Epoch [1/4], Batch [31], Loss: 1.3613288402557373\n",
      "Epoch [1/4], Batch [32], Loss: 1.3348491191864014\n",
      "Epoch [1/4], Batch [33], Loss: 1.304980993270874\n",
      "Epoch [1/4], Batch [34], Loss: 1.2785171270370483\n",
      "Epoch [1/4], Batch [35], Loss: 1.2489001750946045\n",
      "Epoch [1/4], Batch [36], Loss: 1.2125214338302612\n",
      "Epoch [1/4], Batch [37], Loss: 1.192643404006958\n",
      "Epoch [1/4], Batch [38], Loss: 1.163348913192749\n",
      "Epoch [1/4], Batch [39], Loss: 1.1400872468948364\n",
      "Epoch [1/4], Batch [40], Loss: 1.1115351915359497\n",
      "Epoch [1/4], Batch [41], Loss: 1.097601056098938\n",
      "Epoch [1/4], Batch [42], Loss: 1.0679954290390015\n",
      "Epoch [1/4], Batch [43], Loss: 1.0485851764678955\n",
      "Epoch [1/4], Batch [44], Loss: 1.0275163650512695\n",
      "Epoch [1/4], Batch [45], Loss: 1.0083461999893188\n",
      "Epoch [1/4], Batch [46], Loss: 0.9817848801612854\n",
      "Epoch [1/4], Batch [47], Loss: 0.9633655548095703\n",
      "Epoch [1/4], Batch [48], Loss: 0.9413696527481079\n",
      "Epoch [1/4], Batch [49], Loss: 0.9291383028030396\n",
      "Epoch [1/4], Batch [50], Loss: 0.9039722681045532\n",
      "Epoch [1/4], Batch [51], Loss: 0.8854793906211853\n",
      "Epoch [1/4], Batch [52], Loss: 0.8669180870056152\n",
      "Epoch [1/4], Batch [53], Loss: 0.8555843830108643\n",
      "Epoch [1/4], Batch [54], Loss: 0.8344237208366394\n",
      "Epoch [1/4], Batch [55], Loss: 0.8218798637390137\n",
      "Epoch [1/4], Batch [56], Loss: 0.8138598799705505\n",
      "Accuracy of the network on the 560000 train inputs: 91.41053571428571 %\n",
      "Epoch [2/4], Batch [1], Loss: 0.7490808367729187\n",
      "Epoch [2/4], Batch [2], Loss: 0.7329994440078735\n",
      "Epoch [2/4], Batch [3], Loss: 0.7287747263908386\n",
      "Epoch [2/4], Batch [4], Loss: 0.7224809527397156\n",
      "Epoch [2/4], Batch [5], Loss: 0.710229218006134\n",
      "Epoch [2/4], Batch [6], Loss: 0.7035837173461914\n",
      "Epoch [2/4], Batch [7], Loss: 0.6824626922607422\n",
      "Epoch [2/4], Batch [8], Loss: 0.6736775636672974\n",
      "Epoch [2/4], Batch [9], Loss: 0.6651498675346375\n",
      "Epoch [2/4], Batch [10], Loss: 0.6553874015808105\n",
      "Epoch [2/4], Batch [11], Loss: 0.6506561040878296\n",
      "Epoch [2/4], Batch [12], Loss: 0.6341570019721985\n",
      "Epoch [2/4], Batch [13], Loss: 0.6243895292282104\n",
      "Epoch [2/4], Batch [14], Loss: 0.6192625164985657\n",
      "Epoch [2/4], Batch [15], Loss: 0.6093140840530396\n",
      "Epoch [2/4], Batch [16], Loss: 0.6056798100471497\n",
      "Epoch [2/4], Batch [17], Loss: 0.5870442390441895\n",
      "Epoch [2/4], Batch [18], Loss: 0.5844244360923767\n",
      "Epoch [2/4], Batch [19], Loss: 0.5768757462501526\n",
      "Epoch [2/4], Batch [20], Loss: 0.5699651837348938\n",
      "Epoch [2/4], Batch [21], Loss: 0.5542815923690796\n",
      "Epoch [2/4], Batch [22], Loss: 0.5485917925834656\n",
      "Epoch [2/4], Batch [23], Loss: 0.5428419709205627\n",
      "Epoch [2/4], Batch [24], Loss: 0.5366465449333191\n",
      "Epoch [2/4], Batch [25], Loss: 0.5354528427124023\n",
      "Epoch [2/4], Batch [26], Loss: 0.526674211025238\n",
      "Epoch [2/4], Batch [27], Loss: 0.5089000463485718\n",
      "Epoch [2/4], Batch [28], Loss: 0.5115494728088379\n",
      "Epoch [2/4], Batch [29], Loss: 0.51479172706604\n",
      "Epoch [2/4], Batch [30], Loss: 0.4977274537086487\n",
      "Epoch [2/4], Batch [31], Loss: 0.4909645617008209\n",
      "Epoch [2/4], Batch [32], Loss: 0.48994845151901245\n",
      "Epoch [2/4], Batch [33], Loss: 0.4811649024486542\n",
      "Epoch [2/4], Batch [34], Loss: 0.4808408319950104\n",
      "Epoch [2/4], Batch [35], Loss: 0.4757426679134369\n",
      "Epoch [2/4], Batch [36], Loss: 0.4605826735496521\n",
      "Epoch [2/4], Batch [37], Loss: 0.4655601680278778\n",
      "Epoch [2/4], Batch [38], Loss: 0.453142374753952\n",
      "Epoch [2/4], Batch [39], Loss: 0.4537310004234314\n",
      "Epoch [2/4], Batch [40], Loss: 0.44450831413269043\n",
      "Epoch [2/4], Batch [41], Loss: 0.44828855991363525\n",
      "Epoch [2/4], Batch [42], Loss: 0.43820565938949585\n",
      "Epoch [2/4], Batch [43], Loss: 0.4376189410686493\n",
      "Epoch [2/4], Batch [44], Loss: 0.4369603097438812\n",
      "Epoch [2/4], Batch [45], Loss: 0.431521475315094\n",
      "Epoch [2/4], Batch [46], Loss: 0.4206527769565582\n",
      "Epoch [2/4], Batch [47], Loss: 0.4184080958366394\n",
      "Epoch [2/4], Batch [48], Loss: 0.41191455721855164\n",
      "Epoch [2/4], Batch [49], Loss: 0.4132232367992401\n",
      "Epoch [2/4], Batch [50], Loss: 0.4035847783088684\n",
      "Epoch [2/4], Batch [51], Loss: 0.40188178420066833\n",
      "Epoch [2/4], Batch [52], Loss: 0.3935462236404419\n",
      "Epoch [2/4], Batch [53], Loss: 0.39520102739334106\n",
      "Epoch [2/4], Batch [54], Loss: 0.3878263235092163\n",
      "Epoch [2/4], Batch [55], Loss: 0.3872723877429962\n",
      "Epoch [2/4], Batch [56], Loss: 0.3905925154685974\n",
      "Accuracy of the network on the 560000 train inputs: 95.815 %\n",
      "Epoch [3/4], Batch [1], Loss: 0.3587587773799896\n",
      "Epoch [3/4], Batch [2], Loss: 0.3506303131580353\n",
      "Epoch [3/4], Batch [3], Loss: 0.353746622800827\n",
      "Epoch [3/4], Batch [4], Loss: 0.35745829343795776\n",
      "Epoch [3/4], Batch [5], Loss: 0.35284140706062317\n",
      "Epoch [3/4], Batch [6], Loss: 0.354867160320282\n",
      "Epoch [3/4], Batch [7], Loss: 0.3434867560863495\n",
      "Epoch [3/4], Batch [8], Loss: 0.342570036649704\n",
      "Epoch [3/4], Batch [9], Loss: 0.3412172794342041\n",
      "Epoch [3/4], Batch [10], Loss: 0.34014180302619934\n",
      "Epoch [3/4], Batch [11], Loss: 0.34176120162010193\n",
      "Epoch [3/4], Batch [12], Loss: 0.33359140157699585\n",
      "Epoch [3/4], Batch [13], Loss: 0.3300374150276184\n",
      "Epoch [3/4], Batch [14], Loss: 0.33146125078201294\n",
      "Epoch [3/4], Batch [15], Loss: 0.3283524215221405\n",
      "Epoch [3/4], Batch [16], Loss: 0.33098214864730835\n",
      "Epoch [3/4], Batch [17], Loss: 0.3196105659008026\n",
      "Epoch [3/4], Batch [18], Loss: 0.3218291699886322\n",
      "Epoch [3/4], Batch [19], Loss: 0.3205835819244385\n",
      "Epoch [3/4], Batch [20], Loss: 0.32034456729888916\n",
      "Epoch [3/4], Batch [21], Loss: 0.30806219577789307\n",
      "Epoch [3/4], Batch [22], Loss: 0.30875164270401\n",
      "Epoch [3/4], Batch [23], Loss: 0.30820927023887634\n",
      "Epoch [3/4], Batch [24], Loss: 0.3071867823600769\n",
      "Epoch [3/4], Batch [25], Loss: 0.3120243549346924\n",
      "Epoch [3/4], Batch [26], Loss: 0.307669073343277\n",
      "Epoch [3/4], Batch [27], Loss: 0.2948361933231354\n",
      "Epoch [3/4], Batch [28], Loss: 0.3015972673892975\n",
      "Epoch [3/4], Batch [29], Loss: 0.30712246894836426\n",
      "Epoch [3/4], Batch [30], Loss: 0.2955617606639862\n",
      "Epoch [3/4], Batch [31], Loss: 0.2929176688194275\n",
      "Epoch [3/4], Batch [32], Loss: 0.2945623993873596\n",
      "Epoch [3/4], Batch [33], Loss: 0.28848785161972046\n",
      "Epoch [3/4], Batch [34], Loss: 0.2913810610771179\n",
      "Epoch [3/4], Batch [35], Loss: 0.291113018989563\n",
      "Epoch [3/4], Batch [36], Loss: 0.28018635511398315\n",
      "Epoch [3/4], Batch [37], Loss: 0.2890689969062805\n",
      "Epoch [3/4], Batch [38], Loss: 0.2779647409915924\n",
      "Epoch [3/4], Batch [39], Loss: 0.28262630105018616\n",
      "Epoch [3/4], Batch [40], Loss: 0.2766447067260742\n",
      "Epoch [3/4], Batch [41], Loss: 0.28210771083831787\n",
      "Epoch [3/4], Batch [42], Loss: 0.2755754888057709\n",
      "Epoch [3/4], Batch [43], Loss: 0.2777201235294342\n",
      "Epoch [3/4], Batch [44], Loss: 0.28128719329833984\n",
      "Epoch [3/4], Batch [45], Loss: 0.2769239842891693\n",
      "Epoch [3/4], Batch [46], Loss: 0.26873093843460083\n",
      "Epoch [3/4], Batch [47], Loss: 0.2684404253959656\n",
      "Epoch [3/4], Batch [48], Loss: 0.2651313841342926\n",
      "Epoch [3/4], Batch [49], Loss: 0.268505334854126\n",
      "Epoch [3/4], Batch [50], Loss: 0.26194971799850464\n",
      "Epoch [3/4], Batch [51], Loss: 0.26305219531059265\n",
      "Epoch [3/4], Batch [52], Loss: 0.2566624879837036\n",
      "Epoch [3/4], Batch [53], Loss: 0.2595808207988739\n",
      "Epoch [3/4], Batch [54], Loss: 0.255840539932251\n",
      "Epoch [3/4], Batch [55], Loss: 0.25695228576660156\n",
      "Epoch [3/4], Batch [56], Loss: 0.26179686188697815\n",
      "Accuracy of the network on the 560000 train inputs: 96.76892857142857 %\n",
      "Epoch [4/4], Batch [1], Loss: 0.24006737768650055\n",
      "Epoch [4/4], Batch [2], Loss: 0.23425178229808807\n",
      "Epoch [4/4], Batch [3], Loss: 0.23725898563861847\n",
      "Epoch [4/4], Batch [4], Loss: 0.24227233231067657\n",
      "Epoch [4/4], Batch [5], Loss: 0.2388128638267517\n",
      "Epoch [4/4], Batch [6], Loss: 0.2424178421497345\n",
      "Epoch [4/4], Batch [7], Loss: 0.23317526280879974\n",
      "Epoch [4/4], Batch [8], Loss: 0.23393474519252777\n",
      "Epoch [4/4], Batch [9], Loss: 0.23362578451633453\n",
      "Epoch [4/4], Batch [10], Loss: 0.23407769203186035\n",
      "Epoch [4/4], Batch [11], Loss: 0.2367200255393982\n",
      "Epoch [4/4], Batch [12], Loss: 0.23050251603126526\n",
      "Epoch [4/4], Batch [13], Loss: 0.2285400629043579\n",
      "Epoch [4/4], Batch [14], Loss: 0.23069582879543304\n",
      "Epoch [4/4], Batch [15], Loss: 0.22902663052082062\n",
      "Epoch [4/4], Batch [16], Loss: 0.23266398906707764\n",
      "Epoch [4/4], Batch [17], Loss: 0.22351540625095367\n",
      "Epoch [4/4], Batch [18], Loss: 0.22601470351219177\n",
      "Epoch [4/4], Batch [19], Loss: 0.2263278067111969\n",
      "Epoch [4/4], Batch [20], Loss: 0.22713862359523773\n",
      "Epoch [4/4], Batch [21], Loss: 0.21627292037010193\n",
      "Epoch [4/4], Batch [22], Loss: 0.2182219922542572\n",
      "Epoch [4/4], Batch [23], Loss: 0.2188054919242859\n",
      "Epoch [4/4], Batch [24], Loss: 0.21917961537837982\n",
      "Epoch [4/4], Batch [25], Loss: 0.225249245762825\n",
      "Epoch [4/4], Batch [26], Loss: 0.22215285897254944\n",
      "Epoch [4/4], Batch [27], Loss: 0.21121838688850403\n",
      "Epoch [4/4], Batch [28], Loss: 0.2182345688343048\n",
      "Epoch [4/4], Batch [29], Loss: 0.22317543625831604\n",
      "Epoch [4/4], Batch [30], Loss: 0.21401150524616241\n",
      "Epoch [4/4], Batch [31], Loss: 0.21231487393379211\n",
      "Epoch [4/4], Batch [32], Loss: 0.21446110308170319\n",
      "Epoch [4/4], Batch [33], Loss: 0.20895780622959137\n",
      "Epoch [4/4], Batch [34], Loss: 0.2121998816728592\n",
      "Epoch [4/4], Batch [35], Loss: 0.21376067399978638\n",
      "Epoch [4/4], Batch [36], Loss: 0.20420630276203156\n",
      "Epoch [4/4], Batch [37], Loss: 0.21407707035541534\n",
      "Epoch [4/4], Batch [38], Loss: 0.20308920741081238\n",
      "Epoch [4/4], Batch [39], Loss: 0.20865817368030548\n",
      "Epoch [4/4], Batch [40], Loss: 0.20376832783222198\n",
      "Epoch [4/4], Batch [41], Loss: 0.2093750238418579\n",
      "Epoch [4/4], Batch [42], Loss: 0.2042018622159958\n",
      "Epoch [4/4], Batch [43], Loss: 0.20655325055122375\n",
      "Epoch [4/4], Batch [44], Loss: 0.2118276059627533\n",
      "Epoch [4/4], Batch [45], Loss: 0.20739184319972992\n",
      "Epoch [4/4], Batch [46], Loss: 0.20014841854572296\n",
      "Epoch [4/4], Batch [47], Loss: 0.20002566277980804\n",
      "Epoch [4/4], Batch [48], Loss: 0.19811494648456573\n",
      "Epoch [4/4], Batch [49], Loss: 0.20186933875083923\n",
      "Epoch [4/4], Batch [50], Loss: 0.19651831686496735\n",
      "Epoch [4/4], Batch [51], Loss: 0.19817492365837097\n",
      "Epoch [4/4], Batch [52], Loss: 0.1928810328245163\n",
      "Epoch [4/4], Batch [53], Loss: 0.19579511880874634\n",
      "Epoch [4/4], Batch [54], Loss: 0.19353219866752625\n",
      "Epoch [4/4], Batch [55], Loss: 0.19509081542491913\n",
      "Epoch [4/4], Batch [56], Loss: 0.200082927942276\n",
      "Accuracy of the network on the 560000 train inputs: 97.33089285714286 %\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "        # print(len(inputs))\n",
    "        # print(len(vocab))\n",
    "        \n",
    "        # make bow vector for inputs\n",
    "        # bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "        # # print(bow.shape)\n",
    "        # for j in range(batch_size):\n",
    "        #     for token in inputs[j]:\n",
    "\n",
    "        #         bow[j][vocab_dict[token]] += 1\n",
    "\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "\n",
    "        \n",
    "        # convert bow to tensor\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "        # Convert labels to LongTensors\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        # Print the loss for this batch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}')\n",
    "\n",
    "    print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the above model in its entirety in pkl file\n",
    "torch.save(model, 'bow_model_dbpedia.pkl')\n",
    "\n",
    "# load the model\n",
    "model = torch.load('bow_model_dbpedia.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 70000 test inputs: 96.12571428571428 %\n",
      "Confusion Matrix:\n",
      "[[4555   50   46   11   24   62   65   10    1    0    5   55   29   87]\n",
      " [  40 4877   13    1   17    6   31    4    2    0    1    0    4    4]\n",
      " [  36    8 4570   12   83    1   11    0    1    0    1  104   33  140]\n",
      " [   3    1   51 4909   20    2    4    0    0    2    1    1    5    1]\n",
      " [  15   15   84   15 4842    4    7    0    1    1    0    0    5   11]\n",
      " [  45    4    2    2    2 4912   10    5    0    3    2    3    7    3]\n",
      " [  56   65   11    4   16   16 4783   31    4    1    0    3    3    7]\n",
      " [   4    2    2    1    4    3   31 4935    9    3    3    0    1    2]\n",
      " [   4    9    4    1    7    0   30   58 4880    3    0    0    2    2]\n",
      " [   2    5    9    3    5    1    1   14    2 4809  141    0    3    5]\n",
      " [  13    2    4    0    1    2    2   12    0   87 4869    1    2    5]\n",
      " [  13    3   60    4    4    2    6    3    0    2    0 4868   21   14]\n",
      " [  10    4   27    9    5   10    9    6    0    0    1   35 4812   72]\n",
      " [  63   26   66   17   25    6   15    1    2    4    2   17   89 4667]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      5000\n",
      "           1       0.96      0.98      0.97      5000\n",
      "           2       0.92      0.91      0.92      5000\n",
      "           3       0.98      0.98      0.98      5000\n",
      "           4       0.96      0.97      0.96      5000\n",
      "           5       0.98      0.98      0.98      5000\n",
      "           6       0.96      0.96      0.96      5000\n",
      "           7       0.97      0.99      0.98      5000\n",
      "           8       1.00      0.98      0.99      5000\n",
      "           9       0.98      0.96      0.97      5000\n",
      "          10       0.97      0.97      0.97      5000\n",
      "          11       0.96      0.97      0.97      5000\n",
      "          12       0.96      0.96      0.96      5000\n",
      "          13       0.93      0.93      0.93      5000\n",
      "\n",
      "    accuracy                           0.96     70000\n",
      "   macro avg       0.96      0.96      0.96     70000\n",
      "weighted avg       0.96      0.96      0.96     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(0, len(test_tokens), batch_size):\n",
    "        inputs = test_tokens[i:i + batch_size]\n",
    "        # bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "\n",
    "        # for j in range(batch_size):\n",
    "        #     for token in inputs[j]:\n",
    "        #         if token in vocab_dict:\n",
    "        #             bow[j][vocab_dict[token]] += 1\n",
    "        #         else:\n",
    "        #             bow[j][vocab_dict['UNK']] += 1\n",
    "\n",
    "        bow = calculate_tfidf(inputs, vocab_dict)\n",
    "\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        labels = test_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the {total} test inputs: {accuracy} %')\n",
    "\n",
    "    # Calculate confusion matrix, precision, recall, and F1 scores\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    classification_rep = classification_report(true_labels, predictions, target_names=[str(i) for i in range(14)])\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    # print(\"F1 Micro:\", f1_micro)\n",
    "    # print(\"F1 Macro:\", f1_macro)\n",
    "    # print(\"F1 Weighted:\", f1_weighted)\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
