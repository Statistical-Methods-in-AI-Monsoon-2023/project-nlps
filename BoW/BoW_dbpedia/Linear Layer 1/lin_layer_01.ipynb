{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('dbpedia.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 560000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 70000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data_title = train_dataset['title']\n",
    "train_data_content = train_dataset['content']\n",
    "train_labels = train_dataset['label']\n",
    "test_data_title = test_dataset['title']\n",
    "test_data_content = test_dataset['content']\n",
    "test_labels = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_title[0])\n",
    "# print(train_data_content[0])\n",
    "# print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data_title + train_data_content\n",
    "# test_data = test_data_title + test_data_content\n",
    "\n",
    "train_data = [None] * len(train_data_title)\n",
    "for i in range(len(train_data_title)):\n",
    "    train_data[i] = train_data_title[i] + train_data_content[i]\n",
    "\n",
    "test_data = [None] * len(test_data_title)\n",
    "for i in range(len(test_data_title)):\n",
    "    test_data[i] = test_data_title[i] + test_data_content[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def custom_contractions_fix(text):\n",
    "#     # Define custom contractions to expand\n",
    "#     contractions_dict = {\n",
    "#         \"don't\": \"do not\",\n",
    "#         \"doesn't\": \"does not\",\n",
    "#         \"didn't\": \"did not\",\n",
    "#         # Add more contractions as needed\n",
    "#     }\n",
    "    \n",
    "#     # Use a regular expression to find and replace contractions\n",
    "#     contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "#     def replace(match):\n",
    "#         return contractions_dict[match.group(0)]\n",
    "    \n",
    "#     expanded_text = contractions_re.sub(replace, text)\n",
    "#     return expanded_text\n",
    "\n",
    "\n",
    "# def tokenize_text(text):\n",
    "    \n",
    "#     # if there is a word between () then write it twice\n",
    "#     text = re.sub(r'\\((.*?)\\)', r'\\1 \\1', text)\n",
    "    \n",
    "#     # Apply custom contractions expansion\n",
    "#     text = custom_contractions_fix(text)\n",
    "    \n",
    "#     # Replace hyphens with spaces\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     temp = [word for word in tokens if len(word) > 1 and word.isupper()]\n",
    "#     tokens.extend(temp)\n",
    "    \n",
    "#     # Convert to lowercase\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "#     # Remove periods\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # Remove punctuation and stopwords\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(tokenize_text(train_data[10]))\n",
    "\n",
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('dbpedia_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('dbpedia_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('dbpedia_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('dbpedia_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('dbpedia_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 30\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('dbpedia_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dbpedia_train_tokens_filtered.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mdbpedia_train_tokens_filtered.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     train_tokens \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m      7\u001b[0m test_tokens \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dbpedia_train_tokens_filtered.pkl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('dbpedia_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('dbpedia_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36315\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the order of the training data and the testing data along with their labels with a specific seed\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "train_data = []\n",
    "new_train_labels = []\n",
    "\n",
    "test_data = []\n",
    "new_test_labels = []\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    train_data.append(train_tokens[i])\n",
    "    new_train_labels.append(train_labels[i])\n",
    "\n",
    "for i in range(len(test_tokens)):\n",
    "    test_data.append(test_tokens[i])\n",
    "    new_test_labels.append(test_labels[i])\n",
    "\n",
    "# shuffle the data\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(train_data, new_train_labels))\n",
    "random.shuffle(temp)\n",
    "train_data, train_labels = zip(*temp)\n",
    "\n",
    "random.seed(random_seed)\n",
    "temp = list(zip(test_data, new_test_labels))\n",
    "random.shuffle(temp)\n",
    "test_data, test_labels = zip(*temp)\n",
    "\n",
    "# convert them back to the format of train_tokens and test_tokens\n",
    "train_tokens = list(train_data)\n",
    "train_labels = list(train_labels)\n",
    "test_tokens = list(test_data)\n",
    "test_labels = list(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 4\n",
    "learning_rate = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 14  # Output size is 13 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/d7g63tw141g81ynhx4f_hngr0000gn/T/ipykernel_1633/2154661103.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Batch [1], Loss: 2.639680862426758\n",
      "Epoch [1/4], Batch [2], Loss: 2.3701698780059814\n",
      "Epoch [1/4], Batch [3], Loss: 2.1155292987823486\n",
      "Epoch [1/4], Batch [4], Loss: 1.88688063621521\n",
      "Epoch [1/4], Batch [5], Loss: 1.6768887042999268\n",
      "Epoch [1/4], Batch [6], Loss: 1.485598087310791\n",
      "Epoch [1/4], Batch [7], Loss: 1.3217095136642456\n",
      "Epoch [1/4], Batch [8], Loss: 1.1722667217254639\n",
      "Epoch [1/4], Batch [9], Loss: 1.0590989589691162\n",
      "Epoch [1/4], Batch [10], Loss: 0.9593086838722229\n",
      "Epoch [1/4], Batch [11], Loss: 0.870664656162262\n",
      "Epoch [1/4], Batch [12], Loss: 0.7856693863868713\n",
      "Epoch [1/4], Batch [13], Loss: 0.7300308346748352\n",
      "Epoch [1/4], Batch [14], Loss: 0.6807668209075928\n",
      "Epoch [1/4], Batch [15], Loss: 0.6308932304382324\n",
      "Epoch [1/4], Batch [16], Loss: 0.5899574160575867\n",
      "Epoch [1/4], Batch [17], Loss: 0.5491325855255127\n",
      "Epoch [1/4], Batch [18], Loss: 0.5169516801834106\n",
      "Epoch [1/4], Batch [19], Loss: 0.48686420917510986\n",
      "Epoch [1/4], Batch [20], Loss: 0.4715655744075775\n",
      "Epoch [1/4], Batch [21], Loss: 0.42984285950660706\n",
      "Epoch [1/4], Batch [22], Loss: 0.4189990758895874\n",
      "Epoch [1/4], Batch [23], Loss: 0.3971102237701416\n",
      "Epoch [1/4], Batch [24], Loss: 0.3743659555912018\n",
      "Epoch [1/4], Batch [25], Loss: 0.37531009316444397\n",
      "Epoch [1/4], Batch [26], Loss: 0.3580220937728882\n",
      "Epoch [1/4], Batch [27], Loss: 0.3309391438961029\n",
      "Epoch [1/4], Batch [28], Loss: 0.32749542593955994\n",
      "Epoch [1/4], Batch [29], Loss: 0.3248008191585541\n",
      "Epoch [1/4], Batch [30], Loss: 0.3070642352104187\n",
      "Epoch [1/4], Batch [31], Loss: 0.2948663830757141\n",
      "Epoch [1/4], Batch [32], Loss: 0.2890269160270691\n",
      "Epoch [1/4], Batch [33], Loss: 0.27655142545700073\n",
      "Epoch [1/4], Batch [34], Loss: 0.27330711483955383\n",
      "Epoch [1/4], Batch [35], Loss: 0.27572375535964966\n",
      "Epoch [1/4], Batch [36], Loss: 0.2612234652042389\n",
      "Epoch [1/4], Batch [37], Loss: 0.2605981230735779\n",
      "Epoch [1/4], Batch [38], Loss: 0.2465541958808899\n",
      "Epoch [1/4], Batch [39], Loss: 0.24711497128009796\n",
      "Epoch [1/4], Batch [40], Loss: 0.23788684606552124\n",
      "Epoch [1/4], Batch [41], Loss: 0.23864787817001343\n",
      "Epoch [1/4], Batch [42], Loss: 0.2345903515815735\n",
      "Epoch [1/4], Batch [43], Loss: 0.22887147963047028\n",
      "Epoch [1/4], Batch [44], Loss: 0.23443078994750977\n",
      "Epoch [1/4], Batch [45], Loss: 0.22230693697929382\n",
      "Epoch [1/4], Batch [46], Loss: 0.2134793996810913\n",
      "Epoch [1/4], Batch [47], Loss: 0.21274645626544952\n",
      "Epoch [1/4], Batch [48], Loss: 0.2075634002685547\n",
      "Epoch [1/4], Batch [49], Loss: 0.20700517296791077\n",
      "Epoch [1/4], Batch [50], Loss: 0.200709730386734\n",
      "Epoch [1/4], Batch [51], Loss: 0.20321312546730042\n",
      "Epoch [1/4], Batch [52], Loss: 0.19444532692432404\n",
      "Epoch [1/4], Batch [53], Loss: 0.19585050642490387\n",
      "Epoch [1/4], Batch [54], Loss: 0.19138792157173157\n",
      "Epoch [1/4], Batch [55], Loss: 0.18943263590335846\n",
      "Epoch [1/4], Batch [56], Loss: 0.1967395395040512\n",
      "Accuracy of the network on the 560000 train inputs: 94.10982142857142 %\n",
      "Epoch [2/4], Batch [1], Loss: 0.15503384172916412\n",
      "Epoch [2/4], Batch [2], Loss: 0.15361890196800232\n",
      "Epoch [2/4], Batch [3], Loss: 0.15513178706169128\n",
      "Epoch [2/4], Batch [4], Loss: 0.15698735415935516\n",
      "Epoch [2/4], Batch [5], Loss: 0.1530710756778717\n",
      "Epoch [2/4], Batch [6], Loss: 0.15913164615631104\n",
      "Epoch [2/4], Batch [7], Loss: 0.14992061257362366\n",
      "Epoch [2/4], Batch [8], Loss: 0.14526169002056122\n",
      "Epoch [2/4], Batch [9], Loss: 0.14779658615589142\n",
      "Epoch [2/4], Batch [10], Loss: 0.14578783512115479\n",
      "Epoch [2/4], Batch [11], Loss: 0.14662127196788788\n",
      "Epoch [2/4], Batch [12], Loss: 0.14048780500888824\n",
      "Epoch [2/4], Batch [13], Loss: 0.1396838277578354\n",
      "Epoch [2/4], Batch [14], Loss: 0.14614933729171753\n",
      "Epoch [2/4], Batch [15], Loss: 0.1401325762271881\n",
      "Epoch [2/4], Batch [16], Loss: 0.14275908470153809\n",
      "Epoch [2/4], Batch [17], Loss: 0.13803493976593018\n",
      "Epoch [2/4], Batch [18], Loss: 0.13542461395263672\n",
      "Epoch [2/4], Batch [19], Loss: 0.13418985903263092\n",
      "Epoch [2/4], Batch [20], Loss: 0.13874687254428864\n",
      "Epoch [2/4], Batch [21], Loss: 0.12683187425136566\n",
      "Epoch [2/4], Batch [22], Loss: 0.1336153745651245\n",
      "Epoch [2/4], Batch [23], Loss: 0.1301695704460144\n",
      "Epoch [2/4], Batch [24], Loss: 0.12890537083148956\n",
      "Epoch [2/4], Batch [25], Loss: 0.13989782333374023\n",
      "Epoch [2/4], Batch [26], Loss: 0.13419704139232635\n",
      "Epoch [2/4], Batch [27], Loss: 0.12614113092422485\n",
      "Epoch [2/4], Batch [28], Loss: 0.12901316583156586\n",
      "Epoch [2/4], Batch [29], Loss: 0.12958037853240967\n",
      "Epoch [2/4], Batch [30], Loss: 0.12765945494174957\n",
      "Epoch [2/4], Batch [31], Loss: 0.12331606447696686\n",
      "Epoch [2/4], Batch [32], Loss: 0.12684965133666992\n",
      "Epoch [2/4], Batch [33], Loss: 0.11887092143297195\n",
      "Epoch [2/4], Batch [34], Loss: 0.12122033536434174\n",
      "Epoch [2/4], Batch [35], Loss: 0.1292269080877304\n",
      "Epoch [2/4], Batch [36], Loss: 0.12016087770462036\n",
      "Epoch [2/4], Batch [37], Loss: 0.12620045244693756\n",
      "Epoch [2/4], Batch [38], Loss: 0.11822891980409622\n",
      "Epoch [2/4], Batch [39], Loss: 0.12185589969158173\n",
      "Epoch [2/4], Batch [40], Loss: 0.11833728104829788\n",
      "Epoch [2/4], Batch [41], Loss: 0.12141145020723343\n",
      "Epoch [2/4], Batch [42], Loss: 0.12070207297801971\n",
      "Epoch [2/4], Batch [43], Loss: 0.11763237416744232\n",
      "Epoch [2/4], Batch [44], Loss: 0.12673498690128326\n",
      "Epoch [2/4], Batch [45], Loss: 0.11825691908597946\n",
      "Epoch [2/4], Batch [46], Loss: 0.11363080888986588\n",
      "Epoch [2/4], Batch [47], Loss: 0.11589405685663223\n",
      "Epoch [2/4], Batch [48], Loss: 0.11228998750448227\n",
      "Epoch [2/4], Batch [49], Loss: 0.1139870211482048\n",
      "Epoch [2/4], Batch [50], Loss: 0.1105557382106781\n",
      "Epoch [2/4], Batch [51], Loss: 0.11127930879592896\n",
      "Epoch [2/4], Batch [52], Loss: 0.1098129153251648\n",
      "Epoch [2/4], Batch [53], Loss: 0.11237256973981857\n",
      "Epoch [2/4], Batch [54], Loss: 0.10988154262304306\n",
      "Epoch [2/4], Batch [55], Loss: 0.11008170247077942\n",
      "Epoch [2/4], Batch [56], Loss: 0.11609941720962524\n",
      "Accuracy of the network on the 560000 train inputs: 98.21303571428571 %\n",
      "Epoch [3/4], Batch [1], Loss: 0.09571333974599838\n",
      "Epoch [3/4], Batch [2], Loss: 0.09558883309364319\n",
      "Epoch [3/4], Batch [3], Loss: 0.09701324999332428\n",
      "Epoch [3/4], Batch [4], Loss: 0.098905548453331\n",
      "Epoch [3/4], Batch [5], Loss: 0.09342753142118454\n",
      "Epoch [3/4], Batch [6], Loss: 0.10022485256195068\n",
      "Epoch [3/4], Batch [7], Loss: 0.09402772784233093\n",
      "Epoch [3/4], Batch [8], Loss: 0.09233483672142029\n",
      "Epoch [3/4], Batch [9], Loss: 0.09489273279905319\n",
      "Epoch [3/4], Batch [10], Loss: 0.09433116018772125\n",
      "Epoch [3/4], Batch [11], Loss: 0.09618552774190903\n",
      "Epoch [3/4], Batch [12], Loss: 0.08956301212310791\n",
      "Epoch [3/4], Batch [13], Loss: 0.08945494890213013\n",
      "Epoch [3/4], Batch [14], Loss: 0.09605811536312103\n",
      "Epoch [3/4], Batch [15], Loss: 0.09180089086294174\n",
      "Epoch [3/4], Batch [16], Loss: 0.09553498774766922\n",
      "Epoch [3/4], Batch [17], Loss: 0.09186407178640366\n",
      "Epoch [3/4], Batch [18], Loss: 0.08888012170791626\n",
      "Epoch [3/4], Batch [19], Loss: 0.08940514177083969\n",
      "Epoch [3/4], Batch [20], Loss: 0.09253863245248795\n",
      "Epoch [3/4], Batch [21], Loss: 0.0836649239063263\n",
      "Epoch [3/4], Batch [22], Loss: 0.09013193100690842\n",
      "Epoch [3/4], Batch [23], Loss: 0.08656848967075348\n",
      "Epoch [3/4], Batch [24], Loss: 0.08619961887598038\n",
      "Epoch [3/4], Batch [25], Loss: 0.09555554389953613\n",
      "Epoch [3/4], Batch [26], Loss: 0.09065178781747818\n",
      "Epoch [3/4], Batch [27], Loss: 0.0867476686835289\n",
      "Epoch [3/4], Batch [28], Loss: 0.08741439133882523\n",
      "Epoch [3/4], Batch [29], Loss: 0.08678079396486282\n",
      "Epoch [3/4], Batch [30], Loss: 0.08749306946992874\n",
      "Epoch [3/4], Batch [31], Loss: 0.08345908671617508\n",
      "Epoch [3/4], Batch [32], Loss: 0.08805473148822784\n",
      "Epoch [3/4], Batch [33], Loss: 0.08031846582889557\n",
      "Epoch [3/4], Batch [34], Loss: 0.08251460641622543\n",
      "Epoch [3/4], Batch [35], Loss: 0.08972103148698807\n",
      "Epoch [3/4], Batch [36], Loss: 0.08181539922952652\n",
      "Epoch [3/4], Batch [37], Loss: 0.08912043273448944\n",
      "Epoch [3/4], Batch [38], Loss: 0.08166687935590744\n",
      "Epoch [3/4], Batch [39], Loss: 0.08480418473482132\n",
      "Epoch [3/4], Batch [40], Loss: 0.0829746350646019\n",
      "Epoch [3/4], Batch [41], Loss: 0.08429624885320663\n",
      "Epoch [3/4], Batch [42], Loss: 0.08481940627098083\n",
      "Epoch [3/4], Batch [43], Loss: 0.08145836740732193\n",
      "Epoch [3/4], Batch [44], Loss: 0.09076262265443802\n",
      "Epoch [3/4], Batch [45], Loss: 0.08321899175643921\n",
      "Epoch [3/4], Batch [46], Loss: 0.07931414991617203\n",
      "Epoch [3/4], Batch [47], Loss: 0.08223964273929596\n",
      "Epoch [3/4], Batch [48], Loss: 0.07822109013795853\n",
      "Epoch [3/4], Batch [49], Loss: 0.08054379373788834\n",
      "Epoch [3/4], Batch [50], Loss: 0.07746533304452896\n",
      "Epoch [3/4], Batch [51], Loss: 0.07745322585105896\n",
      "Epoch [3/4], Batch [52], Loss: 0.07776685804128647\n",
      "Epoch [3/4], Batch [53], Loss: 0.08041875809431076\n",
      "Epoch [3/4], Batch [54], Loss: 0.07791455090045929\n",
      "Epoch [3/4], Batch [55], Loss: 0.07894152402877808\n",
      "Epoch [3/4], Batch [56], Loss: 0.08360424637794495\n",
      "Accuracy of the network on the 560000 train inputs: 98.78732142857143 %\n",
      "Epoch [4/4], Batch [1], Loss: 0.07078517228364944\n",
      "Epoch [4/4], Batch [2], Loss: 0.07132719457149506\n",
      "Epoch [4/4], Batch [3], Loss: 0.07190528512001038\n",
      "Epoch [4/4], Batch [4], Loss: 0.07395573705434799\n",
      "Epoch [4/4], Batch [5], Loss: 0.06784083694219589\n",
      "Epoch [4/4], Batch [6], Loss: 0.07385240495204926\n",
      "Epoch [4/4], Batch [7], Loss: 0.06882370263338089\n",
      "Epoch [4/4], Batch [8], Loss: 0.06880564242601395\n",
      "Epoch [4/4], Batch [9], Loss: 0.07104408740997314\n",
      "Epoch [4/4], Batch [10], Loss: 0.0703306645154953\n",
      "Epoch [4/4], Batch [11], Loss: 0.07255572825670242\n",
      "Epoch [4/4], Batch [12], Loss: 0.06591358035802841\n",
      "Epoch [4/4], Batch [13], Loss: 0.06578421592712402\n",
      "Epoch [4/4], Batch [14], Loss: 0.07134096324443817\n",
      "Epoch [4/4], Batch [15], Loss: 0.0683927908539772\n",
      "Epoch [4/4], Batch [16], Loss: 0.07287492603063583\n",
      "Epoch [4/4], Batch [17], Loss: 0.06947052478790283\n",
      "Epoch [4/4], Batch [18], Loss: 0.06640391796827316\n",
      "Epoch [4/4], Batch [19], Loss: 0.06795120984315872\n",
      "Epoch [4/4], Batch [20], Loss: 0.07004069536924362\n",
      "Epoch [4/4], Batch [21], Loss: 0.06274554878473282\n",
      "Epoch [4/4], Batch [22], Loss: 0.06844662874937057\n",
      "Epoch [4/4], Batch [23], Loss: 0.06486108154058456\n",
      "Epoch [4/4], Batch [24], Loss: 0.06494355946779251\n",
      "Epoch [4/4], Batch [25], Loss: 0.07289355993270874\n",
      "Epoch [4/4], Batch [26], Loss: 0.0680423229932785\n",
      "Epoch [4/4], Batch [27], Loss: 0.06666124612092972\n",
      "Epoch [4/4], Batch [28], Loss: 0.06595663726329803\n",
      "Epoch [4/4], Batch [29], Loss: 0.06533782929182053\n",
      "Epoch [4/4], Batch [30], Loss: 0.06663893163204193\n",
      "Epoch [4/4], Batch [31], Loss: 0.06288153678178787\n",
      "Epoch [4/4], Batch [32], Loss: 0.06764888018369675\n",
      "Epoch [4/4], Batch [33], Loss: 0.06052325293421745\n",
      "Epoch [4/4], Batch [34], Loss: 0.06248246505856514\n",
      "Epoch [4/4], Batch [35], Loss: 0.06861409544944763\n",
      "Epoch [4/4], Batch [36], Loss: 0.0617266483604908\n",
      "Epoch [4/4], Batch [37], Loss: 0.06914729624986649\n",
      "Epoch [4/4], Batch [38], Loss: 0.06209792569279671\n",
      "Epoch [4/4], Batch [39], Loss: 0.0651513934135437\n",
      "Epoch [4/4], Batch [40], Loss: 0.06383108347654343\n",
      "Epoch [4/4], Batch [41], Loss: 0.06399975717067719\n",
      "Epoch [4/4], Batch [42], Loss: 0.06539154052734375\n",
      "Epoch [4/4], Batch [43], Loss: 0.0619351863861084\n",
      "Epoch [4/4], Batch [44], Loss: 0.07095558941364288\n",
      "Epoch [4/4], Batch [45], Loss: 0.06418196856975555\n",
      "Epoch [4/4], Batch [46], Loss: 0.06098383292555809\n",
      "Epoch [4/4], Batch [47], Loss: 0.0638008639216423\n",
      "Epoch [4/4], Batch [48], Loss: 0.05963223800063133\n",
      "Epoch [4/4], Batch [49], Loss: 0.062010712921619415\n",
      "Epoch [4/4], Batch [50], Loss: 0.05926809832453728\n",
      "Epoch [4/4], Batch [51], Loss: 0.059165313839912415\n",
      "Epoch [4/4], Batch [52], Loss: 0.060118168592453\n",
      "Epoch [4/4], Batch [53], Loss: 0.06234433129429817\n",
      "Epoch [4/4], Batch [54], Loss: 0.060026030987501144\n",
      "Epoch [4/4], Batch [55], Loss: 0.061248041689395905\n",
      "Epoch [4/4], Batch [56], Loss: 0.0650586411356926\n",
      "Accuracy of the network on the 560000 train inputs: 99.1025 %\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "        # print(len(inputs))\n",
    "        # print(len(vocab))\n",
    "        \n",
    "        # make bow vector for inputs\n",
    "        bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "        # print(bow.shape)\n",
    "        for j in range(batch_size):\n",
    "            for token in inputs[j]:\n",
    "\n",
    "                bow[j][vocab_dict[token]] += 1\n",
    "\n",
    "        \n",
    "        # convert bow to tensor\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "        # Convert labels to LongTensors\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        # Print the loss for this batch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}')\n",
    "\n",
    "    print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the above model in its entirety in pkl file\n",
    "torch.save(model, 'bow_model_dbpedia.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/d7g63tw141g81ynhx4f_hngr0000gn/T/ipykernel_1633/3551140216.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 70000 test inputs: 97.45285714285714 %\n",
      "Confusion Matrix:\n",
      "[[4731   42   31    4   16   41   48    7    4    3    7   18    9   39]\n",
      " [  44 4899    5    1    6    0   32    4    2    1    2    1    0    3]\n",
      " [  31    5 4733    9   71    1    8    1    1    4    0   46   25   65]\n",
      " [   3    2   34 4933   12    0    0    1    1    8    0    2    1    3]\n",
      " [   9   10   73    9 4868    1    6    1    2    4    0    2    6    9]\n",
      " [  49    0    3    0    3 4921   10    4    3    1    1    1    2    2]\n",
      " [  73   44    9    0   11   12 4803   35    5    2    0    1    3    2]\n",
      " [   5    0    0    0    2    1   26 4953    9    1    0    1    1    1]\n",
      " [   2    1    1    2    1    0    9   13 4970    0    0    0    0    1]\n",
      " [   1    0    3    0    0    2    1    5    3 4901   82    0    0    2]\n",
      " [  13    0    0    0    0    2    1    7    1   91 4882    0    0    3]\n",
      " [   6    0   35    2    0    1    1    2    1    0    0 4930   14    8]\n",
      " [  10    1   17    4    0    1    2    1    0    2    0   17 4904   41]\n",
      " [  50    9   59    5   11    5    8    3    1    5    2   10   43 4789]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      5000\n",
      "           1       0.98      0.98      0.98      5000\n",
      "           2       0.95      0.95      0.95      5000\n",
      "           3       0.99      0.99      0.99      5000\n",
      "           4       0.97      0.97      0.97      5000\n",
      "           5       0.99      0.98      0.99      5000\n",
      "           6       0.97      0.96      0.96      5000\n",
      "           7       0.98      0.99      0.99      5000\n",
      "           8       0.99      0.99      0.99      5000\n",
      "           9       0.98      0.98      0.98      5000\n",
      "          10       0.98      0.98      0.98      5000\n",
      "          11       0.98      0.99      0.98      5000\n",
      "          12       0.98      0.98      0.98      5000\n",
      "          13       0.96      0.96      0.96      5000\n",
      "\n",
      "    accuracy                           0.97     70000\n",
      "   macro avg       0.97      0.97      0.97     70000\n",
      "weighted avg       0.97      0.97      0.97     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(0, len(test_tokens), batch_size):\n",
    "        inputs = test_tokens[i:i + batch_size]\n",
    "        bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            for token in inputs[j]:\n",
    "                if token in vocab_dict:\n",
    "                    bow[j][vocab_dict[token]] += 1\n",
    "                else:\n",
    "                    bow[j][vocab_dict['UNK']] += 1\n",
    "\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        labels = test_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the {total} test inputs: {accuracy} %')\n",
    "\n",
    "    # Calculate confusion matrix, precision, recall, and F1 scores\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    classification_rep = classification_report(true_labels, predictions, target_names=[str(i) for i in range(14)])\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    # print(\"F1 Micro:\", f1_micro)\n",
    "    # print(\"F1 Macro:\", f1_macro)\n",
    "    # print(\"F1 Weighted:\", f1_weighted)\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
