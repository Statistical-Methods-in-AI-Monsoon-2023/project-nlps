{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriteja/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "test_data = test_dataset['text']\n",
    "test_labels = test_dataset['label']\n",
    "\n",
    "# Convert labels to lists (optional)\n",
    "# train_labels = train_labels.tolist()\n",
    "# test_labels = test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sriteja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sriteja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /home/sriteja/anaconda3/lib/python3.10/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/sriteja/anaconda3/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re  # Import the regular expressions module\n",
    "\n",
    "# # use tokenizer to remove punctuation\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     # Replace \"!\" with \"exm\" using regular expressions\n",
    "#     text = re.sub(r'!', ' exm', text)\n",
    "    \n",
    "#     expanded_words = []\n",
    "#     for word in text.split():\n",
    "#         # using contractions.fix to expand the shortened words\n",
    "#         expanded_words.append(contractions.fix(word))   \n",
    "    \n",
    "#     expanded_text = ' '.join(expanded_words)\n",
    "#     # print(expanded_text)\n",
    "    \n",
    "#     text = expanded_text    \n",
    "    \n",
    "#     # replace a-b with a and b\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     # temp = [word if (len(word) > 1 and word.isupper()) else None for word in tokens]\n",
    "#     temp = []\n",
    "#     for word in tokens:\n",
    "#         if len(word) > 1 and word.isupper():\n",
    "#             temp.append(word)\n",
    "#     tokens.extend(x for x in temp if x)\n",
    "    \n",
    "#     # convert to lower case\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "#     # dr. = dr and st. = st and so on\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # remove punctuation\n",
    "#     # tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# # print(train_data[4])\n",
    "# # tokenize_text(train_data[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('yelp_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('yelp_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45287\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_train_tokens = train_tokens\n",
    "\n",
    "# # Build the BoW representation manually\n",
    "# # Create a vocabulary by collecting unique words from the training data\n",
    "# vocab_reduced = set()\n",
    "# for tokens in reduced_train_tokens:\n",
    "#     vocab_reduced.update(tokens)\n",
    "\n",
    "# # Create a dictionary to map words to indices in the vocabulary\n",
    "# vocab_dict_reduced = {}\n",
    "# for i, word in enumerate(vocab_reduced):\n",
    "#     vocab_dict_reduced[word] = i\n",
    "\n",
    "# print(len(vocab_dict_reduced))\n",
    "\n",
    "# train_bow = np.zeros((len(reduced_train_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow train\")\n",
    "# # Convert text to BoW vectors\n",
    "# for i, tokens in enumerate(reduced_train_tokens):\n",
    "#     if i % 5000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         train_bow[i][vocab_dict_reduced[token]] += 1\n",
    "\n",
    "\n",
    "\n",
    "# reduced_test_tokens = test_tokens[:5000]\n",
    "# test_bow = np.zeros((len(reduced_test_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow test\")\n",
    "\n",
    "# for i, tokens in enumerate(reduced_test_tokens):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         if token in vocab_dict_reduced:\n",
    "#             test_bow[i][vocab_dict_reduced[token]] += 1\n",
    "#         else:    # if there is an unknown word, add it to the UNK column \n",
    "#             test_bow[i][vocab_dict_reduced['UNK']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 5  # Output size is 5 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59067/1574498921.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(bow, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1], Loss: 1.611720085144043\n",
      "Epoch [1/10], Batch [2], Loss: 1.5434863567352295\n",
      "Epoch [1/10], Batch [3], Loss: 1.4610981941223145\n",
      "Epoch [1/10], Batch [4], Loss: 1.3896337747573853\n",
      "Epoch [1/10], Batch [5], Loss: 1.3513985872268677\n",
      "Epoch [1/10], Batch [6], Loss: 1.3095823526382446\n",
      "Epoch [1/10], Batch [7], Loss: 1.2709180116653442\n",
      "Epoch [1/10], Batch [8], Loss: 1.2528363466262817\n",
      "Epoch [1/10], Batch [9], Loss: 1.252113699913025\n",
      "Epoch [1/10], Batch [10], Loss: 1.2170946598052979\n",
      "Epoch [1/10], Batch [11], Loss: 1.2019774913787842\n",
      "Epoch [1/10], Batch [12], Loss: 1.188161015510559\n",
      "Epoch [1/10], Batch [13], Loss: 1.1883318424224854\n",
      "Epoch [1/10], Batch [14], Loss: 1.1776231527328491\n",
      "Epoch [1/10], Batch [15], Loss: 1.1259827613830566\n",
      "Epoch [1/10], Batch [16], Loss: 1.2076561450958252\n",
      "Epoch [1/10], Batch [17], Loss: 1.1732213497161865\n",
      "Epoch [1/10], Batch [18], Loss: 1.168060302734375\n",
      "Epoch [1/10], Batch [19], Loss: 1.170930027961731\n",
      "Epoch [1/10], Batch [20], Loss: 1.1557295322418213\n",
      "Epoch [1/10], Batch [21], Loss: 1.1447298526763916\n",
      "Epoch [1/10], Batch [22], Loss: 1.1499769687652588\n",
      "Epoch [1/10], Batch [23], Loss: 1.1286752223968506\n",
      "Epoch [1/10], Batch [24], Loss: 1.1293916702270508\n",
      "Epoch [1/10], Batch [25], Loss: 1.0561214685440063\n",
      "Epoch [1/10], Batch [26], Loss: 1.041465401649475\n",
      "Epoch [1/10], Batch [27], Loss: 1.0486841201782227\n",
      "Epoch [1/10], Batch [28], Loss: 1.1041630506515503\n",
      "Epoch [1/10], Batch [29], Loss: 1.1111798286437988\n",
      "Epoch [1/10], Batch [30], Loss: 1.0889397859573364\n",
      "Epoch [1/10], Batch [31], Loss: 1.0794410705566406\n",
      "Epoch [1/10], Batch [32], Loss: 1.0724934339523315\n",
      "Epoch [1/10], Batch [33], Loss: 1.0881563425064087\n",
      "Epoch [1/10], Batch [34], Loss: 1.1785420179367065\n",
      "Epoch [1/10], Batch [35], Loss: 1.0547651052474976\n",
      "Epoch [1/10], Batch [36], Loss: 1.0969656705856323\n",
      "Epoch [1/10], Batch [37], Loss: 1.0661062002182007\n",
      "Epoch [1/10], Batch [38], Loss: 1.0681662559509277\n",
      "Epoch [1/10], Batch [39], Loss: 1.0830485820770264\n",
      "Epoch [1/10], Batch [40], Loss: 1.0513535737991333\n",
      "Epoch [1/10], Batch [41], Loss: 1.0314743518829346\n",
      "Epoch [1/10], Batch [42], Loss: 1.0384422540664673\n",
      "Epoch [1/10], Batch [43], Loss: 1.0298577547073364\n",
      "Epoch [1/10], Batch [44], Loss: 0.9676578044891357\n",
      "Epoch [1/10], Batch [45], Loss: 1.0014675855636597\n",
      "Epoch [1/10], Batch [46], Loss: 1.070462942123413\n",
      "Epoch [1/10], Batch [47], Loss: 1.1026325225830078\n",
      "Epoch [1/10], Batch [48], Loss: 1.0230309963226318\n",
      "Epoch [1/10], Batch [49], Loss: 1.0094330310821533\n",
      "Epoch [1/10], Batch [50], Loss: 1.0660127401351929\n",
      "Epoch [1/10], Batch [51], Loss: 1.0166269540786743\n",
      "Epoch [1/10], Batch [52], Loss: 0.9811967015266418\n",
      "Epoch [1/10], Batch [53], Loss: 0.9944570064544678\n",
      "Epoch [1/10], Batch [54], Loss: 1.0027587413787842\n",
      "Epoch [1/10], Batch [55], Loss: 0.9981037378311157\n",
      "Epoch [1/10], Batch [56], Loss: 0.98795485496521\n",
      "Epoch [1/10], Batch [57], Loss: 0.9635213613510132\n",
      "Epoch [1/10], Batch [58], Loss: 1.0009773969650269\n",
      "Epoch [1/10], Batch [59], Loss: 0.9784685373306274\n",
      "Epoch [1/10], Batch [60], Loss: 0.961250901222229\n",
      "Epoch [1/10], Batch [61], Loss: 0.9484505653381348\n",
      "Epoch [1/10], Batch [62], Loss: 0.9630778431892395\n",
      "Epoch [1/10], Batch [63], Loss: 0.9380714893341064\n",
      "Epoch [1/10], Batch [64], Loss: 0.9233331084251404\n",
      "Epoch [1/10], Batch [65], Loss: 0.9080967903137207\n",
      "Epoch [2/10], Batch [1], Loss: 0.9651749730110168\n",
      "Epoch [2/10], Batch [2], Loss: 0.9772896766662598\n",
      "Epoch [2/10], Batch [3], Loss: 0.9272111058235168\n",
      "Epoch [2/10], Batch [4], Loss: 0.9428806900978088\n",
      "Epoch [2/10], Batch [5], Loss: 0.9364991188049316\n",
      "Epoch [2/10], Batch [6], Loss: 0.9555780291557312\n",
      "Epoch [2/10], Batch [7], Loss: 0.9309585094451904\n",
      "Epoch [2/10], Batch [8], Loss: 0.962653636932373\n",
      "Epoch [2/10], Batch [9], Loss: 0.9832468628883362\n",
      "Epoch [2/10], Batch [10], Loss: 0.9580639600753784\n",
      "Epoch [2/10], Batch [11], Loss: 0.9560011625289917\n",
      "Epoch [2/10], Batch [12], Loss: 0.9666478633880615\n",
      "Epoch [2/10], Batch [13], Loss: 1.0060335397720337\n",
      "Epoch [2/10], Batch [14], Loss: 0.9764524698257446\n",
      "Epoch [2/10], Batch [15], Loss: 0.9381412863731384\n",
      "Epoch [2/10], Batch [16], Loss: 1.031975507736206\n",
      "Epoch [2/10], Batch [17], Loss: 1.0028339624404907\n",
      "Epoch [2/10], Batch [18], Loss: 1.008217453956604\n",
      "Epoch [2/10], Batch [19], Loss: 1.0093364715576172\n",
      "Epoch [2/10], Batch [20], Loss: 1.002962350845337\n",
      "Epoch [2/10], Batch [21], Loss: 0.9929655194282532\n",
      "Epoch [2/10], Batch [22], Loss: 0.9949700236320496\n",
      "Epoch [2/10], Batch [23], Loss: 0.9860590696334839\n",
      "Epoch [2/10], Batch [24], Loss: 0.9940299987792969\n",
      "Epoch [2/10], Batch [25], Loss: 0.9354809522628784\n",
      "Epoch [2/10], Batch [26], Loss: 0.9213345646858215\n",
      "Epoch [2/10], Batch [27], Loss: 0.9263339638710022\n",
      "Epoch [2/10], Batch [28], Loss: 0.9880595803260803\n",
      "Epoch [2/10], Batch [29], Loss: 0.9976572394371033\n",
      "Epoch [2/10], Batch [30], Loss: 0.9816761016845703\n",
      "Epoch [2/10], Batch [31], Loss: 0.9750849604606628\n",
      "Epoch [2/10], Batch [32], Loss: 0.9714540839195251\n",
      "Epoch [2/10], Batch [33], Loss: 0.9806326031684875\n",
      "Epoch [2/10], Batch [34], Loss: 1.072439193725586\n",
      "Epoch [2/10], Batch [35], Loss: 0.9488527178764343\n",
      "Epoch [2/10], Batch [36], Loss: 0.9768603444099426\n",
      "Epoch [2/10], Batch [37], Loss: 0.9654546976089478\n",
      "Epoch [2/10], Batch [38], Loss: 0.9742298126220703\n",
      "Epoch [2/10], Batch [39], Loss: 0.9872216582298279\n",
      "Epoch [2/10], Batch [40], Loss: 0.9586846828460693\n",
      "Epoch [2/10], Batch [41], Loss: 0.9385882019996643\n",
      "Epoch [2/10], Batch [42], Loss: 0.9538258910179138\n",
      "Epoch [2/10], Batch [43], Loss: 0.9413418173789978\n",
      "Epoch [2/10], Batch [44], Loss: 0.8810682892799377\n",
      "Epoch [2/10], Batch [45], Loss: 0.9142385721206665\n",
      "Epoch [2/10], Batch [46], Loss: 0.973766028881073\n",
      "Epoch [2/10], Batch [47], Loss: 0.978527843952179\n",
      "Epoch [2/10], Batch [48], Loss: 0.9344918727874756\n",
      "Epoch [2/10], Batch [49], Loss: 0.9339406490325928\n",
      "Epoch [2/10], Batch [50], Loss: 0.9863781332969666\n",
      "Epoch [2/10], Batch [51], Loss: 0.9385849833488464\n",
      "Epoch [2/10], Batch [52], Loss: 0.9036437273025513\n",
      "Epoch [2/10], Batch [53], Loss: 0.919485867023468\n",
      "Epoch [2/10], Batch [54], Loss: 0.9253110289573669\n",
      "Epoch [2/10], Batch [55], Loss: 0.9198194146156311\n",
      "Epoch [2/10], Batch [56], Loss: 0.9164797067642212\n",
      "Epoch [2/10], Batch [57], Loss: 0.8927629590034485\n",
      "Epoch [2/10], Batch [58], Loss: 0.9314313530921936\n",
      "Epoch [2/10], Batch [59], Loss: 0.9125962853431702\n",
      "Epoch [2/10], Batch [60], Loss: 0.8895248770713806\n",
      "Epoch [2/10], Batch [61], Loss: 0.8834814429283142\n",
      "Epoch [2/10], Batch [62], Loss: 0.9050310254096985\n",
      "Epoch [2/10], Batch [63], Loss: 0.8709652423858643\n",
      "Epoch [2/10], Batch [64], Loss: 0.8585109114646912\n",
      "Epoch [2/10], Batch [65], Loss: 0.8418132066726685\n",
      "Epoch [3/10], Batch [1], Loss: 0.9308173060417175\n",
      "Epoch [3/10], Batch [2], Loss: 0.9408472776412964\n",
      "Epoch [3/10], Batch [3], Loss: 0.8905373811721802\n",
      "Epoch [3/10], Batch [4], Loss: 0.905113935470581\n",
      "Epoch [3/10], Batch [5], Loss: 0.8997822403907776\n",
      "Epoch [3/10], Batch [6], Loss: 0.920480489730835\n",
      "Epoch [3/10], Batch [7], Loss: 0.895052433013916\n",
      "Epoch [3/10], Batch [8], Loss: 0.9259994029998779\n",
      "Epoch [3/10], Batch [9], Loss: 0.9463969469070435\n",
      "Epoch [3/10], Batch [10], Loss: 0.9163119196891785\n",
      "Epoch [3/10], Batch [11], Loss: 0.9153661131858826\n",
      "Epoch [3/10], Batch [12], Loss: 0.9276537895202637\n",
      "Epoch [3/10], Batch [13], Loss: 0.9686828255653381\n",
      "Epoch [3/10], Batch [14], Loss: 0.939504086971283\n",
      "Epoch [3/10], Batch [15], Loss: 0.9050820469856262\n",
      "Epoch [3/10], Batch [16], Loss: 0.9925028085708618\n",
      "Epoch [3/10], Batch [17], Loss: 0.9623281359672546\n",
      "Epoch [3/10], Batch [18], Loss: 0.9684460163116455\n",
      "Epoch [3/10], Batch [19], Loss: 0.9735967516899109\n",
      "Epoch [3/10], Batch [20], Loss: 0.9671230316162109\n",
      "Epoch [3/10], Batch [21], Loss: 0.9580737352371216\n",
      "Epoch [3/10], Batch [22], Loss: 0.9596627950668335\n",
      "Epoch [3/10], Batch [23], Loss: 0.9490545988082886\n",
      "Epoch [3/10], Batch [24], Loss: 0.959851086139679\n",
      "Epoch [3/10], Batch [25], Loss: 0.9027180671691895\n",
      "Epoch [3/10], Batch [26], Loss: 0.8904252052307129\n",
      "Epoch [3/10], Batch [27], Loss: 0.8936169147491455\n",
      "Epoch [3/10], Batch [28], Loss: 0.9549481272697449\n",
      "Epoch [3/10], Batch [29], Loss: 0.963301956653595\n",
      "Epoch [3/10], Batch [30], Loss: 0.9469619393348694\n",
      "Epoch [3/10], Batch [31], Loss: 0.943831741809845\n",
      "Epoch [3/10], Batch [32], Loss: 0.939339280128479\n",
      "Epoch [3/10], Batch [33], Loss: 0.9466159343719482\n",
      "Epoch [3/10], Batch [34], Loss: 0.9733225703239441\n",
      "Epoch [3/10], Batch [35], Loss: 0.9116407036781311\n",
      "Epoch [3/10], Batch [36], Loss: 0.9362666010856628\n",
      "Epoch [3/10], Batch [37], Loss: 0.9312282204627991\n",
      "Epoch [3/10], Batch [38], Loss: 0.9397155046463013\n",
      "Epoch [3/10], Batch [39], Loss: 0.9534141421318054\n",
      "Epoch [3/10], Batch [40], Loss: 0.924538254737854\n",
      "Epoch [3/10], Batch [41], Loss: 0.9039661884307861\n",
      "Epoch [3/10], Batch [42], Loss: 0.9241718053817749\n",
      "Epoch [3/10], Batch [43], Loss: 0.9100938439369202\n",
      "Epoch [3/10], Batch [44], Loss: 0.8481576442718506\n",
      "Epoch [3/10], Batch [45], Loss: 0.8818753957748413\n",
      "Epoch [3/10], Batch [46], Loss: 0.933771014213562\n",
      "Epoch [3/10], Batch [47], Loss: 0.9309844970703125\n",
      "Epoch [3/10], Batch [48], Loss: 0.8960379958152771\n",
      "Epoch [3/10], Batch [49], Loss: 0.8923659324645996\n",
      "Epoch [3/10], Batch [50], Loss: 0.9514726400375366\n",
      "Epoch [3/10], Batch [51], Loss: 0.9056113362312317\n",
      "Epoch [3/10], Batch [52], Loss: 0.8621072173118591\n",
      "Epoch [3/10], Batch [53], Loss: 0.8817979693412781\n",
      "Epoch [3/10], Batch [54], Loss: 0.8873571157455444\n",
      "Epoch [3/10], Batch [55], Loss: 0.8814229369163513\n",
      "Epoch [3/10], Batch [56], Loss: 0.8711341619491577\n",
      "Epoch [3/10], Batch [57], Loss: 0.8595097661018372\n",
      "Epoch [3/10], Batch [58], Loss: 0.8926312327384949\n",
      "Epoch [3/10], Batch [59], Loss: 0.8747740983963013\n",
      "Epoch [3/10], Batch [60], Loss: 0.8509582877159119\n",
      "Epoch [3/10], Batch [61], Loss: 0.8419185280799866\n",
      "Epoch [3/10], Batch [62], Loss: 0.8556241393089294\n",
      "Epoch [3/10], Batch [63], Loss: 0.8412358164787292\n",
      "Epoch [3/10], Batch [64], Loss: 0.8268117308616638\n",
      "Epoch [3/10], Batch [65], Loss: 0.808112382888794\n",
      "Epoch [4/10], Batch [1], Loss: 0.9123132824897766\n",
      "Epoch [4/10], Batch [2], Loss: 0.9227238297462463\n",
      "Epoch [4/10], Batch [3], Loss: 0.8724886775016785\n",
      "Epoch [4/10], Batch [4], Loss: 0.8876064419746399\n",
      "Epoch [4/10], Batch [5], Loss: 0.8802259564399719\n",
      "Epoch [4/10], Batch [6], Loss: 0.9006789922714233\n",
      "Epoch [4/10], Batch [7], Loss: 0.8760759830474854\n",
      "Epoch [4/10], Batch [8], Loss: 0.905348539352417\n",
      "Epoch [4/10], Batch [9], Loss: 0.9258148670196533\n",
      "Epoch [4/10], Batch [10], Loss: 0.8943316340446472\n",
      "Epoch [4/10], Batch [11], Loss: 0.8934413194656372\n",
      "Epoch [4/10], Batch [12], Loss: 0.9053343534469604\n",
      "Epoch [4/10], Batch [13], Loss: 0.9473973512649536\n",
      "Epoch [4/10], Batch [14], Loss: 0.9187257885932922\n",
      "Epoch [4/10], Batch [15], Loss: 0.887664258480072\n",
      "Epoch [4/10], Batch [16], Loss: 0.9700677990913391\n",
      "Epoch [4/10], Batch [17], Loss: 0.9418255090713501\n",
      "Epoch [4/10], Batch [18], Loss: 0.9478574991226196\n",
      "Epoch [4/10], Batch [19], Loss: 0.9526022672653198\n",
      "Epoch [4/10], Batch [20], Loss: 0.9471924901008606\n",
      "Epoch [4/10], Batch [21], Loss: 0.9380984306335449\n",
      "Epoch [4/10], Batch [22], Loss: 0.9390819072723389\n",
      "Epoch [4/10], Batch [23], Loss: 0.9283596873283386\n",
      "Epoch [4/10], Batch [24], Loss: 0.9403879642486572\n",
      "Epoch [4/10], Batch [25], Loss: 0.8835194110870361\n",
      "Epoch [4/10], Batch [26], Loss: 0.8722254633903503\n",
      "Epoch [4/10], Batch [27], Loss: 0.875620424747467\n",
      "Epoch [4/10], Batch [28], Loss: 0.936767578125\n",
      "Epoch [4/10], Batch [29], Loss: 0.9444076418876648\n",
      "Epoch [4/10], Batch [30], Loss: 0.926872193813324\n",
      "Epoch [4/10], Batch [31], Loss: 0.9240351319313049\n",
      "Epoch [4/10], Batch [32], Loss: 0.9208311438560486\n",
      "Epoch [4/10], Batch [33], Loss: 0.9278732538223267\n",
      "Epoch [4/10], Batch [34], Loss: 0.9680730700492859\n",
      "Epoch [4/10], Batch [35], Loss: 0.8910880088806152\n",
      "Epoch [4/10], Batch [36], Loss: 0.9094526171684265\n",
      "Epoch [4/10], Batch [37], Loss: 0.9115055799484253\n",
      "Epoch [4/10], Batch [38], Loss: 0.9200685620307922\n",
      "Epoch [4/10], Batch [39], Loss: 0.9355494379997253\n",
      "Epoch [4/10], Batch [40], Loss: 0.906042754650116\n",
      "Epoch [4/10], Batch [41], Loss: 0.8847039937973022\n",
      "Epoch [4/10], Batch [42], Loss: 0.9086600542068481\n",
      "Epoch [4/10], Batch [43], Loss: 0.8939552903175354\n",
      "Epoch [4/10], Batch [44], Loss: 0.8289518356323242\n",
      "Epoch [4/10], Batch [45], Loss: 0.863454282283783\n",
      "Epoch [4/10], Batch [46], Loss: 0.9111703038215637\n",
      "Epoch [4/10], Batch [47], Loss: 0.8986415863037109\n",
      "Epoch [4/10], Batch [48], Loss: 0.8728891611099243\n",
      "Epoch [4/10], Batch [49], Loss: 0.8759172558784485\n",
      "Epoch [4/10], Batch [50], Loss: 0.9353228807449341\n",
      "Epoch [4/10], Batch [51], Loss: 0.8910113573074341\n",
      "Epoch [4/10], Batch [52], Loss: 0.8446381688117981\n",
      "Epoch [4/10], Batch [53], Loss: 0.8659475445747375\n",
      "Epoch [4/10], Batch [54], Loss: 0.8692850470542908\n",
      "Epoch [4/10], Batch [55], Loss: 0.8653214573860168\n",
      "Epoch [4/10], Batch [56], Loss: 0.8573980331420898\n",
      "Epoch [4/10], Batch [57], Loss: 0.8479585647583008\n",
      "Epoch [4/10], Batch [58], Loss: 0.8785862326622009\n",
      "Epoch [4/10], Batch [59], Loss: 0.8640449047088623\n",
      "Epoch [4/10], Batch [60], Loss: 0.838878333568573\n",
      "Epoch [4/10], Batch [61], Loss: 0.8311339616775513\n",
      "Epoch [4/10], Batch [62], Loss: 0.8415952920913696\n",
      "Epoch [4/10], Batch [63], Loss: 0.8241114020347595\n",
      "Epoch [4/10], Batch [64], Loss: 0.8097822666168213\n",
      "Epoch [4/10], Batch [65], Loss: 0.7920926213264465\n",
      "Epoch [5/10], Batch [1], Loss: 0.8984284996986389\n",
      "Epoch [5/10], Batch [2], Loss: 0.9078812599182129\n",
      "Epoch [5/10], Batch [3], Loss: 0.8585622310638428\n",
      "Epoch [5/10], Batch [4], Loss: 0.8760952949523926\n",
      "Epoch [5/10], Batch [5], Loss: 0.8680750727653503\n",
      "Epoch [5/10], Batch [6], Loss: 0.8876122832298279\n",
      "Epoch [5/10], Batch [7], Loss: 0.8630699515342712\n",
      "Epoch [5/10], Batch [8], Loss: 0.8905115127563477\n",
      "Epoch [5/10], Batch [9], Loss: 0.9110659956932068\n",
      "Epoch [5/10], Batch [10], Loss: 0.8788696527481079\n",
      "Epoch [5/10], Batch [11], Loss: 0.8780890703201294\n",
      "Epoch [5/10], Batch [12], Loss: 0.8906945586204529\n",
      "Epoch [5/10], Batch [13], Loss: 0.9321096539497375\n",
      "Epoch [5/10], Batch [14], Loss: 0.9045422077178955\n",
      "Epoch [5/10], Batch [15], Loss: 0.8753460645675659\n",
      "Epoch [5/10], Batch [16], Loss: 0.9551210999488831\n",
      "Epoch [5/10], Batch [17], Loss: 0.9288654327392578\n",
      "Epoch [5/10], Batch [18], Loss: 0.9320632815361023\n",
      "Epoch [5/10], Batch [19], Loss: 0.938498318195343\n",
      "Epoch [5/10], Batch [20], Loss: 0.9338092803955078\n",
      "Epoch [5/10], Batch [21], Loss: 0.9235832095146179\n",
      "Epoch [5/10], Batch [22], Loss: 0.9250549077987671\n",
      "Epoch [5/10], Batch [23], Loss: 0.9146556854248047\n",
      "Epoch [5/10], Batch [24], Loss: 0.9273614287376404\n",
      "Epoch [5/10], Batch [25], Loss: 0.8702359199523926\n",
      "Epoch [5/10], Batch [26], Loss: 0.8593171834945679\n",
      "Epoch [5/10], Batch [27], Loss: 0.8631994128227234\n",
      "Epoch [5/10], Batch [28], Loss: 0.9242289066314697\n",
      "Epoch [5/10], Batch [29], Loss: 0.9314249157905579\n",
      "Epoch [5/10], Batch [30], Loss: 0.9127480387687683\n",
      "Epoch [5/10], Batch [31], Loss: 0.9096075296401978\n",
      "Epoch [5/10], Batch [32], Loss: 0.9085575938224792\n",
      "Epoch [5/10], Batch [33], Loss: 0.9134886860847473\n",
      "Epoch [5/10], Batch [34], Loss: 0.9213008880615234\n",
      "Epoch [5/10], Batch [35], Loss: 0.8781582713127136\n",
      "Epoch [5/10], Batch [36], Loss: 0.8906881809234619\n",
      "Epoch [5/10], Batch [37], Loss: 0.8975111246109009\n",
      "Epoch [5/10], Batch [38], Loss: 0.9064503908157349\n",
      "Epoch [5/10], Batch [39], Loss: 0.9220997095108032\n",
      "Epoch [5/10], Batch [40], Loss: 0.8886036276817322\n",
      "Epoch [5/10], Batch [41], Loss: 0.8696190714836121\n",
      "Epoch [5/10], Batch [42], Loss: 0.8949723839759827\n",
      "Epoch [5/10], Batch [43], Loss: 0.8752115964889526\n",
      "Epoch [5/10], Batch [44], Loss: 0.8162964582443237\n",
      "Epoch [5/10], Batch [45], Loss: 0.847471296787262\n",
      "Epoch [5/10], Batch [46], Loss: 0.8931566476821899\n",
      "Epoch [5/10], Batch [47], Loss: 0.9092584848403931\n",
      "Epoch [5/10], Batch [48], Loss: 0.858741819858551\n",
      "Epoch [5/10], Batch [49], Loss: 0.8621428608894348\n",
      "Epoch [5/10], Batch [50], Loss: 0.917198657989502\n",
      "Epoch [5/10], Batch [51], Loss: 0.8706328272819519\n",
      "Epoch [5/10], Batch [52], Loss: 0.8293551802635193\n",
      "Epoch [5/10], Batch [53], Loss: 0.8480914831161499\n",
      "Epoch [5/10], Batch [54], Loss: 0.8502846956253052\n",
      "Epoch [5/10], Batch [55], Loss: 0.8441838026046753\n",
      "Epoch [5/10], Batch [56], Loss: 0.8389840126037598\n",
      "Epoch [5/10], Batch [57], Loss: 0.8293443322181702\n",
      "Epoch [5/10], Batch [58], Loss: 0.8590156435966492\n",
      "Epoch [5/10], Batch [59], Loss: 0.8437394499778748\n",
      "Epoch [5/10], Batch [60], Loss: 0.8162322044372559\n",
      "Epoch [5/10], Batch [61], Loss: 0.8121241927146912\n",
      "Epoch [5/10], Batch [62], Loss: 0.8385751843452454\n",
      "Epoch [5/10], Batch [63], Loss: 0.8144413232803345\n",
      "Epoch [5/10], Batch [64], Loss: 0.7959747314453125\n",
      "Epoch [5/10], Batch [65], Loss: 0.7767705321311951\n",
      "Epoch [6/10], Batch [1], Loss: 0.8900212645530701\n",
      "Epoch [6/10], Batch [2], Loss: 0.8978517651557922\n",
      "Epoch [6/10], Batch [3], Loss: 0.8480405211448669\n",
      "Epoch [6/10], Batch [4], Loss: 0.8680572509765625\n",
      "Epoch [6/10], Batch [5], Loss: 0.8599498867988586\n",
      "Epoch [6/10], Batch [6], Loss: 0.8786861300468445\n",
      "Epoch [6/10], Batch [7], Loss: 0.8536656498908997\n",
      "Epoch [6/10], Batch [8], Loss: 0.8794543743133545\n",
      "Epoch [6/10], Batch [9], Loss: 0.8992263674736023\n",
      "Epoch [6/10], Batch [10], Loss: 0.8676802515983582\n",
      "Epoch [6/10], Batch [11], Loss: 0.8671509027481079\n",
      "Epoch [6/10], Batch [12], Loss: 0.8800933361053467\n",
      "Epoch [6/10], Batch [13], Loss: 0.9209115505218506\n",
      "Epoch [6/10], Batch [14], Loss: 0.893241822719574\n",
      "Epoch [6/10], Batch [15], Loss: 0.8657016754150391\n",
      "Epoch [6/10], Batch [16], Loss: 0.943732500076294\n",
      "Epoch [6/10], Batch [17], Loss: 0.9175897240638733\n",
      "Epoch [6/10], Batch [18], Loss: 0.9220563769340515\n",
      "Epoch [6/10], Batch [19], Loss: 0.9279758930206299\n",
      "Epoch [6/10], Batch [20], Loss: 0.924260139465332\n",
      "Epoch [6/10], Batch [21], Loss: 0.9125640392303467\n",
      "Epoch [6/10], Batch [22], Loss: 0.9144810438156128\n",
      "Epoch [6/10], Batch [23], Loss: 0.9039101600646973\n",
      "Epoch [6/10], Batch [24], Loss: 0.9157511591911316\n",
      "Epoch [6/10], Batch [25], Loss: 0.8607035279273987\n",
      "Epoch [6/10], Batch [26], Loss: 0.8494239449501038\n",
      "Epoch [6/10], Batch [27], Loss: 0.8533669114112854\n",
      "Epoch [6/10], Batch [28], Loss: 0.9142650365829468\n",
      "Epoch [6/10], Batch [29], Loss: 0.9219624996185303\n",
      "Epoch [6/10], Batch [30], Loss: 0.902679979801178\n",
      "Epoch [6/10], Batch [31], Loss: 0.8982465863227844\n",
      "Epoch [6/10], Batch [32], Loss: 0.8977186679840088\n",
      "Epoch [6/10], Batch [33], Loss: 0.9026914238929749\n",
      "Epoch [6/10], Batch [34], Loss: 0.8837093710899353\n",
      "Epoch [6/10], Batch [35], Loss: 0.866706371307373\n",
      "Epoch [6/10], Batch [36], Loss: 0.8761388659477234\n",
      "Epoch [6/10], Batch [37], Loss: 0.8870002031326294\n",
      "Epoch [6/10], Batch [38], Loss: 0.8961190581321716\n",
      "Epoch [6/10], Batch [39], Loss: 0.9128271341323853\n",
      "Epoch [6/10], Batch [40], Loss: 0.8782830834388733\n",
      "Epoch [6/10], Batch [41], Loss: 0.8587655425071716\n",
      "Epoch [6/10], Batch [42], Loss: 0.8856834769248962\n",
      "Epoch [6/10], Batch [43], Loss: 0.8656207919120789\n",
      "Epoch [6/10], Batch [44], Loss: 0.8064491152763367\n",
      "Epoch [6/10], Batch [45], Loss: 0.8372856378555298\n",
      "Epoch [6/10], Batch [46], Loss: 0.8816845417022705\n",
      "Epoch [6/10], Batch [47], Loss: 0.864876925945282\n",
      "Epoch [6/10], Batch [48], Loss: 0.84946608543396\n",
      "Epoch [6/10], Batch [49], Loss: 0.8525678515434265\n",
      "Epoch [6/10], Batch [50], Loss: 0.907276451587677\n",
      "Epoch [6/10], Batch [51], Loss: 0.8611604571342468\n",
      "Epoch [6/10], Batch [52], Loss: 0.8198438286781311\n",
      "Epoch [6/10], Batch [53], Loss: 0.8383939266204834\n",
      "Epoch [6/10], Batch [54], Loss: 0.8395218849182129\n",
      "Epoch [6/10], Batch [55], Loss: 0.8338108658790588\n",
      "Epoch [6/10], Batch [56], Loss: 0.826728343963623\n",
      "Epoch [6/10], Batch [57], Loss: 0.8187583088874817\n",
      "Epoch [6/10], Batch [58], Loss: 0.8494447469711304\n",
      "Epoch [6/10], Batch [59], Loss: 0.8336053490638733\n",
      "Epoch [6/10], Batch [60], Loss: 0.8062784075737\n",
      "Epoch [6/10], Batch [61], Loss: 0.797877848148346\n",
      "Epoch [6/10], Batch [62], Loss: 0.8088825345039368\n",
      "Epoch [6/10], Batch [63], Loss: 0.8048504590988159\n",
      "Epoch [6/10], Batch [64], Loss: 0.7871068716049194\n",
      "Epoch [6/10], Batch [65], Loss: 0.7667003870010376\n",
      "Epoch [7/10], Batch [1], Loss: 0.8839625716209412\n",
      "Epoch [7/10], Batch [2], Loss: 0.8905393481254578\n",
      "Epoch [7/10], Batch [3], Loss: 0.8403106331825256\n",
      "Epoch [7/10], Batch [4], Loss: 0.8605766892433167\n",
      "Epoch [7/10], Batch [5], Loss: 0.8534071445465088\n",
      "Epoch [7/10], Batch [6], Loss: 0.8722691535949707\n",
      "Epoch [7/10], Batch [7], Loss: 0.8479660153388977\n",
      "Epoch [7/10], Batch [8], Loss: 0.8724811673164368\n",
      "Epoch [7/10], Batch [9], Loss: 0.8908461928367615\n",
      "Epoch [7/10], Batch [10], Loss: 0.8582307696342468\n",
      "Epoch [7/10], Batch [11], Loss: 0.8591723442077637\n",
      "Epoch [7/10], Batch [12], Loss: 0.8721632957458496\n",
      "Epoch [7/10], Batch [13], Loss: 0.9134786128997803\n",
      "Epoch [7/10], Batch [14], Loss: 0.8846877813339233\n",
      "Epoch [7/10], Batch [15], Loss: 0.8578858375549316\n",
      "Epoch [7/10], Batch [16], Loss: 0.9344237446784973\n",
      "Epoch [7/10], Batch [17], Loss: 0.9100764393806458\n",
      "Epoch [7/10], Batch [18], Loss: 0.9146274328231812\n",
      "Epoch [7/10], Batch [19], Loss: 0.9200466871261597\n",
      "Epoch [7/10], Batch [20], Loss: 0.916528046131134\n",
      "Epoch [7/10], Batch [21], Loss: 0.9046762585639954\n",
      "Epoch [7/10], Batch [22], Loss: 0.9064335823059082\n",
      "Epoch [7/10], Batch [23], Loss: 0.8952271342277527\n",
      "Epoch [7/10], Batch [24], Loss: 0.9076221585273743\n",
      "Epoch [7/10], Batch [25], Loss: 0.8529278039932251\n",
      "Epoch [7/10], Batch [26], Loss: 0.8416549563407898\n",
      "Epoch [7/10], Batch [27], Loss: 0.8456228375434875\n",
      "Epoch [7/10], Batch [28], Loss: 0.9055246114730835\n",
      "Epoch [7/10], Batch [29], Loss: 0.9136376976966858\n",
      "Epoch [7/10], Batch [30], Loss: 0.894874095916748\n",
      "Epoch [7/10], Batch [31], Loss: 0.8895078301429749\n",
      "Epoch [7/10], Batch [32], Loss: 0.8893793225288391\n",
      "Epoch [7/10], Batch [33], Loss: 0.8937503099441528\n",
      "Epoch [7/10], Batch [34], Loss: 0.8745293021202087\n",
      "Epoch [7/10], Batch [35], Loss: 0.8586058616638184\n",
      "Epoch [7/10], Batch [36], Loss: 0.8654491901397705\n",
      "Epoch [7/10], Batch [37], Loss: 0.879199206829071\n",
      "Epoch [7/10], Batch [38], Loss: 0.8888356685638428\n",
      "Epoch [7/10], Batch [39], Loss: 0.9054628610610962\n",
      "Epoch [7/10], Batch [40], Loss: 0.8704714775085449\n",
      "Epoch [7/10], Batch [41], Loss: 0.8518956303596497\n",
      "Epoch [7/10], Batch [42], Loss: 0.8784468770027161\n",
      "Epoch [7/10], Batch [43], Loss: 0.8587704300880432\n",
      "Epoch [7/10], Batch [44], Loss: 0.7992702722549438\n",
      "Epoch [7/10], Batch [45], Loss: 0.8296537399291992\n",
      "Epoch [7/10], Batch [46], Loss: 0.8720978498458862\n",
      "Epoch [7/10], Batch [47], Loss: 0.853263258934021\n",
      "Epoch [7/10], Batch [48], Loss: 0.8430168032646179\n",
      "Epoch [7/10], Batch [49], Loss: 0.8465960025787354\n",
      "Epoch [7/10], Batch [50], Loss: 0.9004814624786377\n",
      "Epoch [7/10], Batch [51], Loss: 0.8546643257141113\n",
      "Epoch [7/10], Batch [52], Loss: 0.8127932548522949\n",
      "Epoch [7/10], Batch [53], Loss: 0.8314559459686279\n",
      "Epoch [7/10], Batch [54], Loss: 0.8323510885238647\n",
      "Epoch [7/10], Batch [55], Loss: 0.8264480233192444\n",
      "Epoch [7/10], Batch [56], Loss: 0.819341778755188\n",
      "Epoch [7/10], Batch [57], Loss: 0.8098176717758179\n",
      "Epoch [7/10], Batch [58], Loss: 0.8410779237747192\n",
      "Epoch [7/10], Batch [59], Loss: 0.8260990381240845\n",
      "Epoch [7/10], Batch [60], Loss: 0.7981064915657043\n",
      "Epoch [7/10], Batch [61], Loss: 0.7893024682998657\n",
      "Epoch [7/10], Batch [62], Loss: 0.7934688329696655\n",
      "Epoch [7/10], Batch [63], Loss: 0.7970092296600342\n",
      "Epoch [7/10], Batch [64], Loss: 0.779472291469574\n",
      "Epoch [7/10], Batch [65], Loss: 0.7601426839828491\n",
      "Epoch [8/10], Batch [1], Loss: 0.8798151612281799\n",
      "Epoch [8/10], Batch [2], Loss: 0.885357141494751\n",
      "Epoch [8/10], Batch [3], Loss: 0.8338257670402527\n",
      "Epoch [8/10], Batch [4], Loss: 0.8559046983718872\n",
      "Epoch [8/10], Batch [5], Loss: 0.8486965894699097\n",
      "Epoch [8/10], Batch [6], Loss: 0.8671963810920715\n",
      "Epoch [8/10], Batch [7], Loss: 0.8433365225791931\n",
      "Epoch [8/10], Batch [8], Loss: 0.8676912188529968\n",
      "Epoch [8/10], Batch [9], Loss: 0.8848548531532288\n",
      "Epoch [8/10], Batch [10], Loss: 0.8515084981918335\n",
      "Epoch [8/10], Batch [11], Loss: 0.8529058694839478\n",
      "Epoch [8/10], Batch [12], Loss: 0.8661145567893982\n",
      "Epoch [8/10], Batch [13], Loss: 0.9081202149391174\n",
      "Epoch [8/10], Batch [14], Loss: 0.879051148891449\n",
      "Epoch [8/10], Batch [15], Loss: 0.8526632785797119\n",
      "Epoch [8/10], Batch [16], Loss: 0.9270980358123779\n",
      "Epoch [8/10], Batch [17], Loss: 0.9034748077392578\n",
      "Epoch [8/10], Batch [18], Loss: 0.9093829989433289\n",
      "Epoch [8/10], Batch [19], Loss: 0.9139062762260437\n",
      "Epoch [8/10], Batch [20], Loss: 0.9094122052192688\n",
      "Epoch [8/10], Batch [21], Loss: 0.8982479572296143\n",
      "Epoch [8/10], Batch [22], Loss: 0.9003840684890747\n",
      "Epoch [8/10], Batch [23], Loss: 0.8876904249191284\n",
      "Epoch [8/10], Batch [24], Loss: 0.9000490307807922\n",
      "Epoch [8/10], Batch [25], Loss: 0.8463740944862366\n",
      "Epoch [8/10], Batch [26], Loss: 0.8357851505279541\n",
      "Epoch [8/10], Batch [27], Loss: 0.8399254679679871\n",
      "Epoch [8/10], Batch [28], Loss: 0.8985428810119629\n",
      "Epoch [8/10], Batch [29], Loss: 0.9068058729171753\n",
      "Epoch [8/10], Batch [30], Loss: 0.8878694176673889\n",
      "Epoch [8/10], Batch [31], Loss: 0.8828885555267334\n",
      "Epoch [8/10], Batch [32], Loss: 0.8832917809486389\n",
      "Epoch [8/10], Batch [33], Loss: 0.8871871829032898\n",
      "Epoch [8/10], Batch [34], Loss: 0.8796188235282898\n",
      "Epoch [8/10], Batch [35], Loss: 0.8518650531768799\n",
      "Epoch [8/10], Batch [36], Loss: 0.8573134541511536\n",
      "Epoch [8/10], Batch [37], Loss: 0.8724390864372253\n",
      "Epoch [8/10], Batch [38], Loss: 0.8822504878044128\n",
      "Epoch [8/10], Batch [39], Loss: 0.8990106582641602\n",
      "Epoch [8/10], Batch [40], Loss: 0.8641631603240967\n",
      "Epoch [8/10], Batch [41], Loss: 0.847082793712616\n",
      "Epoch [8/10], Batch [42], Loss: 0.8724061250686646\n",
      "Epoch [8/10], Batch [43], Loss: 0.8517683744430542\n",
      "Epoch [8/10], Batch [44], Loss: 0.7934679985046387\n",
      "Epoch [8/10], Batch [45], Loss: 0.8234291076660156\n",
      "Epoch [8/10], Batch [46], Loss: 0.8643398284912109\n",
      "Epoch [8/10], Batch [47], Loss: 0.8368080258369446\n",
      "Epoch [8/10], Batch [48], Loss: 0.838797390460968\n",
      "Epoch [8/10], Batch [49], Loss: 0.8425288200378418\n",
      "Epoch [8/10], Batch [50], Loss: 0.8970249891281128\n",
      "Epoch [8/10], Batch [51], Loss: 0.851060152053833\n",
      "Epoch [8/10], Batch [52], Loss: 0.8082364797592163\n",
      "Epoch [8/10], Batch [53], Loss: 0.830371081829071\n",
      "Epoch [8/10], Batch [54], Loss: 0.8293553590774536\n",
      "Epoch [8/10], Batch [55], Loss: 0.8239771723747253\n",
      "Epoch [8/10], Batch [56], Loss: 0.816368818283081\n",
      "Epoch [8/10], Batch [57], Loss: 0.8047376871109009\n",
      "Epoch [8/10], Batch [58], Loss: 0.835601270198822\n",
      "Epoch [8/10], Batch [59], Loss: 0.8207195401191711\n",
      "Epoch [8/10], Batch [60], Loss: 0.7927451133728027\n",
      "Epoch [8/10], Batch [61], Loss: 0.7814359068870544\n",
      "Epoch [8/10], Batch [62], Loss: 0.7768280506134033\n",
      "Epoch [8/10], Batch [63], Loss: 0.7895804047584534\n",
      "Epoch [8/10], Batch [64], Loss: 0.7739933729171753\n",
      "Epoch [8/10], Batch [65], Loss: 0.7541355490684509\n",
      "Epoch [9/10], Batch [1], Loss: 0.8768838047981262\n",
      "Epoch [9/10], Batch [2], Loss: 0.882128119468689\n",
      "Epoch [9/10], Batch [3], Loss: 0.8290680646896362\n",
      "Epoch [9/10], Batch [4], Loss: 0.8508316278457642\n",
      "Epoch [9/10], Batch [5], Loss: 0.8443934321403503\n",
      "Epoch [9/10], Batch [6], Loss: 0.8633605241775513\n",
      "Epoch [9/10], Batch [7], Loss: 0.839982807636261\n",
      "Epoch [9/10], Batch [8], Loss: 0.864462673664093\n",
      "Epoch [9/10], Batch [9], Loss: 0.8808145523071289\n",
      "Epoch [9/10], Batch [10], Loss: 0.8463812470436096\n",
      "Epoch [9/10], Batch [11], Loss: 0.8478798866271973\n",
      "Epoch [9/10], Batch [12], Loss: 0.8610379695892334\n",
      "Epoch [9/10], Batch [13], Loss: 0.9042136669158936\n",
      "Epoch [9/10], Batch [14], Loss: 0.8743685483932495\n",
      "Epoch [9/10], Batch [15], Loss: 0.8491389751434326\n",
      "Epoch [9/10], Batch [16], Loss: 0.9213278293609619\n",
      "Epoch [9/10], Batch [17], Loss: 0.8983270525932312\n",
      "Epoch [9/10], Batch [18], Loss: 0.9044874310493469\n",
      "Epoch [9/10], Batch [19], Loss: 0.9093178510665894\n",
      "Epoch [9/10], Batch [20], Loss: 0.9049649238586426\n",
      "Epoch [9/10], Batch [21], Loss: 0.893502414226532\n",
      "Epoch [9/10], Batch [22], Loss: 0.8952749967575073\n",
      "Epoch [9/10], Batch [23], Loss: 0.8823972940444946\n",
      "Epoch [9/10], Batch [24], Loss: 0.8949199318885803\n",
      "Epoch [9/10], Batch [25], Loss: 0.8412478566169739\n",
      "Epoch [9/10], Batch [26], Loss: 0.8310840129852295\n",
      "Epoch [9/10], Batch [27], Loss: 0.8354167938232422\n",
      "Epoch [9/10], Batch [28], Loss: 0.8936693072319031\n",
      "Epoch [9/10], Batch [29], Loss: 0.9015227556228638\n",
      "Epoch [9/10], Batch [30], Loss: 0.8812991976737976\n",
      "Epoch [9/10], Batch [31], Loss: 0.8779599666595459\n",
      "Epoch [9/10], Batch [32], Loss: 0.8785900473594666\n",
      "Epoch [9/10], Batch [33], Loss: 0.8821883797645569\n",
      "Epoch [9/10], Batch [34], Loss: 0.8946181535720825\n",
      "Epoch [9/10], Batch [35], Loss: 0.84663987159729\n",
      "Epoch [9/10], Batch [36], Loss: 0.8504687547683716\n",
      "Epoch [9/10], Batch [37], Loss: 0.8671087026596069\n",
      "Epoch [9/10], Batch [38], Loss: 0.8773597478866577\n",
      "Epoch [9/10], Batch [39], Loss: 0.8936641812324524\n",
      "Epoch [9/10], Batch [40], Loss: 0.8589309453964233\n",
      "Epoch [9/10], Batch [41], Loss: 0.8421996235847473\n",
      "Epoch [9/10], Batch [42], Loss: 0.8680247068405151\n",
      "Epoch [9/10], Batch [43], Loss: 0.845971405506134\n",
      "Epoch [9/10], Batch [44], Loss: 0.7884088158607483\n",
      "Epoch [9/10], Batch [45], Loss: 0.8173074126243591\n",
      "Epoch [9/10], Batch [46], Loss: 0.8582644462585449\n",
      "Epoch [9/10], Batch [47], Loss: 0.8313185572624207\n",
      "Epoch [9/10], Batch [48], Loss: 0.8347472548484802\n",
      "Epoch [9/10], Batch [49], Loss: 0.8382507562637329\n",
      "Epoch [9/10], Batch [50], Loss: 0.8947985172271729\n",
      "Epoch [9/10], Batch [51], Loss: 0.8482869863510132\n",
      "Epoch [9/10], Batch [52], Loss: 0.8049176335334778\n",
      "Epoch [9/10], Batch [53], Loss: 0.8306121826171875\n",
      "Epoch [9/10], Batch [54], Loss: 0.826543927192688\n",
      "Epoch [9/10], Batch [55], Loss: 0.8221448063850403\n",
      "Epoch [9/10], Batch [56], Loss: 0.8153584003448486\n",
      "Epoch [9/10], Batch [57], Loss: 0.8034283518791199\n",
      "Epoch [9/10], Batch [58], Loss: 0.8328943252563477\n",
      "Epoch [9/10], Batch [59], Loss: 0.8181915283203125\n",
      "Epoch [9/10], Batch [60], Loss: 0.7893926501274109\n",
      "Epoch [9/10], Batch [61], Loss: 0.7824627161026001\n",
      "Epoch [9/10], Batch [62], Loss: 0.7779073119163513\n",
      "Epoch [9/10], Batch [63], Loss: 0.7851741909980774\n",
      "Epoch [9/10], Batch [64], Loss: 0.7702119946479797\n",
      "Epoch [9/10], Batch [65], Loss: 0.7516809701919556\n",
      "Epoch [10/10], Batch [1], Loss: 0.8731547594070435\n",
      "Epoch [10/10], Batch [2], Loss: 0.8795040249824524\n",
      "Epoch [10/10], Batch [3], Loss: 0.8257368206977844\n",
      "Epoch [10/10], Batch [4], Loss: 0.8472825884819031\n",
      "Epoch [10/10], Batch [5], Loss: 0.8403178453445435\n",
      "Epoch [10/10], Batch [6], Loss: 0.8596014380455017\n",
      "Epoch [10/10], Batch [7], Loss: 0.8369346857070923\n",
      "Epoch [10/10], Batch [8], Loss: 0.8614648580551147\n",
      "Epoch [10/10], Batch [9], Loss: 0.8780613541603088\n",
      "Epoch [10/10], Batch [10], Loss: 0.843190610408783\n",
      "Epoch [10/10], Batch [11], Loss: 0.8440791964530945\n",
      "Epoch [10/10], Batch [12], Loss: 0.8570552468299866\n",
      "Epoch [10/10], Batch [13], Loss: 0.9001762866973877\n",
      "Epoch [10/10], Batch [14], Loss: 0.8702996969223022\n",
      "Epoch [10/10], Batch [15], Loss: 0.8465558886528015\n",
      "Epoch [10/10], Batch [16], Loss: 0.9171332716941833\n",
      "Epoch [10/10], Batch [17], Loss: 0.8943053483963013\n",
      "Epoch [10/10], Batch [18], Loss: 0.900575578212738\n",
      "Epoch [10/10], Batch [19], Loss: 0.9049577116966248\n",
      "Epoch [10/10], Batch [20], Loss: 0.9010756015777588\n",
      "Epoch [10/10], Batch [21], Loss: 0.8904428482055664\n",
      "Epoch [10/10], Batch [22], Loss: 0.8904798030853271\n",
      "Epoch [10/10], Batch [23], Loss: 0.8775832056999207\n",
      "Epoch [10/10], Batch [24], Loss: 0.8906244039535522\n",
      "Epoch [10/10], Batch [25], Loss: 0.837507426738739\n",
      "Epoch [10/10], Batch [26], Loss: 0.8276405334472656\n",
      "Epoch [10/10], Batch [27], Loss: 0.8315191268920898\n",
      "Epoch [10/10], Batch [28], Loss: 0.8902249932289124\n",
      "Epoch [10/10], Batch [29], Loss: 0.897377073764801\n",
      "Epoch [10/10], Batch [30], Loss: 0.876149594783783\n",
      "Epoch [10/10], Batch [31], Loss: 0.8737184405326843\n",
      "Epoch [10/10], Batch [32], Loss: 0.8746625185012817\n",
      "Epoch [10/10], Batch [33], Loss: 0.8781324028968811\n",
      "Epoch [10/10], Batch [34], Loss: 0.8406064510345459\n",
      "Epoch [10/10], Batch [35], Loss: 0.8412466645240784\n",
      "Epoch [10/10], Batch [36], Loss: 0.8439199328422546\n",
      "Epoch [10/10], Batch [37], Loss: 0.8626562356948853\n",
      "Epoch [10/10], Batch [38], Loss: 0.8729817271232605\n",
      "Epoch [10/10], Batch [39], Loss: 0.8897838592529297\n",
      "Epoch [10/10], Batch [40], Loss: 0.8559093475341797\n",
      "Epoch [10/10], Batch [41], Loss: 0.8364794850349426\n",
      "Epoch [10/10], Batch [42], Loss: 0.8633728623390198\n",
      "Epoch [10/10], Batch [43], Loss: 0.8394864201545715\n",
      "Epoch [10/10], Batch [44], Loss: 0.78489089012146\n",
      "Epoch [10/10], Batch [45], Loss: 0.8117539286613464\n",
      "Epoch [10/10], Batch [46], Loss: 0.8520554900169373\n",
      "Epoch [10/10], Batch [47], Loss: 0.8152822852134705\n",
      "Epoch [10/10], Batch [48], Loss: 0.8191335201263428\n",
      "Epoch [10/10], Batch [49], Loss: 0.8264461159706116\n",
      "Epoch [10/10], Batch [50], Loss: 0.8820737600326538\n",
      "Epoch [10/10], Batch [51], Loss: 0.8358856439590454\n",
      "Epoch [10/10], Batch [52], Loss: 0.79746013879776\n",
      "Epoch [10/10], Batch [53], Loss: 0.8119235038757324\n",
      "Epoch [10/10], Batch [54], Loss: 0.8151578307151794\n",
      "Epoch [10/10], Batch [55], Loss: 0.8109029531478882\n",
      "Epoch [10/10], Batch [56], Loss: 0.8047017455101013\n",
      "Epoch [10/10], Batch [57], Loss: 0.7932794094085693\n",
      "Epoch [10/10], Batch [58], Loss: 0.8234150409698486\n",
      "Epoch [10/10], Batch [59], Loss: 0.8097057342529297\n",
      "Epoch [10/10], Batch [60], Loss: 0.7833184599876404\n",
      "Epoch [10/10], Batch [61], Loss: 0.7724219560623169\n",
      "Epoch [10/10], Batch [62], Loss: 0.7652463912963867\n",
      "Epoch [10/10], Batch [63], Loss: 0.7795891761779785\n",
      "Epoch [10/10], Batch [64], Loss: 0.7652484178543091\n",
      "Epoch [10/10], Batch [65], Loss: 0.7449185848236084\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "        # print(len(inputs))\n",
    "        # print(len(vocab))\n",
    "        \n",
    "        # make bow vector for inputs\n",
    "        bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "        # print(bow.shape)\n",
    "        for j in range(batch_size):\n",
    "            for token in inputs[j]:\n",
    "                # try :\n",
    "                bow[j][vocab_dict[token]] += 1\n",
    "                # except:\n",
    "                # print(token)\n",
    "                # bow[j][token] += 1\n",
    "        \n",
    "        # convert bow to tensor\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "        # Convert labels to LongTensors\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss for this batch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
