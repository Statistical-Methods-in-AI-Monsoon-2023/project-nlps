{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "test_data = test_dataset['text']\n",
    "test_labels = test_dataset['label']\n",
    "\n",
    "# Convert labels to lists (optional)\n",
    "# train_labels = train_labels.tolist()\n",
    "# test_labels = test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sriteja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sriteja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /home/sriteja/anaconda3/lib/python3.10/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/sriteja/anaconda3/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re  # Import the regular expressions module\n",
    "\n",
    "# # use tokenizer to remove punctuation\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     # Replace \"!\" with \"exm\" using regular expressions\n",
    "#     text = re.sub(r'!', ' exm', text)\n",
    "    \n",
    "#     expanded_words = []\n",
    "#     for word in text.split():\n",
    "#         # using contractions.fix to expand the shortened words\n",
    "#         expanded_words.append(contractions.fix(word))   \n",
    "    \n",
    "#     expanded_text = ' '.join(expanded_words)\n",
    "#     # print(expanded_text)\n",
    "    \n",
    "#     text = expanded_text    \n",
    "    \n",
    "#     # replace a-b with a and b\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     # temp = [word if (len(word) > 1 and word.isupper()) else None for word in tokens]\n",
    "#     temp = []\n",
    "#     for word in tokens:\n",
    "#         if len(word) > 1 and word.isupper():\n",
    "#             temp.append(word)\n",
    "#     tokens.extend(x for x in temp if x)\n",
    "    \n",
    "#     # convert to lower case\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "#     # dr. = dr and st. = st and so on\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # remove punctuation\n",
    "#     # tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# # print(train_data[4])\n",
    "# # tokenize_text(train_data[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('yelp_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('yelp_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45287\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_train_tokens = train_tokens\n",
    "\n",
    "# # Build the BoW representation manually\n",
    "# # Create a vocabulary by collecting unique words from the training data\n",
    "# vocab_reduced = set()\n",
    "# for tokens in reduced_train_tokens:\n",
    "#     vocab_reduced.update(tokens)\n",
    "\n",
    "# # Create a dictionary to map words to indices in the vocabulary\n",
    "# vocab_dict_reduced = {}\n",
    "# for i, word in enumerate(vocab_reduced):\n",
    "#     vocab_dict_reduced[word] = i\n",
    "\n",
    "# print(len(vocab_dict_reduced))\n",
    "\n",
    "# train_bow = np.zeros((len(reduced_train_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow train\")\n",
    "# # Convert text to BoW vectors\n",
    "# for i, tokens in enumerate(reduced_train_tokens):\n",
    "#     if i % 5000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         train_bow[i][vocab_dict_reduced[token]] += 1\n",
    "\n",
    "\n",
    "\n",
    "# reduced_test_tokens = test_tokens[:5000]\n",
    "# test_bow = np.zeros((len(reduced_test_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow test\")\n",
    "\n",
    "# for i, tokens in enumerate(reduced_test_tokens):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         if token in vocab_dict_reduced:\n",
    "#             test_bow[i][vocab_dict_reduced[token]] += 1\n",
    "#         else:    # if there is an unknown word, add it to the UNK column \n",
    "#             test_bow[i][vocab_dict_reduced['UNK']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.05\n",
    "batch_size = 5000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 5  # Output size is 5 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1], Loss: 1.6094996929168701\n",
      "Epoch [1/10], Batch [2], Loss: 1.5401923656463623\n",
      "Epoch [1/10], Batch [3], Loss: 1.4586058855056763\n",
      "Epoch [1/10], Batch [4], Loss: 1.3866132497787476\n",
      "Epoch [1/10], Batch [5], Loss: 1.3490192890167236\n",
      "Epoch [1/10], Batch [6], Loss: 1.3084720373153687\n",
      "Epoch [1/10], Batch [7], Loss: 1.2692068815231323\n",
      "Epoch [1/10], Batch [8], Loss: 1.2505382299423218\n",
      "Epoch [1/10], Batch [9], Loss: 1.2511183023452759\n",
      "Epoch [1/10], Batch [10], Loss: 1.2164318561553955\n",
      "Epoch [1/10], Batch [11], Loss: 1.2010159492492676\n",
      "Epoch [1/10], Batch [12], Loss: 1.1869926452636719\n",
      "Epoch [1/10], Batch [13], Loss: 1.1876052618026733\n",
      "Epoch [1/10], Batch [14], Loss: 1.176352858543396\n",
      "Epoch [1/10], Batch [15], Loss: 1.1256908178329468\n",
      "Epoch [1/10], Batch [16], Loss: 1.207547664642334\n",
      "Epoch [1/10], Batch [17], Loss: 1.1719365119934082\n",
      "Epoch [1/10], Batch [18], Loss: 1.1670870780944824\n",
      "Epoch [1/10], Batch [19], Loss: 1.170340657234192\n",
      "Epoch [1/10], Batch [20], Loss: 1.154589056968689\n",
      "Epoch [1/10], Batch [21], Loss: 1.143921136856079\n",
      "Epoch [1/10], Batch [22], Loss: 1.1493024826049805\n",
      "Epoch [1/10], Batch [23], Loss: 1.1289821863174438\n",
      "Epoch [1/10], Batch [24], Loss: 1.129572868347168\n",
      "Epoch [1/10], Batch [25], Loss: 1.0553208589553833\n",
      "Epoch [1/10], Batch [26], Loss: 1.0414491891860962\n",
      "Epoch [1/10], Batch [27], Loss: 1.0491505861282349\n",
      "Epoch [1/10], Batch [28], Loss: 1.1038446426391602\n",
      "Epoch [1/10], Batch [29], Loss: 1.1107673645019531\n",
      "Epoch [1/10], Batch [30], Loss: 1.0885200500488281\n",
      "Epoch [1/10], Batch [31], Loss: 1.0798605680465698\n",
      "Epoch [1/10], Batch [32], Loss: 1.0726640224456787\n",
      "Epoch [1/10], Batch [33], Loss: 1.0887165069580078\n",
      "Epoch [1/10], Batch [34], Loss: 1.178511381149292\n",
      "Epoch [1/10], Batch [35], Loss: 1.055393934249878\n",
      "Epoch [1/10], Batch [36], Loss: 1.0976251363754272\n",
      "Epoch [1/10], Batch [37], Loss: 1.0666561126708984\n",
      "Epoch [1/10], Batch [38], Loss: 1.0689541101455688\n",
      "Epoch [1/10], Batch [39], Loss: 1.082395076751709\n",
      "Epoch [1/10], Batch [40], Loss: 1.0521425008773804\n",
      "Epoch [1/10], Batch [41], Loss: 1.0306240320205688\n",
      "Epoch [1/10], Batch [42], Loss: 1.0389081239700317\n",
      "Epoch [1/10], Batch [43], Loss: 1.0303802490234375\n",
      "Epoch [1/10], Batch [44], Loss: 0.9675409197807312\n",
      "Epoch [1/10], Batch [45], Loss: 1.0014764070510864\n",
      "Epoch [1/10], Batch [46], Loss: 1.070812463760376\n",
      "Epoch [1/10], Batch [47], Loss: 1.1021294593811035\n",
      "Epoch [1/10], Batch [48], Loss: 1.0226335525512695\n",
      "Epoch [1/10], Batch [49], Loss: 1.0089079141616821\n",
      "Epoch [1/10], Batch [50], Loss: 1.066821813583374\n",
      "Epoch [1/10], Batch [51], Loss: 1.0169864892959595\n",
      "Epoch [1/10], Batch [52], Loss: 0.9807645678520203\n",
      "Epoch [1/10], Batch [53], Loss: 0.9945129752159119\n",
      "Epoch [1/10], Batch [54], Loss: 1.0024049282073975\n",
      "Epoch [1/10], Batch [55], Loss: 0.9973776340484619\n",
      "Epoch [1/10], Batch [56], Loss: 0.987527072429657\n",
      "Epoch [1/10], Batch [57], Loss: 0.9631676077842712\n",
      "Epoch [1/10], Batch [58], Loss: 1.0008819103240967\n",
      "Epoch [1/10], Batch [59], Loss: 0.9782889485359192\n",
      "Epoch [1/10], Batch [60], Loss: 0.9608865976333618\n",
      "Epoch [1/10], Batch [61], Loss: 0.9479379057884216\n",
      "Epoch [1/10], Batch [62], Loss: 0.9631257057189941\n",
      "Epoch [1/10], Batch [63], Loss: 0.9370452165603638\n",
      "Epoch [1/10], Batch [64], Loss: 0.9229427576065063\n",
      "Epoch [1/10], Batch [65], Loss: 0.9077068567276001\n",
      "Accuracy of the network on the 650000 train inputs: 55.03830769230769 %\n",
      "Epoch [2/10], Batch [1], Loss: 0.9645088911056519\n",
      "Epoch [2/10], Batch [2], Loss: 0.9763989448547363\n",
      "Epoch [2/10], Batch [3], Loss: 0.9271255135536194\n",
      "Epoch [2/10], Batch [4], Loss: 0.9428620338439941\n",
      "Epoch [2/10], Batch [5], Loss: 0.9365541934967041\n",
      "Epoch [2/10], Batch [6], Loss: 0.9552147388458252\n",
      "Epoch [2/10], Batch [7], Loss: 0.9308897256851196\n",
      "Epoch [2/10], Batch [8], Loss: 0.9624453186988831\n",
      "Epoch [2/10], Batch [9], Loss: 0.9831784963607788\n",
      "Epoch [2/10], Batch [10], Loss: 0.9579257965087891\n",
      "Epoch [2/10], Batch [11], Loss: 0.9561836123466492\n",
      "Epoch [2/10], Batch [12], Loss: 0.9662625193595886\n",
      "Epoch [2/10], Batch [13], Loss: 1.005896806716919\n",
      "Epoch [2/10], Batch [14], Loss: 0.9759677648544312\n",
      "Epoch [2/10], Batch [15], Loss: 0.9381137490272522\n",
      "Epoch [2/10], Batch [16], Loss: 1.032067894935608\n",
      "Epoch [2/10], Batch [17], Loss: 1.0029315948486328\n",
      "Epoch [2/10], Batch [18], Loss: 1.007525086402893\n",
      "Epoch [2/10], Batch [19], Loss: 1.009119987487793\n",
      "Epoch [2/10], Batch [20], Loss: 1.0027894973754883\n",
      "Epoch [2/10], Batch [21], Loss: 0.9930219650268555\n",
      "Epoch [2/10], Batch [22], Loss: 0.9948517680168152\n",
      "Epoch [2/10], Batch [23], Loss: 0.9859727621078491\n",
      "Epoch [2/10], Batch [24], Loss: 0.994213879108429\n",
      "Epoch [2/10], Batch [25], Loss: 0.9353489279747009\n",
      "Epoch [2/10], Batch [26], Loss: 0.9213047623634338\n",
      "Epoch [2/10], Batch [27], Loss: 0.9266764521598816\n",
      "Epoch [2/10], Batch [28], Loss: 0.9876326322555542\n",
      "Epoch [2/10], Batch [29], Loss: 0.9974297881126404\n",
      "Epoch [2/10], Batch [30], Loss: 0.9818903207778931\n",
      "Epoch [2/10], Batch [31], Loss: 0.9751236438751221\n",
      "Epoch [2/10], Batch [32], Loss: 0.9715625047683716\n",
      "Epoch [2/10], Batch [33], Loss: 0.9812914133071899\n",
      "Epoch [2/10], Batch [34], Loss: 1.0762676000595093\n",
      "Epoch [2/10], Batch [35], Loss: 0.9499205350875854\n",
      "Epoch [2/10], Batch [36], Loss: 0.9770174026489258\n",
      "Epoch [2/10], Batch [37], Loss: 0.9661750793457031\n",
      "Epoch [2/10], Batch [38], Loss: 0.9753038287162781\n",
      "Epoch [2/10], Batch [39], Loss: 0.9864171147346497\n",
      "Epoch [2/10], Batch [40], Loss: 0.959343433380127\n",
      "Epoch [2/10], Batch [41], Loss: 0.938060462474823\n",
      "Epoch [2/10], Batch [42], Loss: 0.9547440409660339\n",
      "Epoch [2/10], Batch [43], Loss: 0.9420445561408997\n",
      "Epoch [2/10], Batch [44], Loss: 0.8809967041015625\n",
      "Epoch [2/10], Batch [45], Loss: 0.9146135449409485\n",
      "Epoch [2/10], Batch [46], Loss: 0.9742015600204468\n",
      "Epoch [2/10], Batch [47], Loss: 0.977703332901001\n",
      "Epoch [2/10], Batch [48], Loss: 0.933784544467926\n",
      "Epoch [2/10], Batch [49], Loss: 0.933033287525177\n",
      "Epoch [2/10], Batch [50], Loss: 0.9865905046463013\n",
      "Epoch [2/10], Batch [51], Loss: 0.9380370378494263\n",
      "Epoch [2/10], Batch [52], Loss: 0.9028523564338684\n",
      "Epoch [2/10], Batch [53], Loss: 0.9193000197410583\n",
      "Epoch [2/10], Batch [54], Loss: 0.9249858260154724\n",
      "Epoch [2/10], Batch [55], Loss: 0.9196810722351074\n",
      "Epoch [2/10], Batch [56], Loss: 0.9157728552818298\n",
      "Epoch [2/10], Batch [57], Loss: 0.8923037052154541\n",
      "Epoch [2/10], Batch [58], Loss: 0.931361734867096\n",
      "Epoch [2/10], Batch [59], Loss: 0.9123795628547668\n",
      "Epoch [2/10], Batch [60], Loss: 0.8891681432723999\n",
      "Epoch [2/10], Batch [61], Loss: 0.8825070261955261\n",
      "Epoch [2/10], Batch [62], Loss: 0.9046610593795776\n",
      "Epoch [2/10], Batch [63], Loss: 0.8701309561729431\n",
      "Epoch [2/10], Batch [64], Loss: 0.8580552935600281\n",
      "Epoch [2/10], Batch [65], Loss: 0.8414217829704285\n",
      "Accuracy of the network on the 650000 train inputs: 61.03230769230769 %\n",
      "Epoch [3/10], Batch [1], Loss: 0.9305534362792969\n",
      "Epoch [3/10], Batch [2], Loss: 0.9403863549232483\n",
      "Epoch [3/10], Batch [3], Loss: 0.8903941512107849\n",
      "Epoch [3/10], Batch [4], Loss: 0.9051519632339478\n",
      "Epoch [3/10], Batch [5], Loss: 0.90004563331604\n",
      "Epoch [3/10], Batch [6], Loss: 0.9202356338500977\n",
      "Epoch [3/10], Batch [7], Loss: 0.894931435585022\n",
      "Epoch [3/10], Batch [8], Loss: 0.9258696436882019\n",
      "Epoch [3/10], Batch [9], Loss: 0.9465367197990417\n",
      "Epoch [3/10], Batch [10], Loss: 0.9162626266479492\n",
      "Epoch [3/10], Batch [11], Loss: 0.9156768321990967\n",
      "Epoch [3/10], Batch [12], Loss: 0.9274436235427856\n",
      "Epoch [3/10], Batch [13], Loss: 0.9687142372131348\n",
      "Epoch [3/10], Batch [14], Loss: 0.9391809701919556\n",
      "Epoch [3/10], Batch [15], Loss: 0.9051265716552734\n",
      "Epoch [3/10], Batch [16], Loss: 0.992523729801178\n",
      "Epoch [3/10], Batch [17], Loss: 0.9627028107643127\n",
      "Epoch [3/10], Batch [18], Loss: 0.9680581092834473\n",
      "Epoch [3/10], Batch [19], Loss: 0.9733898639678955\n",
      "Epoch [3/10], Batch [20], Loss: 0.9670902490615845\n",
      "Epoch [3/10], Batch [21], Loss: 0.9581988453865051\n",
      "Epoch [3/10], Batch [22], Loss: 0.9598007798194885\n",
      "Epoch [3/10], Batch [23], Loss: 0.9487791061401367\n",
      "Epoch [3/10], Batch [24], Loss: 0.9600088000297546\n",
      "Epoch [3/10], Batch [25], Loss: 0.9026079177856445\n",
      "Epoch [3/10], Batch [26], Loss: 0.8903840780258179\n",
      "Epoch [3/10], Batch [27], Loss: 0.8937703967094421\n",
      "Epoch [3/10], Batch [28], Loss: 0.954599142074585\n",
      "Epoch [3/10], Batch [29], Loss: 0.9629374146461487\n",
      "Epoch [3/10], Batch [30], Loss: 0.9470618963241577\n",
      "Epoch [3/10], Batch [31], Loss: 0.9439041018486023\n",
      "Epoch [3/10], Batch [32], Loss: 0.9393240213394165\n",
      "Epoch [3/10], Batch [33], Loss: 0.9470512866973877\n",
      "Epoch [3/10], Batch [34], Loss: 0.9682492017745972\n",
      "Epoch [3/10], Batch [35], Loss: 0.9121067523956299\n",
      "Epoch [3/10], Batch [36], Loss: 0.9360952377319336\n",
      "Epoch [3/10], Batch [37], Loss: 0.9316360354423523\n",
      "Epoch [3/10], Batch [38], Loss: 0.9405862092971802\n",
      "Epoch [3/10], Batch [39], Loss: 0.9525111317634583\n",
      "Epoch [3/10], Batch [40], Loss: 0.9245918989181519\n",
      "Epoch [3/10], Batch [41], Loss: 0.90304034948349\n",
      "Epoch [3/10], Batch [42], Loss: 0.9248862266540527\n",
      "Epoch [3/10], Batch [43], Loss: 0.9102010726928711\n",
      "Epoch [3/10], Batch [44], Loss: 0.8478842973709106\n",
      "Epoch [3/10], Batch [45], Loss: 0.8821704387664795\n",
      "Epoch [3/10], Batch [46], Loss: 0.9338176846504211\n",
      "Epoch [3/10], Batch [47], Loss: 0.9294993281364441\n",
      "Epoch [3/10], Batch [48], Loss: 0.8950018286705017\n",
      "Epoch [3/10], Batch [49], Loss: 0.8913843631744385\n",
      "Epoch [3/10], Batch [50], Loss: 0.9511617422103882\n",
      "Epoch [3/10], Batch [51], Loss: 0.9050016403198242\n",
      "Epoch [3/10], Batch [52], Loss: 0.8617265820503235\n",
      "Epoch [3/10], Batch [53], Loss: 0.8810755014419556\n",
      "Epoch [3/10], Batch [54], Loss: 0.8867974877357483\n",
      "Epoch [3/10], Batch [55], Loss: 0.8815478682518005\n",
      "Epoch [3/10], Batch [56], Loss: 0.8716129064559937\n",
      "Epoch [3/10], Batch [57], Loss: 0.8590577244758606\n",
      "Epoch [3/10], Batch [58], Loss: 0.8931185603141785\n",
      "Epoch [3/10], Batch [59], Loss: 0.8751478791236877\n",
      "Epoch [3/10], Batch [60], Loss: 0.8512060642242432\n",
      "Epoch [3/10], Batch [61], Loss: 0.8421476483345032\n",
      "Epoch [3/10], Batch [62], Loss: 0.8557626008987427\n",
      "Epoch [3/10], Batch [63], Loss: 0.8409310579299927\n",
      "Epoch [3/10], Batch [64], Loss: 0.8263871073722839\n",
      "Epoch [3/10], Batch [65], Loss: 0.8074445128440857\n",
      "Accuracy of the network on the 650000 train inputs: 62.38753846153846 %\n",
      "Epoch [4/10], Batch [1], Loss: 0.9123536348342896\n",
      "Epoch [4/10], Batch [2], Loss: 0.9226300716400146\n",
      "Epoch [4/10], Batch [3], Loss: 0.8726248741149902\n",
      "Epoch [4/10], Batch [4], Loss: 0.8874726295471191\n",
      "Epoch [4/10], Batch [5], Loss: 0.8803879022598267\n",
      "Epoch [4/10], Batch [6], Loss: 0.9007918834686279\n",
      "Epoch [4/10], Batch [7], Loss: 0.8765878677368164\n",
      "Epoch [4/10], Batch [8], Loss: 0.9053598642349243\n",
      "Epoch [4/10], Batch [9], Loss: 0.9261718988418579\n",
      "Epoch [4/10], Batch [10], Loss: 0.8943588137626648\n",
      "Epoch [4/10], Batch [11], Loss: 0.894047737121582\n",
      "Epoch [4/10], Batch [12], Loss: 0.905253529548645\n",
      "Epoch [4/10], Batch [13], Loss: 0.9476611614227295\n",
      "Epoch [4/10], Batch [14], Loss: 0.918572187423706\n",
      "Epoch [4/10], Batch [15], Loss: 0.8879155516624451\n",
      "Epoch [4/10], Batch [16], Loss: 0.9701969623565674\n",
      "Epoch [4/10], Batch [17], Loss: 0.9419381022453308\n",
      "Epoch [4/10], Batch [18], Loss: 0.9479718804359436\n",
      "Epoch [4/10], Batch [19], Loss: 0.9528296589851379\n",
      "Epoch [4/10], Batch [20], Loss: 0.9471016526222229\n",
      "Epoch [4/10], Batch [21], Loss: 0.9383725523948669\n",
      "Epoch [4/10], Batch [22], Loss: 0.9391098618507385\n",
      "Epoch [4/10], Batch [23], Loss: 0.9280826449394226\n",
      "Epoch [4/10], Batch [24], Loss: 0.9405078887939453\n",
      "Epoch [4/10], Batch [25], Loss: 0.8835172653198242\n",
      "Epoch [4/10], Batch [26], Loss: 0.8721280097961426\n",
      "Epoch [4/10], Batch [27], Loss: 0.8756960034370422\n",
      "Epoch [4/10], Batch [28], Loss: 0.9363127946853638\n",
      "Epoch [4/10], Batch [29], Loss: 0.9439494013786316\n",
      "Epoch [4/10], Batch [30], Loss: 0.9269313216209412\n",
      "Epoch [4/10], Batch [31], Loss: 0.9244734644889832\n",
      "Epoch [4/10], Batch [32], Loss: 0.920845091342926\n",
      "Epoch [4/10], Batch [33], Loss: 0.9284245371818542\n",
      "Epoch [4/10], Batch [34], Loss: 0.9738214612007141\n",
      "Epoch [4/10], Batch [35], Loss: 0.892035961151123\n",
      "Epoch [4/10], Batch [36], Loss: 0.9094402194023132\n",
      "Epoch [4/10], Batch [37], Loss: 0.9118818640708923\n",
      "Epoch [4/10], Batch [38], Loss: 0.9206085205078125\n",
      "Epoch [4/10], Batch [39], Loss: 0.9347459673881531\n",
      "Epoch [4/10], Batch [40], Loss: 0.9063869118690491\n",
      "Epoch [4/10], Batch [41], Loss: 0.8837822079658508\n",
      "Epoch [4/10], Batch [42], Loss: 0.9093426465988159\n",
      "Epoch [4/10], Batch [43], Loss: 0.8938223719596863\n",
      "Epoch [4/10], Batch [44], Loss: 0.8288945555686951\n",
      "Epoch [4/10], Batch [45], Loss: 0.8638371825218201\n",
      "Epoch [4/10], Batch [46], Loss: 0.9114226698875427\n",
      "Epoch [4/10], Batch [47], Loss: 0.896446168422699\n",
      "Epoch [4/10], Batch [48], Loss: 0.8724417090415955\n",
      "Epoch [4/10], Batch [49], Loss: 0.8755800724029541\n",
      "Epoch [4/10], Batch [50], Loss: 0.9357147216796875\n",
      "Epoch [4/10], Batch [51], Loss: 0.8905363082885742\n",
      "Epoch [4/10], Batch [52], Loss: 0.844325602054596\n",
      "Epoch [4/10], Batch [53], Loss: 0.8659014701843262\n",
      "Epoch [4/10], Batch [54], Loss: 0.8689850568771362\n",
      "Epoch [4/10], Batch [55], Loss: 0.8659152388572693\n",
      "Epoch [4/10], Batch [56], Loss: 0.8570414781570435\n",
      "Epoch [4/10], Batch [57], Loss: 0.8474953174591064\n",
      "Epoch [4/10], Batch [58], Loss: 0.8788091540336609\n",
      "Epoch [4/10], Batch [59], Loss: 0.8641636967658997\n",
      "Epoch [4/10], Batch [60], Loss: 0.8389543890953064\n",
      "Epoch [4/10], Batch [61], Loss: 0.8301518559455872\n",
      "Epoch [4/10], Batch [62], Loss: 0.8383945226669312\n",
      "Epoch [4/10], Batch [63], Loss: 0.8232830762863159\n",
      "Epoch [4/10], Batch [64], Loss: 0.8098044991493225\n",
      "Epoch [4/10], Batch [65], Loss: 0.7921364903450012\n",
      "Accuracy of the network on the 650000 train inputs: 63.13415384615384 %\n",
      "Epoch [5/10], Batch [1], Loss: 0.8980956077575684\n",
      "Epoch [5/10], Batch [2], Loss: 0.9077524542808533\n",
      "Epoch [5/10], Batch [3], Loss: 0.8590151071548462\n",
      "Epoch [5/10], Batch [4], Loss: 0.8760039806365967\n",
      "Epoch [5/10], Batch [5], Loss: 0.8678883910179138\n",
      "Epoch [5/10], Batch [6], Loss: 0.8873817324638367\n",
      "Epoch [5/10], Batch [7], Loss: 0.863803505897522\n",
      "Epoch [5/10], Batch [8], Loss: 0.8907901644706726\n",
      "Epoch [5/10], Batch [9], Loss: 0.9116488099098206\n",
      "Epoch [5/10], Batch [10], Loss: 0.8789218664169312\n",
      "Epoch [5/10], Batch [11], Loss: 0.8788135051727295\n",
      "Epoch [5/10], Batch [12], Loss: 0.8907538056373596\n",
      "Epoch [5/10], Batch [13], Loss: 0.9325659871101379\n",
      "Epoch [5/10], Batch [14], Loss: 0.9041129946708679\n",
      "Epoch [5/10], Batch [15], Loss: 0.875664472579956\n",
      "Epoch [5/10], Batch [16], Loss: 0.9553467631340027\n",
      "Epoch [5/10], Batch [17], Loss: 0.9290964603424072\n",
      "Epoch [5/10], Batch [18], Loss: 0.9321051836013794\n",
      "Epoch [5/10], Batch [19], Loss: 0.9388493895530701\n",
      "Epoch [5/10], Batch [20], Loss: 0.9337174892425537\n",
      "Epoch [5/10], Batch [21], Loss: 0.9240274429321289\n",
      "Epoch [5/10], Batch [22], Loss: 0.9252532124519348\n",
      "Epoch [5/10], Batch [23], Loss: 0.9143597483634949\n",
      "Epoch [5/10], Batch [24], Loss: 0.9271873235702515\n",
      "Epoch [5/10], Batch [25], Loss: 0.8701797723770142\n",
      "Epoch [5/10], Batch [26], Loss: 0.8594424724578857\n",
      "Epoch [5/10], Batch [27], Loss: 0.8633675575256348\n",
      "Epoch [5/10], Batch [28], Loss: 0.9236956238746643\n",
      "Epoch [5/10], Batch [29], Loss: 0.9307344555854797\n",
      "Epoch [5/10], Batch [30], Loss: 0.9128503203392029\n",
      "Epoch [5/10], Batch [31], Loss: 0.9103067517280579\n",
      "Epoch [5/10], Batch [32], Loss: 0.9085814356803894\n",
      "Epoch [5/10], Batch [33], Loss: 0.9138619303703308\n",
      "Epoch [5/10], Batch [34], Loss: 0.9239635467529297\n",
      "Epoch [5/10], Batch [35], Loss: 0.8791195154190063\n",
      "Epoch [5/10], Batch [36], Loss: 0.8905240297317505\n",
      "Epoch [5/10], Batch [37], Loss: 0.8979426026344299\n",
      "Epoch [5/10], Batch [38], Loss: 0.907095193862915\n",
      "Epoch [5/10], Batch [39], Loss: 0.9214454889297485\n",
      "Epoch [5/10], Batch [40], Loss: 0.8887481689453125\n",
      "Epoch [5/10], Batch [41], Loss: 0.8687825202941895\n",
      "Epoch [5/10], Batch [42], Loss: 0.8957082033157349\n",
      "Epoch [5/10], Batch [43], Loss: 0.875011146068573\n",
      "Epoch [5/10], Batch [44], Loss: 0.8162343502044678\n",
      "Epoch [5/10], Batch [45], Loss: 0.8478243350982666\n",
      "Epoch [5/10], Batch [46], Loss: 0.8934140801429749\n",
      "Epoch [5/10], Batch [47], Loss: 0.8965306878089905\n",
      "Epoch [5/10], Batch [48], Loss: 0.8579211831092834\n",
      "Epoch [5/10], Batch [49], Loss: 0.8611547946929932\n",
      "Epoch [5/10], Batch [50], Loss: 0.9176040291786194\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "\n",
    "        # inputs = inputs.to(device)\n",
    "        # print(len(inputs))\n",
    "        # print(len(vocab))\n",
    "        \n",
    "        # make bow vector for inputs\n",
    "        bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32).to(device)\n",
    "        # print(bow.shape)\n",
    "        for j in range(batch_size):\n",
    "            # token_indices = [vocab_dict[token] for token in train_tokens[i + j]]\n",
    "            # bow[j, token_indices] = torch.tensor([train_tokens[i + j].count(token) for token in vocab_dict.keys() if token in train_tokens[i + j]]).float().to(device)\n",
    "\n",
    "            for token in inputs[j]:        \n",
    "                bow[j][vocab_dict[token]] += 1\n",
    "    \n",
    "        \n",
    "        # convert bow to tensor\n",
    "        # inputs = torch.tensor(bow, dtype=torch.float32)\n",
    "        inputs = bow.to(device)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "        # Convert labels to LongTensors\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print the loss for this batch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}')\n",
    "        \n",
    "    print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'lin_layer_01.pth')\n",
    "\n",
    "# save model as pkl file\n",
    "torch.save(model, 'lin_layer_01.pkl')\n",
    "\n",
    "# save model as pkl file\n",
    "# torch.save(model,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
