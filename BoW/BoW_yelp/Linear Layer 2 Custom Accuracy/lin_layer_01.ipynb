{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "test_data = test_dataset['text']\n",
    "test_labels = test_dataset['label']\n",
    "\n",
    "# Convert labels to lists (optional)\n",
    "# train_labels = train_labels.tolist()\n",
    "# test_labels = test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re  # Import the regular expressions module\n",
    "\n",
    "# # use tokenizer to remove punctuation\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     # Replace \"!\" with \"exm\" using regular expressions\n",
    "#     text = re.sub(r'!', ' exm', text)\n",
    "    \n",
    "#     expanded_words = []\n",
    "#     for word in text.split():\n",
    "#         # using contractions.fix to expand the shortened words\n",
    "#         expanded_words.append(contractions.fix(word))   \n",
    "    \n",
    "#     expanded_text = ' '.join(expanded_words)\n",
    "#     # print(expanded_text)\n",
    "    \n",
    "#     text = expanded_text    \n",
    "    \n",
    "#     # replace a-b with a and b\n",
    "#     text = text.replace('-', ' ')\n",
    "    \n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     # Add an extra occurrence for all-uppercase words with more than one letter\n",
    "#     # temp = [word if (len(word) > 1 and word.isupper()) else None for word in tokens]\n",
    "#     temp = []\n",
    "#     for word in tokens:\n",
    "#         if len(word) > 1 and word.isupper():\n",
    "#             temp.append(word)\n",
    "#     tokens.extend(x for x in temp if x)\n",
    "    \n",
    "#     # convert to lower case\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "#     # dr. = dr and st. = st and so on\n",
    "#     tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "#     # remove punctuation\n",
    "#     # tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# # print(train_data[4])\n",
    "# # tokenize_text(train_data[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# print(len(train_data))\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     tokens.append(tokenize_text(train_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# # save to pkl file\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_train_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_train_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_test = []\n",
    "# print(len(test_data))\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     tokens_test.append(tokenize_text(test_data[i]))\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load the tokenized data\n",
    "# with open('yelp_test_tokens_no_stop.pkl', 'rb') as file:\n",
    "#     tokens = pickle.load(file)\n",
    "\n",
    "# # Define the frequency cutoff threshold\n",
    "# frequency_cutoff = 10\n",
    "\n",
    "# # Count word frequencies\n",
    "# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n",
    "\n",
    "# # Filter out words with counts less than the threshold\n",
    "# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n",
    "\n",
    "# # Save the filtered tokens to a new file\n",
    "# with open('yelp_test_tokens_filtered.pkl', 'wb') as file:\n",
    "#     pickle.dump(filtered_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('yelp_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('yelp_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45287\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_train_tokens = train_tokens\n",
    "\n",
    "# # Build the BoW representation manually\n",
    "# # Create a vocabulary by collecting unique words from the training data\n",
    "# vocab_reduced = set()\n",
    "# for tokens in reduced_train_tokens:\n",
    "#     vocab_reduced.update(tokens)\n",
    "\n",
    "# # Create a dictionary to map words to indices in the vocabulary\n",
    "# vocab_dict_reduced = {}\n",
    "# for i, word in enumerate(vocab_reduced):\n",
    "#     vocab_dict_reduced[word] = i\n",
    "\n",
    "# print(len(vocab_dict_reduced))\n",
    "\n",
    "# train_bow = np.zeros((len(reduced_train_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow train\")\n",
    "# # Convert text to BoW vectors\n",
    "# for i, tokens in enumerate(reduced_train_tokens):\n",
    "#     if i % 5000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         train_bow[i][vocab_dict_reduced[token]] += 1\n",
    "\n",
    "\n",
    "\n",
    "# reduced_test_tokens = test_tokens[:5000]\n",
    "# test_bow = np.zeros((len(reduced_test_tokens), len(vocab_reduced)))\n",
    "\n",
    "# print(\"bow test\")\n",
    "\n",
    "# for i, tokens in enumerate(reduced_test_tokens):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i)\n",
    "#     for token in tokens:\n",
    "#         if token in vocab_dict_reduced:\n",
    "#             test_bow[i][vocab_dict_reduced[token]] += 1\n",
    "#         else:    # if there is an unknown word, add it to the UNK column \n",
    "#             test_bow[i][vocab_dict_reduced['UNK']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 4\n",
    "learning_rate = 0.01\n",
    "batch_size = 10000\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear layer\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "input_size = len(vocab)  # Input size is the size of the vocabulary\n",
    "output_size = 5  # Output size is 5 dimensions\n",
    "\n",
    "model = BoWClassifier(input_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/d7g63tw141g81ynhx4f_hngr0000gn/T/ipykernel_38372/1891355772.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Batch [1], Loss: 1.4408131837844849, Accuracy: 80.7725 %\n",
      "Epoch [1/4], Batch [2], Loss: 1.4724866151809692, Accuracy: 80.28125 %\n",
      "Epoch [1/4], Batch [3], Loss: 1.4061048030853271, Accuracy: 79.5775 %\n",
      "Epoch [1/4], Batch [4], Loss: 1.3453079462051392, Accuracy: 79.920625 %\n",
      "Epoch [1/4], Batch [5], Loss: 1.3129788637161255, Accuracy: 80.5085 %\n",
      "Epoch [1/4], Batch [6], Loss: 1.2893041372299194, Accuracy: 80.955 %\n",
      "Epoch [1/4], Batch [7], Loss: 1.2474092245101929, Accuracy: 81.34678571428572 %\n",
      "Epoch [1/4], Batch [8], Loss: 1.2326922416687012, Accuracy: 81.693125 %\n",
      "Epoch [1/4], Batch [9], Loss: 1.2362111806869507, Accuracy: 81.96333333333334 %\n",
      "Epoch [1/4], Batch [10], Loss: 1.2036726474761963, Accuracy: 82.21475 %\n",
      "Epoch [1/4], Batch [11], Loss: 1.191344976425171, Accuracy: 82.43772727272727 %\n",
      "Epoch [1/4], Batch [12], Loss: 1.1764874458312988, Accuracy: 82.66375 %\n",
      "Epoch [1/4], Batch [13], Loss: 1.1839802265167236, Accuracy: 82.82692307692308 %\n",
      "Epoch [1/4], Batch [14], Loss: 1.1714584827423096, Accuracy: 82.94517857142857 %\n",
      "Epoch [1/4], Batch [15], Loss: 1.1201517581939697, Accuracy: 83.11233333333334 %\n",
      "Epoch [1/4], Batch [16], Loss: 1.207817554473877, Accuracy: 83.20296875 %\n",
      "Epoch [1/4], Batch [17], Loss: 1.1729804277420044, Accuracy: 83.31632352941176 %\n",
      "Epoch [1/4], Batch [18], Loss: 1.1673246622085571, Accuracy: 83.38722222222222 %\n",
      "Epoch [1/4], Batch [19], Loss: 1.1716173887252808, Accuracy: 83.44197368421052 %\n",
      "Epoch [1/4], Batch [20], Loss: 1.1576690673828125, Accuracy: 83.51275 %\n",
      "Epoch [1/4], Batch [21], Loss: 1.1438525915145874, Accuracy: 83.5975 %\n",
      "Epoch [1/4], Batch [22], Loss: 1.1487089395523071, Accuracy: 83.6778409090909 %\n",
      "Epoch [1/4], Batch [23], Loss: 1.1296018362045288, Accuracy: 83.75326086956522 %\n",
      "Epoch [1/4], Batch [24], Loss: 1.1307430267333984, Accuracy: 83.814375 %\n",
      "Epoch [1/4], Batch [25], Loss: 1.0532188415527344, Accuracy: 83.9102 %\n",
      "Epoch [1/4], Batch [26], Loss: 1.040560007095337, Accuracy: 83.9998076923077 %\n",
      "Epoch [1/4], Batch [27], Loss: 1.0487284660339355, Accuracy: 84.06925925925925 %\n",
      "Epoch [1/4], Batch [28], Loss: 1.106275200843811, Accuracy: 84.13589285714286 %\n",
      "Epoch [1/4], Batch [29], Loss: 1.1138237714767456, Accuracy: 84.19224137931035 %\n",
      "Epoch [1/4], Batch [30], Loss: 1.0890235900878906, Accuracy: 84.25066666666666 %\n",
      "Epoch [1/4], Batch [31], Loss: 1.078490972518921, Accuracy: 84.31290322580645 %\n",
      "Epoch [1/4], Batch [32], Loss: 1.0720925331115723, Accuracy: 84.37046875 %\n",
      "Epoch [1/4], Batch [33], Loss: 1.0886479616165161, Accuracy: 84.42265151515151 %\n",
      "Epoch [1/4], Batch [34], Loss: 1.1775423288345337, Accuracy: 84.42992647058824 %\n",
      "Epoch [1/4], Batch [35], Loss: 1.0559024810791016, Accuracy: 84.49085714285714 %\n",
      "Epoch [1/4], Batch [36], Loss: 1.0989527702331543, Accuracy: 84.526875 %\n",
      "Epoch [1/4], Batch [37], Loss: 1.0686594247817993, Accuracy: 84.57932432432432 %\n",
      "Epoch [1/4], Batch [38], Loss: 1.0702552795410156, Accuracy: 84.62460526315789 %\n",
      "Epoch [1/4], Batch [39], Loss: 1.0827380418777466, Accuracy: 84.66391025641026 %\n",
      "Epoch [1/4], Batch [40], Loss: 1.0524606704711914, Accuracy: 84.7106875 %\n",
      "Epoch [1/4], Batch [41], Loss: 1.0306593179702759, Accuracy: 84.7589024390244 %\n",
      "Epoch [1/4], Batch [42], Loss: 1.0388305187225342, Accuracy: 84.80339285714285 %\n",
      "Epoch [1/4], Batch [43], Loss: 1.0308489799499512, Accuracy: 84.84976744186046 %\n",
      "Epoch [1/4], Batch [44], Loss: 0.9670769572257996, Accuracy: 84.90431818181818 %\n",
      "Epoch [1/4], Batch [45], Loss: 1.0023417472839355, Accuracy: 84.95844444444444 %\n",
      "Epoch [1/4], Batch [46], Loss: 1.0723274946212769, Accuracy: 84.98929347826088 %\n",
      "Epoch [1/4], Batch [47], Loss: 1.1039708852767944, Accuracy: 85.00638297872341 %\n",
      "Epoch [1/4], Batch [48], Loss: 1.0222364664077759, Accuracy: 85.04442708333333 %\n",
      "Epoch [1/4], Batch [49], Loss: 1.008245587348938, Accuracy: 85.08913265306123 %\n",
      "Epoch [1/4], Batch [50], Loss: 1.0666364431381226, Accuracy: 85.10825 %\n",
      "Epoch [1/4], Batch [51], Loss: 1.017941951751709, Accuracy: 85.14450980392157 %\n",
      "Epoch [1/4], Batch [52], Loss: 0.980830192565918, Accuracy: 85.19125 %\n",
      "Epoch [1/4], Batch [53], Loss: 0.9946544766426086, Accuracy: 85.22660377358491 %\n",
      "Epoch [1/4], Batch [54], Loss: 1.0027806758880615, Accuracy: 85.2587962962963 %\n",
      "Epoch [1/4], Batch [55], Loss: 0.9984734654426575, Accuracy: 85.29268181818182 %\n",
      "Epoch [1/4], Batch [56], Loss: 0.9867467880249023, Accuracy: 85.33191964285714 %\n",
      "Epoch [1/4], Batch [57], Loss: 0.9622897505760193, Accuracy: 85.37179824561403 %\n",
      "Epoch [1/4], Batch [58], Loss: 1.0007331371307373, Accuracy: 85.40547413793104 %\n",
      "Epoch [1/4], Batch [59], Loss: 0.9787889719009399, Accuracy: 85.44322033898305 %\n",
      "Epoch [1/4], Batch [60], Loss: 0.9613560438156128, Accuracy: 85.48066666666666 %\n",
      "Epoch [1/4], Batch [61], Loss: 0.9484777450561523, Accuracy: 85.52319672131148 %\n",
      "Epoch [1/4], Batch [62], Loss: 0.9633173942565918, Accuracy: 85.56052419354839 %\n",
      "Epoch [1/4], Batch [63], Loss: 0.9384934306144714, Accuracy: 85.60194444444444 %\n",
      "Epoch [1/4], Batch [64], Loss: 0.9242736101150513, Accuracy: 85.644765625 %\n",
      "Epoch [1/4], Batch [65], Loss: 0.9091114401817322, Accuracy: 85.69096153846154 %\n",
      "Accuracy of the network on the 650000 train inputs: 85.69096153846154 %\n",
      "Epoch [2/4], Batch [1], Loss: 0.9153022170066833, Accuracy: 88.1875 %\n",
      "Epoch [2/4], Batch [2], Loss: 0.9807881116867065, Accuracy: 87.35625 %\n",
      "Epoch [2/4], Batch [3], Loss: 0.930149495601654, Accuracy: 87.47166666666666 %\n",
      "Epoch [2/4], Batch [4], Loss: 0.9436441659927368, Accuracy: 87.538125 %\n",
      "Epoch [2/4], Batch [5], Loss: 0.9357739090919495, Accuracy: 87.6795 %\n",
      "Epoch [2/4], Batch [6], Loss: 0.956963062286377, Accuracy: 87.67125 %\n",
      "Epoch [2/4], Batch [7], Loss: 0.931685745716095, Accuracy: 87.74535714285715 %\n",
      "Epoch [2/4], Batch [8], Loss: 0.9619811773300171, Accuracy: 87.795 %\n",
      "Epoch [2/4], Batch [9], Loss: 0.9847262501716614, Accuracy: 87.8013888888889 %\n",
      "Epoch [2/4], Batch [10], Loss: 0.9576120376586914, Accuracy: 87.77725 %\n",
      "Epoch [2/4], Batch [11], Loss: 0.9561163783073425, Accuracy: 87.76636363636364 %\n",
      "Epoch [2/4], Batch [12], Loss: 0.9654733538627625, Accuracy: 87.72208333333333 %\n",
      "Epoch [2/4], Batch [13], Loss: 1.0050163269042969, Accuracy: 87.655 %\n",
      "Epoch [2/4], Batch [14], Loss: 0.9763751029968262, Accuracy: 87.62 %\n",
      "Epoch [2/4], Batch [15], Loss: 0.939961314201355, Accuracy: 87.61083333333333 %\n",
      "Epoch [2/4], Batch [16], Loss: 1.032631516456604, Accuracy: 87.5690625 %\n",
      "Epoch [2/4], Batch [17], Loss: 1.0018794536590576, Accuracy: 87.56514705882353 %\n",
      "Epoch [2/4], Batch [18], Loss: 1.0078141689300537, Accuracy: 87.5425 %\n",
      "Epoch [2/4], Batch [19], Loss: 1.0106401443481445, Accuracy: 87.51671052631579 %\n",
      "Epoch [2/4], Batch [20], Loss: 1.0018284320831299, Accuracy: 87.5005 %\n",
      "Epoch [2/4], Batch [21], Loss: 0.9933570027351379, Accuracy: 87.50297619047619 %\n",
      "Epoch [2/4], Batch [22], Loss: 0.9950440526008606, Accuracy: 87.5 %\n",
      "Epoch [2/4], Batch [23], Loss: 0.9864603281021118, Accuracy: 87.49228260869565 %\n",
      "Epoch [2/4], Batch [24], Loss: 0.9942864179611206, Accuracy: 87.47583333333333 %\n",
      "Epoch [2/4], Batch [25], Loss: 0.9351508021354675, Accuracy: 87.4938 %\n",
      "Epoch [2/4], Batch [26], Loss: 0.9216374754905701, Accuracy: 87.51259615384615 %\n",
      "Epoch [2/4], Batch [27], Loss: 0.9258854389190674, Accuracy: 87.53518518518518 %\n",
      "Epoch [2/4], Batch [28], Loss: 0.9869170188903809, Accuracy: 87.53758928571429 %\n",
      "Epoch [2/4], Batch [29], Loss: 0.9973543882369995, Accuracy: 87.53129310344828 %\n",
      "Epoch [2/4], Batch [30], Loss: 0.982237696647644, Accuracy: 87.53433333333334 %\n",
      "Epoch [2/4], Batch [31], Loss: 0.9756924510002136, Accuracy: 87.53838709677419 %\n",
      "Epoch [2/4], Batch [32], Loss: 0.9712490439414978, Accuracy: 87.54 %\n",
      "Epoch [2/4], Batch [33], Loss: 0.9807613492012024, Accuracy: 87.53901515151514 %\n",
      "Epoch [2/4], Batch [34], Loss: 1.078944206237793, Accuracy: 87.49808823529412 %\n",
      "Epoch [2/4], Batch [35], Loss: 0.9511027336120605, Accuracy: 87.51092857142856 %\n",
      "Epoch [2/4], Batch [36], Loss: 0.9771673679351807, Accuracy: 87.51659722222222 %\n",
      "Epoch [2/4], Batch [37], Loss: 0.9660497903823853, Accuracy: 87.52844594594595 %\n",
      "Epoch [2/4], Batch [38], Loss: 0.9754273295402527, Accuracy: 87.53855263157895 %\n",
      "Epoch [2/4], Batch [39], Loss: 0.9869486093521118, Accuracy: 87.53871794871795 %\n",
      "Epoch [2/4], Batch [40], Loss: 0.9589776396751404, Accuracy: 87.54875 %\n",
      "Epoch [2/4], Batch [41], Loss: 0.9392083287239075, Accuracy: 87.55920731707317 %\n",
      "Epoch [2/4], Batch [42], Loss: 0.9552888870239258, Accuracy: 87.56422619047619 %\n",
      "Epoch [2/4], Batch [43], Loss: 0.9420710206031799, Accuracy: 87.57558139534883 %\n",
      "Epoch [2/4], Batch [44], Loss: 0.8807045817375183, Accuracy: 87.59443181818182 %\n",
      "Epoch [2/4], Batch [45], Loss: 0.9148109555244446, Accuracy: 87.61588888888889 %\n",
      "Epoch [2/4], Batch [46], Loss: 0.9738942384719849, Accuracy: 87.62070652173914 %\n",
      "Epoch [2/4], Batch [47], Loss: 0.9789413213729858, Accuracy: 87.62335106382979 %\n",
      "Epoch [2/4], Batch [48], Loss: 0.9335916042327881, Accuracy: 87.63645833333334 %\n",
      "Epoch [2/4], Batch [49], Loss: 0.933000385761261, Accuracy: 87.65045918367348 %\n",
      "Epoch [2/4], Batch [50], Loss: 0.98642498254776, Accuracy: 87.643 %\n",
      "Epoch [2/4], Batch [51], Loss: 0.9386746883392334, Accuracy: 87.65269607843138 %\n",
      "Epoch [2/4], Batch [52], Loss: 0.9029839634895325, Accuracy: 87.67399038461538 %\n",
      "Epoch [2/4], Batch [53], Loss: 0.9189737439155579, Accuracy: 87.68858490566038 %\n",
      "Epoch [2/4], Batch [54], Loss: 0.9246670603752136, Accuracy: 87.69944444444444 %\n",
      "Epoch [2/4], Batch [55], Loss: 0.9202547073364258, Accuracy: 87.71077272727273 %\n",
      "Epoch [2/4], Batch [56], Loss: 0.9163804650306702, Accuracy: 87.7275 %\n",
      "Epoch [2/4], Batch [57], Loss: 0.8920360207557678, Accuracy: 87.7440350877193 %\n",
      "Epoch [2/4], Batch [58], Loss: 0.9310039281845093, Accuracy: 87.75530172413794 %\n",
      "Epoch [2/4], Batch [59], Loss: 0.9132285118103027, Accuracy: 87.76927966101695 %\n",
      "Epoch [2/4], Batch [60], Loss: 0.8897316455841064, Accuracy: 87.78425 %\n",
      "Epoch [2/4], Batch [61], Loss: 0.8829968571662903, Accuracy: 87.80200819672132 %\n",
      "Epoch [2/4], Batch [62], Loss: 0.9039611220359802, Accuracy: 87.82185483870968 %\n",
      "Epoch [2/4], Batch [63], Loss: 0.8710517287254333, Accuracy: 87.8411507936508 %\n",
      "Epoch [2/4], Batch [64], Loss: 0.8591153025627136, Accuracy: 87.863828125 %\n",
      "Epoch [2/4], Batch [65], Loss: 0.8425288200378418, Accuracy: 87.88857692307693 %\n",
      "Accuracy of the network on the 650000 train inputs: 87.88857692307693 %\n",
      "Epoch [3/4], Batch [1], Loss: 0.9019749164581299, Accuracy: 88.3275 %\n",
      "Epoch [3/4], Batch [2], Loss: 0.9432830214500427, Accuracy: 87.7425 %\n",
      "Epoch [3/4], Batch [3], Loss: 0.8923511505126953, Accuracy: 87.84666666666666 %\n",
      "Epoch [3/4], Batch [4], Loss: 0.9057468771934509, Accuracy: 87.903125 %\n",
      "Epoch [3/4], Batch [5], Loss: 0.8992615342140198, Accuracy: 88.033 %\n",
      "Epoch [3/4], Batch [6], Loss: 0.920822262763977, Accuracy: 88.04208333333334 %\n",
      "Epoch [3/4], Batch [7], Loss: 0.8954218029975891, Accuracy: 88.12071428571429 %\n",
      "Epoch [3/4], Batch [8], Loss: 0.9259275197982788, Accuracy: 88.181875 %\n",
      "Epoch [3/4], Batch [9], Loss: 0.947291910648346, Accuracy: 88.18611111111112 %\n",
      "Epoch [3/4], Batch [10], Loss: 0.9162261486053467, Accuracy: 88.19125 %\n",
      "Epoch [3/4], Batch [11], Loss: 0.9152732491493225, Accuracy: 88.19909090909091 %\n",
      "Epoch [3/4], Batch [12], Loss: 0.9270216822624207, Accuracy: 88.171875 %\n",
      "Epoch [3/4], Batch [13], Loss: 0.9677466750144958, Accuracy: 88.10461538461539 %\n",
      "Epoch [3/4], Batch [14], Loss: 0.9388667941093445, Accuracy: 88.07392857142857 %\n",
      "Epoch [3/4], Batch [15], Loss: 0.9062702059745789, Accuracy: 88.06783333333334 %\n",
      "Epoch [3/4], Batch [16], Loss: 0.9929613471031189, Accuracy: 88.0234375 %\n",
      "Epoch [3/4], Batch [17], Loss: 0.9615943431854248, Accuracy: 88.02455882352942 %\n",
      "Epoch [3/4], Batch [18], Loss: 0.9672528505325317, Accuracy: 88.00791666666667 %\n",
      "Epoch [3/4], Batch [19], Loss: 0.9741177558898926, Accuracy: 87.98815789473684 %\n",
      "Epoch [3/4], Batch [20], Loss: 0.9663991332054138, Accuracy: 87.974875 %\n",
      "Epoch [3/4], Batch [21], Loss: 0.9576601386070251, Accuracy: 87.97988095238095 %\n",
      "Epoch [3/4], Batch [22], Loss: 0.9596198201179504, Accuracy: 87.98522727272727 %\n",
      "Epoch [3/4], Batch [23], Loss: 0.9489937424659729, Accuracy: 87.97619565217391 %\n",
      "Epoch [3/4], Batch [24], Loss: 0.9598789215087891, Accuracy: 87.961875 %\n",
      "Epoch [3/4], Batch [25], Loss: 0.9020928740501404, Accuracy: 87.9769 %\n",
      "Epoch [3/4], Batch [26], Loss: 0.8903774619102478, Accuracy: 87.99 %\n",
      "Epoch [3/4], Batch [27], Loss: 0.8933328986167908, Accuracy: 88.01185185185186 %\n",
      "Epoch [3/4], Batch [28], Loss: 0.9542020559310913, Accuracy: 88.01223214285714 %\n",
      "Epoch [3/4], Batch [29], Loss: 0.9625803828239441, Accuracy: 88.00612068965518 %\n",
      "Epoch [3/4], Batch [30], Loss: 0.9469180703163147, Accuracy: 88.00866666666667 %\n",
      "Epoch [3/4], Batch [31], Loss: 0.9438479542732239, Accuracy: 88.01596774193548 %\n",
      "Epoch [3/4], Batch [32], Loss: 0.9394878149032593, Accuracy: 88.02125 %\n",
      "Epoch [3/4], Batch [33], Loss: 0.9459060430526733, Accuracy: 88.01848484848485 %\n",
      "Epoch [3/4], Batch [34], Loss: 0.967623233795166, Accuracy: 88.00242647058823 %\n",
      "Epoch [3/4], Batch [35], Loss: 0.9123206734657288, Accuracy: 88.01528571428571 %\n",
      "Epoch [3/4], Batch [36], Loss: 0.9354110360145569, Accuracy: 88.02458333333334 %\n",
      "Epoch [3/4], Batch [37], Loss: 0.9308300614356995, Accuracy: 88.03128378378378 %\n",
      "Epoch [3/4], Batch [38], Loss: 0.9397822022438049, Accuracy: 88.04052631578948 %\n",
      "Epoch [3/4], Batch [39], Loss: 0.9520655274391174, Accuracy: 88.03634615384615 %\n",
      "Epoch [3/4], Batch [40], Loss: 0.9238149523735046, Accuracy: 88.04825 %\n",
      "Epoch [3/4], Batch [41], Loss: 0.9032429456710815, Accuracy: 88.05841463414635 %\n",
      "Epoch [3/4], Batch [42], Loss: 0.9244020581245422, Accuracy: 88.06363095238095 %\n",
      "Epoch [3/4], Batch [43], Loss: 0.9102832078933716, Accuracy: 88.07313953488372 %\n",
      "Epoch [3/4], Batch [44], Loss: 0.84744793176651, Accuracy: 88.09232954545455 %\n",
      "Epoch [3/4], Batch [45], Loss: 0.8823046088218689, Accuracy: 88.11133333333333 %\n",
      "Epoch [3/4], Batch [46], Loss: 0.9331049919128418, Accuracy: 88.11614130434782 %\n",
      "Epoch [3/4], Batch [47], Loss: 0.9281074404716492, Accuracy: 88.12143617021276 %\n",
      "Epoch [3/4], Batch [48], Loss: 0.8947322368621826, Accuracy: 88.13364583333333 %\n",
      "Epoch [3/4], Batch [49], Loss: 0.8917325139045715, Accuracy: 88.14382653061224 %\n",
      "Epoch [3/4], Batch [50], Loss: 0.9506469964981079, Accuracy: 88.1363 %\n",
      "Epoch [3/4], Batch [51], Loss: 0.905750572681427, Accuracy: 88.1446568627451 %\n",
      "Epoch [3/4], Batch [52], Loss: 0.8622918128967285, Accuracy: 88.16605769230769 %\n",
      "Epoch [3/4], Batch [53], Loss: 0.881389856338501, Accuracy: 88.18018867924528 %\n",
      "Epoch [3/4], Batch [54], Loss: 0.8864201903343201, Accuracy: 88.18907407407407 %\n",
      "Epoch [3/4], Batch [55], Loss: 0.8816375732421875, Accuracy: 88.20209090909091 %\n",
      "Epoch [3/4], Batch [56], Loss: 0.8728142380714417, Accuracy: 88.22004464285715 %\n",
      "Epoch [3/4], Batch [57], Loss: 0.8594796657562256, Accuracy: 88.23583333333333 %\n",
      "Epoch [3/4], Batch [58], Loss: 0.8928231596946716, Accuracy: 88.24741379310345 %\n",
      "Epoch [3/4], Batch [59], Loss: 0.8757457733154297, Accuracy: 88.26080508474577 %\n",
      "Epoch [3/4], Batch [60], Loss: 0.8527986407279968, Accuracy: 88.27608333333333 %\n",
      "Epoch [3/4], Batch [61], Loss: 0.8433293104171753, Accuracy: 88.29344262295082 %\n",
      "Epoch [3/4], Batch [62], Loss: 0.8535957932472229, Accuracy: 88.31399193548387 %\n",
      "Epoch [3/4], Batch [63], Loss: 0.841239869594574, Accuracy: 88.33142857142857 %\n",
      "Epoch [3/4], Batch [64], Loss: 0.8274046778678894, Accuracy: 88.3529296875 %\n",
      "Epoch [3/4], Batch [65], Loss: 0.8083361983299255, Accuracy: 88.37592307692307 %\n",
      "Accuracy of the network on the 650000 train inputs: 88.37592307692307 %\n",
      "Epoch [4/4], Batch [1], Loss: 0.892588198184967, Accuracy: 88.5825 %\n",
      "Epoch [4/4], Batch [2], Loss: 0.9238819479942322, Accuracy: 88.03875 %\n",
      "Epoch [4/4], Batch [3], Loss: 0.8736878037452698, Accuracy: 88.14416666666666 %\n",
      "Epoch [4/4], Batch [4], Loss: 0.8883622288703918, Accuracy: 88.186875 %\n",
      "Epoch [4/4], Batch [5], Loss: 0.8797869086265564, Accuracy: 88.288 %\n",
      "Epoch [4/4], Batch [6], Loss: 0.9005076289176941, Accuracy: 88.2675 %\n",
      "Epoch [4/4], Batch [7], Loss: 0.8765943646430969, Accuracy: 88.36642857142857 %\n",
      "Epoch [4/4], Batch [8], Loss: 0.9052364230155945, Accuracy: 88.4171875 %\n",
      "Epoch [4/4], Batch [9], Loss: 0.926267683506012, Accuracy: 88.42222222222222 %\n",
      "Epoch [4/4], Batch [10], Loss: 0.8940033316612244, Accuracy: 88.43425 %\n",
      "Epoch [4/4], Batch [11], Loss: 0.893459677696228, Accuracy: 88.44363636363636 %\n",
      "Epoch [4/4], Batch [12], Loss: 0.9051862359046936, Accuracy: 88.42 %\n",
      "Epoch [4/4], Batch [13], Loss: 0.946927547454834, Accuracy: 88.36076923076924 %\n",
      "Epoch [4/4], Batch [14], Loss: 0.9176778197288513, Accuracy: 88.34 %\n",
      "Epoch [4/4], Batch [15], Loss: 0.8882519602775574, Accuracy: 88.33516666666667 %\n",
      "Epoch [4/4], Batch [16], Loss: 0.9705138802528381, Accuracy: 88.290625 %\n",
      "Epoch [4/4], Batch [17], Loss: 0.9416117072105408, Accuracy: 88.29 %\n",
      "Epoch [4/4], Batch [18], Loss: 0.9469485282897949, Accuracy: 88.2761111111111 %\n",
      "Epoch [4/4], Batch [19], Loss: 0.953362226486206, Accuracy: 88.25684210526316 %\n",
      "Epoch [4/4], Batch [20], Loss: 0.9467031359672546, Accuracy: 88.246 %\n",
      "Epoch [4/4], Batch [21], Loss: 0.937701940536499, Accuracy: 88.24964285714286 %\n",
      "Epoch [4/4], Batch [22], Loss: 0.9391494393348694, Accuracy: 88.25227272727273 %\n",
      "Epoch [4/4], Batch [23], Loss: 0.9284552931785583, Accuracy: 88.24315217391305 %\n",
      "Epoch [4/4], Batch [24], Loss: 0.9399488568305969, Accuracy: 88.2321875 %\n",
      "Epoch [4/4], Batch [25], Loss: 0.8829333186149597, Accuracy: 88.244 %\n",
      "Epoch [4/4], Batch [26], Loss: 0.8716453909873962, Accuracy: 88.26125 %\n",
      "Epoch [4/4], Batch [27], Loss: 0.8753106594085693, Accuracy: 88.2775 %\n",
      "Epoch [4/4], Batch [28], Loss: 0.9356414079666138, Accuracy: 88.27571428571429 %\n",
      "Epoch [4/4], Batch [29], Loss: 0.9436571002006531, Accuracy: 88.26896551724138 %\n",
      "Epoch [4/4], Batch [30], Loss: 0.9266253709793091, Accuracy: 88.27133333333333 %\n",
      "Epoch [4/4], Batch [31], Loss: 0.9238107204437256, Accuracy: 88.27709677419355 %\n",
      "Epoch [4/4], Batch [32], Loss: 0.9209007024765015, Accuracy: 88.2821875 %\n",
      "Epoch [4/4], Batch [33], Loss: 0.9271094799041748, Accuracy: 88.28083333333333 %\n",
      "Epoch [4/4], Batch [34], Loss: 0.9676626920700073, Accuracy: 88.25485294117647 %\n",
      "Epoch [4/4], Batch [35], Loss: 0.8918715715408325, Accuracy: 88.269 %\n",
      "Epoch [4/4], Batch [36], Loss: 0.9091871380805969, Accuracy: 88.27958333333333 %\n",
      "Epoch [4/4], Batch [37], Loss: 0.9108733534812927, Accuracy: 88.28648648648648 %\n",
      "Epoch [4/4], Batch [38], Loss: 0.9194309711456299, Accuracy: 88.29822368421053 %\n",
      "Epoch [4/4], Batch [39], Loss: 0.9339824318885803, Accuracy: 88.2951923076923 %\n",
      "Epoch [4/4], Batch [40], Loss: 0.9060097932815552, Accuracy: 88.3043125 %\n",
      "Epoch [4/4], Batch [41], Loss: 0.8841756582260132, Accuracy: 88.31384146341463 %\n",
      "Epoch [4/4], Batch [42], Loss: 0.9081109166145325, Accuracy: 88.32035714285715 %\n",
      "Epoch [4/4], Batch [43], Loss: 0.8936634063720703, Accuracy: 88.33203488372094 %\n",
      "Epoch [4/4], Batch [44], Loss: 0.8284872174263, Accuracy: 88.35380681818182 %\n",
      "Epoch [4/4], Batch [45], Loss: 0.8636991381645203, Accuracy: 88.37272222222222 %\n",
      "Epoch [4/4], Batch [46], Loss: 0.9111208915710449, Accuracy: 88.37978260869565 %\n",
      "Epoch [4/4], Batch [47], Loss: 0.8951646685600281, Accuracy: 88.38920212765957 %\n",
      "Epoch [4/4], Batch [48], Loss: 0.8722442388534546, Accuracy: 88.40203125 %\n",
      "Epoch [4/4], Batch [49], Loss: 0.8752978444099426, Accuracy: 88.40974489795919 %\n",
      "Epoch [4/4], Batch [50], Loss: 0.9347906112670898, Accuracy: 88.4009 %\n",
      "Epoch [4/4], Batch [51], Loss: 0.8902566432952881, Accuracy: 88.41009803921568 %\n",
      "Epoch [4/4], Batch [52], Loss: 0.8440484404563904, Accuracy: 88.43149038461539 %\n",
      "Epoch [4/4], Batch [53], Loss: 0.8657122850418091, Accuracy: 88.44745283018868 %\n",
      "Epoch [4/4], Batch [54], Loss: 0.8682876825332642, Accuracy: 88.45791666666666 %\n",
      "Epoch [4/4], Batch [55], Loss: 0.8648192286491394, Accuracy: 88.47045454545454 %\n",
      "Epoch [4/4], Batch [56], Loss: 0.8568494915962219, Accuracy: 88.48794642857143 %\n",
      "Epoch [4/4], Batch [57], Loss: 0.846811830997467, Accuracy: 88.50236842105264 %\n",
      "Epoch [4/4], Batch [58], Loss: 0.8780356645584106, Accuracy: 88.51172413793104 %\n",
      "Epoch [4/4], Batch [59], Loss: 0.8628603219985962, Accuracy: 88.52453389830508 %\n",
      "Epoch [4/4], Batch [60], Loss: 0.8387771248817444, Accuracy: 88.539125 %\n",
      "Epoch [4/4], Batch [61], Loss: 0.8294541239738464, Accuracy: 88.55426229508197 %\n",
      "Epoch [4/4], Batch [62], Loss: 0.8339716792106628, Accuracy: 88.57447580645162 %\n",
      "Epoch [4/4], Batch [63], Loss: 0.8231909275054932, Accuracy: 88.59142857142857 %\n",
      "Epoch [4/4], Batch [64], Loss: 0.8095566630363464, Accuracy: 88.6126171875 %\n",
      "Epoch [4/4], Batch [65], Loss: 0.7929494380950928, Accuracy: 88.63623076923076 %\n",
      "Accuracy of the network on the 650000 train inputs: 88.63623076923076 %\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "#         inputs = train_tokens[i:i + batch_size]\n",
    "#         # print(len(inputs))\n",
    "#         # print(len(vocab))\n",
    "        \n",
    "#         # make bow vector for inputs\n",
    "#         bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "#         # print(bow.shape)\n",
    "#         for j in range(batch_size):\n",
    "#             for token in inputs[j]:\n",
    "\n",
    "#                 bow[j][vocab_dict[token]] += 1\n",
    "\n",
    "        \n",
    "#         # convert bow to tensor\n",
    "#         inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        \n",
    "#         labels = train_labels[i:i + batch_size]  # Make sure to have train_labels defined\n",
    "#         # Convert labels to LongTensors\n",
    "#         labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # calculate accuracy\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         # correct += (predicted == labels).sum().item()\n",
    "#         # if predicted == labels:\n",
    "#         #     correct += 1\n",
    "#         # if |predicted - labels| == 1:\n",
    "#         #     correct += 0.75\n",
    "#         # if |predicted - labels| == 2:\n",
    "#         #     correct += 0.5\n",
    "#         # if |predicted - labels| == 3:\n",
    "#         #     correct += 0.25\n",
    "#         # if |predicted - labels| == 4:\n",
    "#         #     correct += 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # Print the loss for this batch if needed\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}, Accuracy: {100 * correct / total} %')\n",
    "\n",
    "#     print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "# print('Training finished')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(0, len(train_tokens), batch_size):\n",
    "        \n",
    "        inputs = train_tokens[i:i + batch_size]\n",
    "        \n",
    "        # make bow vector for inputs\n",
    "        bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "        for j in range(batch_size):\n",
    "            for token in inputs[j]:\n",
    "                bow[j][vocab_dict[token]] += 1\n",
    "\n",
    "        # convert bow to tensor\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        \n",
    "        labels = train_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy based on differences between predicted and actual labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        for j in range(batch_size):\n",
    "            diff = abs(predicted[j] - labels[j])\n",
    "            if diff == 0:\n",
    "                correct += 1\n",
    "            elif diff == 1:\n",
    "                correct += 0.75\n",
    "            elif diff == 2:\n",
    "                correct += 0.5\n",
    "            elif diff == 3:\n",
    "                correct += 0.25\n",
    "\n",
    "        # Print the loss for this batch and accuracy\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}], Loss: {loss.item()}, Accuracy: {100 * correct / total} %')\n",
    "\n",
    "    print(f'Accuracy of the network on the {total} train inputs: {100 * correct / total} %')\n",
    "\n",
    "print('Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the above model in its entirety in pkl file\n",
    "torch.save(model, 'bow_final_model.pkl')\n",
    "\n",
    "# load the model\n",
    "model = torch.load('bow_final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/d7g63tw141g81ynhx4f_hngr0000gn/T/ipykernel_38372/393453365.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal Accuracy of the network on the 50000 test inputs: 86.425 %\n",
      "Confusion Matrix:\n",
      "[[7865 1492  300   87  256]\n",
      " [2939 4417 1958  347  339]\n",
      " [ 658 1932 4499 1958  953]\n",
      " [ 183  328 1722 3746 4021]\n",
      " [ 202  105  328 1326 8039]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72     10000\n",
      "           1       0.53      0.44      0.48     10000\n",
      "           2       0.51      0.45      0.48     10000\n",
      "           3       0.50      0.37      0.43     10000\n",
      "           4       0.59      0.80      0.68     10000\n",
      "\n",
      "    accuracy                           0.57     50000\n",
      "   macro avg       0.56      0.57      0.56     50000\n",
      "weighted avg       0.56      0.57      0.56     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(0, len(test_tokens), batch_size):\n",
    "        inputs = test_tokens[i:i + batch_size]\n",
    "        bow = torch.zeros((batch_size, len(vocab)), dtype=torch.float32)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            for token in inputs[j]:\n",
    "                if token in vocab_dict:\n",
    "                    bow[j][vocab_dict[token]] += 1\n",
    "                else:\n",
    "                    bow[j][vocab_dict['UNK']] += 1\n",
    "\n",
    "        inputs = torch.tensor(bow, dtype=torch.float32).to(device)\n",
    "        labels = test_labels[i:i + batch_size]\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # correct += (predicted == labels).sum().item()\n",
    "        for j in range(batch_size):\n",
    "            diff = abs(predicted[j] - labels[j])\n",
    "            if diff == 0:\n",
    "                correct += 1\n",
    "            elif diff == 1:\n",
    "                correct += 0.75\n",
    "            elif diff == 2:\n",
    "                correct += 0.5\n",
    "            elif diff == 3:\n",
    "                correct += 0.25\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy_odrinal = 100 * correct / total\n",
    "    print(f'Ordinal Accuracy of the network on the {total} test inputs: {accuracy_odrinal} %')\n",
    "\n",
    "    # Calculate confusion matrix, precision, recall, and F1 scores\n",
    "    confusion = confusion_matrix(true_labels, predictions)\n",
    "    classification_rep = classification_report(true_labels, predictions, target_names=[str(i) for i in range(5)])\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    # print(\"F1 Micro:\", f1_micro)\n",
    "    # print(\"F1 Macro:\", f1_macro)\n",
    "    # print(\"F1 Weighted:\", f1_weighted)\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
