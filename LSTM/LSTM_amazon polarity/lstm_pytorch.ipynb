{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('amazon.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 3600000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 400000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data_title = train_dataset['title']\n",
    "train_data_content = train_dataset['content']\n",
    "train_labels = train_dataset['label']\n",
    "test_data_title = test_dataset['title']\n",
    "test_data_content = test_dataset['content']\n",
    "test_labels = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_title[0])\n",
    "# print(train_data_content[0])\n",
    "# print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data_title + train_data_content\n",
    "# test_data = test_data_title + test_data_content\n",
    "\n",
    "train_data = [None] * len(train_data_title)\n",
    "for i in range(len(train_data_title)):\n",
    "    train_data[i] = train_data_title[i] + \" MID \" + train_data_content[i]\n",
    "\n",
    "test_data = [None] * len(test_data_title)\n",
    "for i in range(len(test_data_title)):\n",
    "    test_data[i] = test_data_title[i] + \" MID \" + test_data_content[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stuning even for the non-gamer MID This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/revanthgundam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/homebrew/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/homebrew/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/homebrew/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    expanded_words = []\n",
    "    for word in text.split():\n",
    "        # using contractions.fix to expand the shortened words\n",
    "        # print(word)\n",
    "        try:\n",
    "            expanded_words.append(contractions.fix(word))\n",
    "        except:\n",
    "            expanded_words.append(word)\n",
    "        # expanded_words.append(contractions.fix(word))\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    text = expanded_text\n",
    "    # if there is a word between () then write it once\n",
    "    text = re.sub(r'\\((.*?)\\)', r'\\1', text)\n",
    "    # Replace hyphens with spaces\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # replace colons with spaces\n",
    "    text = text.replace(':', ' ')\n",
    "    \n",
    "    # replace commas with spaces\n",
    "    text = text.replace(',', ' ')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove periods\n",
    "    tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "    # replace all numbers with <num>\n",
    "    tokens = [re.sub(r'\\d+', 'NUM', w) for w in tokens]\n",
    "    \n",
    "    # replace the word mid everywhere with MID\n",
    "    tokens = [w.replace('mid', 'MID') for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut entire train and test tokens and labels to 5 lakh and 50k\n",
    "train_data = train_data[:500000]\n",
    "train_labels = train_labels[:500000]\n",
    "test_data = test_data[:50000]\n",
    "test_labels = test_labels[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n",
      "420000\n",
      "421000\n",
      "422000\n",
      "423000\n",
      "424000\n",
      "425000\n",
      "426000\n",
      "427000\n",
      "428000\n",
      "429000\n",
      "430000\n",
      "431000\n",
      "432000\n",
      "433000\n",
      "434000\n",
      "435000\n",
      "436000\n",
      "437000\n",
      "438000\n",
      "439000\n",
      "440000\n",
      "441000\n",
      "442000\n",
      "443000\n",
      "444000\n",
      "445000\n",
      "446000\n",
      "447000\n",
      "448000\n",
      "449000\n",
      "450000\n",
      "451000\n",
      "452000\n",
      "453000\n",
      "454000\n",
      "455000\n",
      "456000\n",
      "457000\n",
      "458000\n",
      "459000\n",
      "460000\n",
      "461000\n",
      "462000\n",
      "463000\n",
      "464000\n",
      "465000\n",
      "466000\n",
      "467000\n",
      "468000\n",
      "469000\n",
      "470000\n",
      "471000\n",
      "472000\n",
      "473000\n",
      "474000\n",
      "475000\n",
      "476000\n",
      "477000\n",
      "478000\n",
      "479000\n",
      "480000\n",
      "481000\n",
      "482000\n",
      "483000\n",
      "484000\n",
      "485000\n",
      "486000\n",
      "487000\n",
      "488000\n",
      "489000\n",
      "490000\n",
      "491000\n",
      "492000\n",
      "493000\n",
      "494000\n",
      "495000\n",
      "496000\n",
      "497000\n",
      "498000\n",
      "499000\n"
     ]
    }
   ],
   "source": [
    "# print(train_data[29876])\n",
    "# print(tokenize_text(train_data[29876]))\n",
    "\n",
    "tokens = []\n",
    "print(len(train_data))\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    tokens.append(tokenize_text(train_data[i]))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "# save to pkl file\n",
    "with open('amazon_train_tokens.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    }
   ],
   "source": [
    "tokens_test = []\n",
    "print(len(test_data))\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    tokens_test.append(tokenize_text(test_data[i]))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "# save to pkl file\n",
    "with open('amazon_test_tokens.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('amazon_train_tokens.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('amazon_test_tokens.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = train_tokens\n",
    "tokens_test = test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  243\n",
      "Min length:  1\n",
      "Number of sentences with length > 200:  479\n",
      "Number of sentences with length < 20:  1855\n",
      "Length of new train data:  497666\n",
      "Length of new test data:  49773\n",
      "497666\n",
      "49773\n"
     ]
    }
   ],
   "source": [
    "# find length of longest sentence\n",
    "max_len = 0\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) > max_len:\n",
    "        max_len = len(tokens[i])\n",
    "        \n",
    "# find length of shortest sentence\n",
    "min_len = 1000000\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) < min_len:\n",
    "        min_len = len(tokens[i])\n",
    "        \n",
    "print(\"Max length: \", max_len)\n",
    "print(\"Min length: \", min_len)\n",
    "\n",
    "# number of sentences with length > 100\n",
    "count = 0\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) > 200:\n",
    "        count += 1\n",
    "        \n",
    "print(\"Number of sentences with length > 200: \", count)\n",
    "\n",
    "# number of sentences with length < 20\n",
    "count = 0\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) < 20:\n",
    "        count += 1\n",
    "\n",
    "print(\"Number of sentences with length < 20: \", count)\n",
    "\n",
    "\n",
    "# remove sentences with length < 10 along with their labels\n",
    "new_tokens = []\n",
    "new_labels = []\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) >= 20:\n",
    "        new_tokens.append(tokens[i])\n",
    "        new_labels.append(train_labels[i])\n",
    "\n",
    "# remove so in test also\n",
    "new_tokens_test = []\n",
    "new_labels_test = []\n",
    "for i in range(len(tokens_test)):\n",
    "    if len(tokens_test[i]) >= 20:\n",
    "        new_tokens_test.append(tokens_test[i])\n",
    "        new_labels_test.append(test_labels[i])\n",
    "\n",
    "# remove sentences with length > 200 along with their labels\n",
    "new_tokens_2 = []\n",
    "new_labels_2 = []\n",
    "for i in range(len(new_tokens)):\n",
    "    if len(new_tokens[i]) <= 200:\n",
    "        new_tokens_2.append(new_tokens[i])\n",
    "        new_labels_2.append(new_labels[i])\n",
    "\n",
    "# remove so in test also\n",
    "new_tokens_test_2 = []\n",
    "new_labels_test_2 = []\n",
    "for i in range(len(new_tokens_test)):\n",
    "    if len(new_tokens_test[i]) <= 200:\n",
    "        new_tokens_test_2.append(new_tokens_test[i])\n",
    "        new_labels_test_2.append(new_labels_test[i])\n",
    "\n",
    "print(\"Length of new train data: \", len(new_tokens_2))\n",
    "print(\"Length of new test data: \", len(new_tokens_test_2))\n",
    "print(len(new_labels_2))\n",
    "print(len(new_labels_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = new_tokens_2\n",
    "new_labels = new_labels_2\n",
    "new_tokens_test = new_tokens_test_2\n",
    "new_labels_test = new_labels_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  216\n",
      "Min length:  13\n"
     ]
    }
   ],
   "source": [
    "# for test data\n",
    "# find length of longest sentence\n",
    "max_len = 0\n",
    "for i in range(len(tokens_test)):\n",
    "    if len(tokens_test[i]) > max_len:\n",
    "        max_len = len(tokens_test[i])\n",
    "        \n",
    "# find length of shortest sentence\n",
    "min_len = 1000000\n",
    "for i in range(len(tokens_test)):\n",
    "    if len(tokens_test[i]) < min_len:\n",
    "        min_len = len(tokens_test[i])\n",
    "        \n",
    "print(\"Max length: \", max_len)\n",
    "print(\"Min length: \", min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding to all sentences and S and EOS tokens\n",
    "def padding(tokens, max_len):\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = ['S'] + tokens[i] + ['EOS']\n",
    "        while len(tokens[i]) < max_len:\n",
    "            tokens[i].append('PAD')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max length of sentence\n",
    "max_len = 0\n",
    "for i in range(len(new_tokens)):\n",
    "    if len(new_tokens[i]) > max_len:\n",
    "        max_len = len(new_tokens[i])\n",
    "\n",
    "max_len_test = 0\n",
    "for i in range(len(new_tokens_test)):\n",
    "    if len(new_tokens_test[i]) > max_len_test:\n",
    "        max_len_test = len(new_tokens_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = padding(new_tokens, (max_len + 2))\n",
    "tokens_test = padding(new_tokens_test, (max_len_test + 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pkl file\n",
    "with open('amazon_train_tokens_padded.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens, file)\n",
    "\n",
    "with open('amazon_test_tokens_padded.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from pkl file\n",
    "with open('amazon_train_tokens_padded.pkl', 'rb') as file:\n",
    "    tokens = pickle.load(file)\n",
    "\n",
    "with open('amazon_test_tokens_padded.pkl', 'rb') as file:\n",
    "    tokens_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'stuning', 'even', 'for', 'the', 'non', 'gamer', 'MID', 'this', 'sound', 'track', 'was', 'beautiful', 'it', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'i', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', 'game', 'music', 'i', 'have', 'played', 'the', 'game', 'chrono', 'cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'i', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', 'it', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', 'it', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['S', 'great', 'cd', 'MID', 'my', 'lovely', 'pat', 'has', 'one', 'of', 'the', 'great', 'voices', 'of', 'her', 'generation', 'i', 'have', 'listened', 'to', 'this', 'cd', 'for', 'years', 'and', 'i', 'still', 'love', 'it', 'when', 'i', 'am', 'in', 'a', 'good', 'mood', 'it', 'makes', 'me', 'feel', 'better', 'a', 'bad', 'mood', 'just', 'evaporates', 'like', 'sugar', 'in', 'the', 'rain', 'this', 'cd', 'just', 'oozes', 'life', 'vocals', 'are', 'jusat', 'stuunning', 'and', 'lyrics', 'just', 'kill', 'one', 'of', 'life', 'hidden', 'gems', 'this', 'is', 'a', 'desert', 'isle', 'cd', 'in', 'my', 'book', 'why', 'she', 'never', 'made', 'it', 'big', 'is', 'just', 'beyond', 'me', 'everytime', 'i', 'play', 'this', 'no', 'matter', 'black', 'white', 'young', 'old', 'male', 'female', 'everybody', 'says', 'one', 'thing', 'who', 'was', 'that', 'singing', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])\n",
    "print(tokens_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if len(tokens[i]) != 202:\n",
    "        print(len(tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111331\n",
      "moneygot\n",
      "348104\n",
      "129201\n",
      "247311\n",
      "109151\n",
      "193633\n",
      "114320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for token in tokens:\n",
    "    vocab.update(token)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(word_to_idx['the'])\n",
    "print(idx_to_word[0])\n",
    "print(word_to_idx['NUM'])\n",
    "print(word_to_idx['UNK'])\n",
    "print(word_to_idx['PAD'])\n",
    "print(word_to_idx['S'])\n",
    "print(word_to_idx['EOS'])\n",
    "print(word_to_idx['MID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        # handle unk\n",
    "        for i in range(len(self.tokens)):\n",
    "            for j in range(len(self.tokens[i])):\n",
    "                if self.tokens[i][j] not in word_to_idx:\n",
    "                    self.tokens[i][j] = 'UNK'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor([word_to_idx[self.tokens[idx][i]] for i in range(len(self.tokens[idx]))]), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AmazonDataset(tokens, new_labels)\n",
    "test_dataset = AmazonDataset(tokens_test, new_labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 300\n",
    "hidden_size = 128\n",
    "num_epochs = 3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LSTM(vocab_size, embedding_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, num_epochs, criterion, optimizer):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for i, (sentences, labels) in enumerate(train_loader):\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(sentences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            if (i+1) % 50 == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item(), (correct_predictions/total_predictions)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [50/7777], Loss: 0.5432, Accuracy: 74.19%\n",
      "Epoch: [1/3], Step: [100/7777], Loss: 0.5201, Accuracy: 74.69%\n",
      "Epoch: [1/3], Step: [150/7777], Loss: 0.4409, Accuracy: 75.38%\n",
      "Epoch: [1/3], Step: [200/7777], Loss: 0.5583, Accuracy: 76.73%\n",
      "Epoch: [1/3], Step: [250/7777], Loss: 0.5497, Accuracy: 77.24%\n",
      "Epoch: [1/3], Step: [300/7777], Loss: 0.4808, Accuracy: 77.69%\n",
      "Epoch: [1/3], Step: [350/7777], Loss: 0.4791, Accuracy: 78.05%\n",
      "Epoch: [1/3], Step: [400/7777], Loss: 0.4441, Accuracy: 78.39%\n",
      "Epoch: [1/3], Step: [450/7777], Loss: 0.4962, Accuracy: 78.85%\n",
      "Epoch: [1/3], Step: [500/7777], Loss: 0.3646, Accuracy: 78.88%\n",
      "Epoch: [1/3], Step: [550/7777], Loss: 0.4626, Accuracy: 79.08%\n",
      "Epoch: [1/3], Step: [600/7777], Loss: 0.4746, Accuracy: 79.50%\n",
      "Epoch: [1/3], Step: [650/7777], Loss: 0.4391, Accuracy: 79.75%\n",
      "Epoch: [1/3], Step: [700/7777], Loss: 0.4138, Accuracy: 80.08%\n",
      "Epoch: [1/3], Step: [750/7777], Loss: 0.3630, Accuracy: 80.40%\n",
      "Epoch: [1/3], Step: [800/7777], Loss: 0.3553, Accuracy: 80.70%\n",
      "Epoch: [1/3], Step: [850/7777], Loss: 0.4626, Accuracy: 80.83%\n",
      "Epoch: [1/3], Step: [900/7777], Loss: 0.5096, Accuracy: 80.64%\n",
      "Epoch: [1/3], Step: [950/7777], Loss: 0.4162, Accuracy: 80.64%\n",
      "Epoch: [1/3], Step: [1000/7777], Loss: 0.4353, Accuracy: 80.72%\n",
      "Epoch: [1/3], Step: [1050/7777], Loss: 0.4070, Accuracy: 80.87%\n",
      "Epoch: [1/3], Step: [1100/7777], Loss: 0.3100, Accuracy: 81.04%\n",
      "Epoch: [1/3], Step: [1150/7777], Loss: 0.3269, Accuracy: 81.24%\n",
      "Epoch: [1/3], Step: [1200/7777], Loss: 0.4410, Accuracy: 81.35%\n",
      "Epoch: [1/3], Step: [1250/7777], Loss: 0.3917, Accuracy: 81.30%\n",
      "Epoch: [1/3], Step: [1300/7777], Loss: 0.3421, Accuracy: 81.37%\n",
      "Epoch: [1/3], Step: [1350/7777], Loss: 0.3704, Accuracy: 81.41%\n",
      "Epoch: [1/3], Step: [1400/7777], Loss: 0.3970, Accuracy: 81.49%\n",
      "Epoch: [1/3], Step: [1450/7777], Loss: 0.4438, Accuracy: 81.64%\n",
      "Epoch: [1/3], Step: [1500/7777], Loss: 0.3336, Accuracy: 81.73%\n",
      "Epoch: [1/3], Step: [1550/7777], Loss: 0.1707, Accuracy: 81.89%\n",
      "Epoch: [1/3], Step: [1600/7777], Loss: 0.4801, Accuracy: 82.05%\n",
      "Epoch: [1/3], Step: [1650/7777], Loss: 0.4291, Accuracy: 82.18%\n",
      "Epoch: [1/3], Step: [1700/7777], Loss: 0.3103, Accuracy: 82.34%\n",
      "Epoch: [1/3], Step: [1750/7777], Loss: 0.2506, Accuracy: 82.49%\n",
      "Epoch: [1/3], Step: [1800/7777], Loss: 0.2842, Accuracy: 82.65%\n",
      "Epoch: [1/3], Step: [1850/7777], Loss: 0.3803, Accuracy: 82.77%\n",
      "Epoch: [1/3], Step: [1900/7777], Loss: 0.2894, Accuracy: 82.94%\n",
      "Epoch: [1/3], Step: [1950/7777], Loss: 0.3980, Accuracy: 83.07%\n",
      "Epoch: [1/3], Step: [2000/7777], Loss: 0.3797, Accuracy: 83.23%\n",
      "Epoch: [1/3], Step: [2050/7777], Loss: 0.1452, Accuracy: 83.37%\n",
      "Epoch: [1/3], Step: [2100/7777], Loss: 0.2199, Accuracy: 83.50%\n",
      "Epoch: [1/3], Step: [2150/7777], Loss: 0.2688, Accuracy: 83.62%\n",
      "Epoch: [1/3], Step: [2200/7777], Loss: 0.2150, Accuracy: 83.74%\n",
      "Epoch: [1/3], Step: [2250/7777], Loss: 0.3972, Accuracy: 83.84%\n",
      "Epoch: [1/3], Step: [2300/7777], Loss: 0.2764, Accuracy: 83.98%\n",
      "Epoch: [1/3], Step: [2350/7777], Loss: 0.2195, Accuracy: 84.11%\n",
      "Epoch: [1/3], Step: [2400/7777], Loss: 0.2786, Accuracy: 84.22%\n",
      "Epoch: [1/3], Step: [2450/7777], Loss: 0.3378, Accuracy: 84.34%\n",
      "Epoch: [1/3], Step: [2500/7777], Loss: 0.2076, Accuracy: 84.43%\n",
      "Epoch: [1/3], Step: [2550/7777], Loss: 0.2048, Accuracy: 84.53%\n",
      "Epoch: [1/3], Step: [2600/7777], Loss: 0.3193, Accuracy: 84.63%\n",
      "Epoch: [1/3], Step: [2650/7777], Loss: 0.2803, Accuracy: 84.72%\n",
      "Epoch: [1/3], Step: [2700/7777], Loss: 0.2127, Accuracy: 84.80%\n",
      "Epoch: [1/3], Step: [2750/7777], Loss: 0.2274, Accuracy: 84.90%\n",
      "Epoch: [1/3], Step: [2800/7777], Loss: 0.1755, Accuracy: 84.99%\n",
      "Epoch: [1/3], Step: [2850/7777], Loss: 0.3014, Accuracy: 85.08%\n",
      "Epoch: [1/3], Step: [2900/7777], Loss: 0.2362, Accuracy: 85.17%\n",
      "Epoch: [1/3], Step: [2950/7777], Loss: 0.2220, Accuracy: 85.25%\n",
      "Epoch: [1/3], Step: [3000/7777], Loss: 0.3739, Accuracy: 85.32%\n",
      "Epoch: [1/3], Step: [3050/7777], Loss: 0.1723, Accuracy: 85.39%\n",
      "Epoch: [1/3], Step: [3100/7777], Loss: 0.3077, Accuracy: 85.47%\n",
      "Epoch: [1/3], Step: [3150/7777], Loss: 0.2830, Accuracy: 85.55%\n",
      "Epoch: [1/3], Step: [3200/7777], Loss: 0.1939, Accuracy: 85.63%\n",
      "Epoch: [1/3], Step: [3250/7777], Loss: 0.2512, Accuracy: 85.71%\n",
      "Epoch: [1/3], Step: [3300/7777], Loss: 0.3231, Accuracy: 85.79%\n",
      "Epoch: [1/3], Step: [3350/7777], Loss: 0.1420, Accuracy: 85.86%\n",
      "Epoch: [1/3], Step: [3400/7777], Loss: 0.1842, Accuracy: 85.95%\n",
      "Epoch: [1/3], Step: [3450/7777], Loss: 0.3520, Accuracy: 86.02%\n",
      "Epoch: [1/3], Step: [3500/7777], Loss: 0.1457, Accuracy: 86.10%\n",
      "Epoch: [1/3], Step: [3550/7777], Loss: 0.2338, Accuracy: 86.17%\n",
      "Epoch: [1/3], Step: [3600/7777], Loss: 0.3609, Accuracy: 86.22%\n",
      "Epoch: [1/3], Step: [3650/7777], Loss: 0.1418, Accuracy: 86.30%\n",
      "Epoch: [1/3], Step: [3700/7777], Loss: 0.2713, Accuracy: 86.35%\n",
      "Epoch: [1/3], Step: [3750/7777], Loss: 0.1529, Accuracy: 86.41%\n",
      "Epoch: [1/3], Step: [3800/7777], Loss: 0.2881, Accuracy: 86.49%\n",
      "Epoch: [1/3], Step: [3850/7777], Loss: 0.1348, Accuracy: 86.55%\n",
      "Epoch: [1/3], Step: [3900/7777], Loss: 0.2489, Accuracy: 86.62%\n",
      "Epoch: [1/3], Step: [3950/7777], Loss: 0.3950, Accuracy: 86.68%\n",
      "Epoch: [1/3], Step: [4000/7777], Loss: 0.2317, Accuracy: 86.74%\n",
      "Epoch: [1/3], Step: [4050/7777], Loss: 0.1079, Accuracy: 86.80%\n",
      "Epoch: [1/3], Step: [4100/7777], Loss: 0.2486, Accuracy: 86.86%\n",
      "Epoch: [1/3], Step: [4150/7777], Loss: 0.1171, Accuracy: 86.93%\n",
      "Epoch: [1/3], Step: [4200/7777], Loss: 0.3183, Accuracy: 86.98%\n",
      "Epoch: [1/3], Step: [4250/7777], Loss: 0.2532, Accuracy: 87.03%\n",
      "Epoch: [1/3], Step: [4300/7777], Loss: 0.2279, Accuracy: 87.08%\n",
      "Epoch: [1/3], Step: [4350/7777], Loss: 0.3530, Accuracy: 87.13%\n",
      "Epoch: [1/3], Step: [4400/7777], Loss: 0.1898, Accuracy: 87.18%\n",
      "Epoch: [1/3], Step: [4450/7777], Loss: 0.2884, Accuracy: 87.23%\n",
      "Epoch: [1/3], Step: [4500/7777], Loss: 0.2376, Accuracy: 87.28%\n",
      "Epoch: [1/3], Step: [4550/7777], Loss: 0.1364, Accuracy: 87.33%\n",
      "Epoch: [1/3], Step: [4600/7777], Loss: 0.2901, Accuracy: 87.38%\n",
      "Epoch: [1/3], Step: [4650/7777], Loss: 0.2147, Accuracy: 87.42%\n",
      "Epoch: [1/3], Step: [4700/7777], Loss: 0.3128, Accuracy: 87.47%\n",
      "Epoch: [1/3], Step: [4750/7777], Loss: 0.2426, Accuracy: 87.50%\n",
      "Epoch: [1/3], Step: [4800/7777], Loss: 0.1601, Accuracy: 87.55%\n",
      "Epoch: [1/3], Step: [4850/7777], Loss: 0.2473, Accuracy: 87.59%\n",
      "Epoch: [1/3], Step: [4900/7777], Loss: 0.2091, Accuracy: 87.63%\n",
      "Epoch: [1/3], Step: [4950/7777], Loss: 0.2377, Accuracy: 87.67%\n",
      "Epoch: [1/3], Step: [5000/7777], Loss: 0.2029, Accuracy: 87.72%\n",
      "Epoch: [1/3], Step: [5050/7777], Loss: 0.1189, Accuracy: 87.76%\n",
      "Epoch: [1/3], Step: [5100/7777], Loss: 0.0949, Accuracy: 87.79%\n",
      "Epoch: [1/3], Step: [5150/7777], Loss: 0.1635, Accuracy: 87.83%\n",
      "Epoch: [1/3], Step: [5200/7777], Loss: 0.1775, Accuracy: 87.88%\n",
      "Epoch: [1/3], Step: [5250/7777], Loss: 0.1560, Accuracy: 87.92%\n",
      "Epoch: [1/3], Step: [5300/7777], Loss: 0.2604, Accuracy: 87.96%\n",
      "Epoch: [1/3], Step: [5350/7777], Loss: 0.3055, Accuracy: 88.00%\n",
      "Epoch: [1/3], Step: [5400/7777], Loss: 0.1874, Accuracy: 88.03%\n",
      "Epoch: [1/3], Step: [5450/7777], Loss: 0.1140, Accuracy: 88.06%\n",
      "Epoch: [1/3], Step: [5500/7777], Loss: 0.2183, Accuracy: 88.11%\n",
      "Epoch: [1/3], Step: [5550/7777], Loss: 0.2490, Accuracy: 88.14%\n",
      "Epoch: [1/3], Step: [5600/7777], Loss: 0.1523, Accuracy: 88.18%\n",
      "Epoch: [1/3], Step: [5650/7777], Loss: 0.1327, Accuracy: 88.23%\n",
      "Epoch: [1/3], Step: [5700/7777], Loss: 0.2234, Accuracy: 88.27%\n",
      "Epoch: [1/3], Step: [5750/7777], Loss: 0.1080, Accuracy: 88.30%\n",
      "Epoch: [1/3], Step: [5800/7777], Loss: 0.1545, Accuracy: 88.34%\n",
      "Epoch: [1/3], Step: [5850/7777], Loss: 0.2045, Accuracy: 88.37%\n",
      "Epoch: [1/3], Step: [5900/7777], Loss: 0.3416, Accuracy: 88.40%\n",
      "Epoch: [1/3], Step: [5950/7777], Loss: 0.1495, Accuracy: 88.44%\n",
      "Epoch: [1/3], Step: [6000/7777], Loss: 0.1970, Accuracy: 88.46%\n",
      "Epoch: [1/3], Step: [6050/7777], Loss: 0.1508, Accuracy: 88.49%\n",
      "Epoch: [1/3], Step: [6100/7777], Loss: 0.2089, Accuracy: 88.52%\n",
      "Epoch: [1/3], Step: [6150/7777], Loss: 0.1852, Accuracy: 88.55%\n",
      "Epoch: [1/3], Step: [6200/7777], Loss: 0.1855, Accuracy: 88.59%\n",
      "Epoch: [1/3], Step: [6250/7777], Loss: 0.1454, Accuracy: 88.62%\n",
      "Epoch: [1/3], Step: [6300/7777], Loss: 0.1994, Accuracy: 88.65%\n",
      "Epoch: [1/3], Step: [6350/7777], Loss: 0.2281, Accuracy: 88.68%\n",
      "Epoch: [1/3], Step: [6400/7777], Loss: 0.1283, Accuracy: 88.71%\n",
      "Epoch: [1/3], Step: [6450/7777], Loss: 0.2865, Accuracy: 88.74%\n",
      "Epoch: [1/3], Step: [6500/7777], Loss: 0.2699, Accuracy: 88.77%\n",
      "Epoch: [1/3], Step: [6550/7777], Loss: 0.1457, Accuracy: 88.80%\n",
      "Epoch: [1/3], Step: [6600/7777], Loss: 0.1465, Accuracy: 88.83%\n",
      "Epoch: [1/3], Step: [6650/7777], Loss: 0.3230, Accuracy: 88.86%\n",
      "Epoch: [1/3], Step: [6700/7777], Loss: 0.2834, Accuracy: 88.89%\n",
      "Epoch: [1/3], Step: [6750/7777], Loss: 0.1324, Accuracy: 88.92%\n",
      "Epoch: [1/3], Step: [6800/7777], Loss: 0.2376, Accuracy: 88.94%\n",
      "Epoch: [1/3], Step: [6850/7777], Loss: 0.2272, Accuracy: 88.97%\n",
      "Epoch: [1/3], Step: [6900/7777], Loss: 0.1543, Accuracy: 89.00%\n",
      "Epoch: [1/3], Step: [6950/7777], Loss: 0.1026, Accuracy: 89.02%\n",
      "Epoch: [1/3], Step: [7000/7777], Loss: 0.1573, Accuracy: 89.05%\n",
      "Epoch: [1/3], Step: [7050/7777], Loss: 0.1626, Accuracy: 89.07%\n",
      "Epoch: [1/3], Step: [7100/7777], Loss: 0.1786, Accuracy: 89.10%\n",
      "Epoch: [1/3], Step: [7150/7777], Loss: 0.1113, Accuracy: 89.12%\n",
      "Epoch: [1/3], Step: [7200/7777], Loss: 0.1418, Accuracy: 89.14%\n",
      "Epoch: [1/3], Step: [7250/7777], Loss: 0.1609, Accuracy: 89.17%\n",
      "Epoch: [1/3], Step: [7300/7777], Loss: 0.1689, Accuracy: 89.20%\n",
      "Epoch: [1/3], Step: [7350/7777], Loss: 0.1889, Accuracy: 89.22%\n",
      "Epoch: [1/3], Step: [7400/7777], Loss: 0.1309, Accuracy: 89.25%\n",
      "Epoch: [1/3], Step: [7450/7777], Loss: 0.1732, Accuracy: 89.27%\n",
      "Epoch: [1/3], Step: [7500/7777], Loss: 0.1889, Accuracy: 89.30%\n",
      "Epoch: [1/3], Step: [7550/7777], Loss: 0.2497, Accuracy: 89.32%\n",
      "Epoch: [1/3], Step: [7600/7777], Loss: 0.1522, Accuracy: 89.35%\n",
      "Epoch: [1/3], Step: [7650/7777], Loss: 0.1863, Accuracy: 89.37%\n",
      "Epoch: [1/3], Step: [7700/7777], Loss: 0.1373, Accuracy: 89.39%\n",
      "Epoch: [1/3], Step: [7750/7777], Loss: 0.1368, Accuracy: 89.41%\n",
      "Epoch: [2/3], Step: [50/7777], Loss: 0.1774, Accuracy: 94.84%\n",
      "Epoch: [2/3], Step: [100/7777], Loss: 0.0809, Accuracy: 94.81%\n",
      "Epoch: [2/3], Step: [150/7777], Loss: 0.1065, Accuracy: 94.65%\n",
      "Epoch: [2/3], Step: [200/7777], Loss: 0.1227, Accuracy: 94.45%\n",
      "Epoch: [2/3], Step: [250/7777], Loss: 0.1618, Accuracy: 94.36%\n",
      "Epoch: [2/3], Step: [300/7777], Loss: 0.2462, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [350/7777], Loss: 0.2828, Accuracy: 94.35%\n",
      "Epoch: [2/3], Step: [400/7777], Loss: 0.0710, Accuracy: 94.35%\n",
      "Epoch: [2/3], Step: [450/7777], Loss: 0.1600, Accuracy: 94.30%\n",
      "Epoch: [2/3], Step: [500/7777], Loss: 0.3041, Accuracy: 94.26%\n",
      "Epoch: [2/3], Step: [550/7777], Loss: 0.1805, Accuracy: 94.28%\n",
      "Epoch: [2/3], Step: [600/7777], Loss: 0.1678, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [650/7777], Loss: 0.1881, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [700/7777], Loss: 0.1021, Accuracy: 94.38%\n",
      "Epoch: [2/3], Step: [750/7777], Loss: 0.0622, Accuracy: 94.42%\n",
      "Epoch: [2/3], Step: [800/7777], Loss: 0.2226, Accuracy: 94.40%\n",
      "Epoch: [2/3], Step: [850/7777], Loss: 0.0933, Accuracy: 94.39%\n",
      "Epoch: [2/3], Step: [900/7777], Loss: 0.1490, Accuracy: 94.39%\n",
      "Epoch: [2/3], Step: [950/7777], Loss: 0.1628, Accuracy: 94.40%\n",
      "Epoch: [2/3], Step: [1000/7777], Loss: 0.2400, Accuracy: 94.39%\n",
      "Epoch: [2/3], Step: [1050/7777], Loss: 0.0890, Accuracy: 94.39%\n",
      "Epoch: [2/3], Step: [1100/7777], Loss: 0.2499, Accuracy: 94.41%\n",
      "Epoch: [2/3], Step: [1150/7777], Loss: 0.1688, Accuracy: 94.41%\n",
      "Epoch: [2/3], Step: [1200/7777], Loss: 0.2158, Accuracy: 94.42%\n",
      "Epoch: [2/3], Step: [1250/7777], Loss: 0.0922, Accuracy: 94.42%\n",
      "Epoch: [2/3], Step: [1300/7777], Loss: 0.2333, Accuracy: 94.41%\n",
      "Epoch: [2/3], Step: [1350/7777], Loss: 0.0678, Accuracy: 94.41%\n",
      "Epoch: [2/3], Step: [1400/7777], Loss: 0.0956, Accuracy: 94.40%\n",
      "Epoch: [2/3], Step: [1450/7777], Loss: 0.1665, Accuracy: 94.40%\n",
      "Epoch: [2/3], Step: [1500/7777], Loss: 0.1699, Accuracy: 94.39%\n",
      "Epoch: [2/3], Step: [1550/7777], Loss: 0.1408, Accuracy: 94.40%\n",
      "Epoch: [2/3], Step: [1600/7777], Loss: 0.1125, Accuracy: 94.40%\n",
      "Epoch: [2/3], Step: [1650/7777], Loss: 0.2217, Accuracy: 94.38%\n",
      "Epoch: [2/3], Step: [1700/7777], Loss: 0.0672, Accuracy: 94.39%\n",
      "Epoch: [2/3], Step: [1750/7777], Loss: 0.1903, Accuracy: 94.37%\n",
      "Epoch: [2/3], Step: [1800/7777], Loss: 0.2052, Accuracy: 94.37%\n",
      "Epoch: [2/3], Step: [1850/7777], Loss: 0.1507, Accuracy: 94.36%\n",
      "Epoch: [2/3], Step: [1900/7777], Loss: 0.1037, Accuracy: 94.37%\n",
      "Epoch: [2/3], Step: [1950/7777], Loss: 0.1798, Accuracy: 94.36%\n",
      "Epoch: [2/3], Step: [2000/7777], Loss: 0.0993, Accuracy: 94.36%\n",
      "Epoch: [2/3], Step: [2050/7777], Loss: 0.0912, Accuracy: 94.35%\n",
      "Epoch: [2/3], Step: [2100/7777], Loss: 0.2365, Accuracy: 94.35%\n",
      "Epoch: [2/3], Step: [2150/7777], Loss: 0.1854, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2200/7777], Loss: 0.1116, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [2250/7777], Loss: 0.2027, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [2300/7777], Loss: 0.1749, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [2350/7777], Loss: 0.0680, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2400/7777], Loss: 0.1565, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2450/7777], Loss: 0.2882, Accuracy: 94.35%\n",
      "Epoch: [2/3], Step: [2500/7777], Loss: 0.0899, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2550/7777], Loss: 0.1920, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2600/7777], Loss: 0.0679, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2650/7777], Loss: 0.1213, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [2700/7777], Loss: 0.0980, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2750/7777], Loss: 0.0997, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2800/7777], Loss: 0.1718, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2850/7777], Loss: 0.1785, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [2900/7777], Loss: 0.1563, Accuracy: 94.35%\n",
      "Epoch: [2/3], Step: [2950/7777], Loss: 0.1288, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [3000/7777], Loss: 0.1738, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [3050/7777], Loss: 0.1243, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3100/7777], Loss: 0.2467, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3150/7777], Loss: 0.1115, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3200/7777], Loss: 0.1362, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [3250/7777], Loss: 0.1895, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [3300/7777], Loss: 0.1068, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3350/7777], Loss: 0.2616, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3400/7777], Loss: 0.1048, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3450/7777], Loss: 0.1360, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3500/7777], Loss: 0.2149, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [3550/7777], Loss: 0.1956, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [3600/7777], Loss: 0.1790, Accuracy: 94.30%\n",
      "Epoch: [2/3], Step: [3650/7777], Loss: 0.1532, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [3700/7777], Loss: 0.2402, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [3750/7777], Loss: 0.2331, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [3800/7777], Loss: 0.1326, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [3850/7777], Loss: 0.0726, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [3900/7777], Loss: 0.1293, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [3950/7777], Loss: 0.1605, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [4000/7777], Loss: 0.1825, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [4050/7777], Loss: 0.2297, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4100/7777], Loss: 0.0802, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4150/7777], Loss: 0.2011, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4200/7777], Loss: 0.1000, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4250/7777], Loss: 0.1346, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4300/7777], Loss: 0.1155, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4350/7777], Loss: 0.1832, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4400/7777], Loss: 0.1994, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4450/7777], Loss: 0.0616, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4500/7777], Loss: 0.2433, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [4550/7777], Loss: 0.1132, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [4600/7777], Loss: 0.0651, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [4650/7777], Loss: 0.1112, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [4700/7777], Loss: 0.2553, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [4750/7777], Loss: 0.1542, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [4800/7777], Loss: 0.2609, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [4850/7777], Loss: 0.2265, Accuracy: 94.34%\n",
      "Epoch: [2/3], Step: [4900/7777], Loss: 0.1565, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [4950/7777], Loss: 0.1222, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5000/7777], Loss: 0.1467, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5050/7777], Loss: 0.1505, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5100/7777], Loss: 0.1169, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5150/7777], Loss: 0.2124, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5200/7777], Loss: 0.0707, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5250/7777], Loss: 0.1299, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5300/7777], Loss: 0.0923, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5350/7777], Loss: 0.1461, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5400/7777], Loss: 0.2476, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5450/7777], Loss: 0.1330, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5500/7777], Loss: 0.1175, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5550/7777], Loss: 0.1767, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5600/7777], Loss: 0.0818, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5650/7777], Loss: 0.0400, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5700/7777], Loss: 0.1064, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5750/7777], Loss: 0.1002, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5800/7777], Loss: 0.0914, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5850/7777], Loss: 0.1554, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5900/7777], Loss: 0.0352, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [5950/7777], Loss: 0.1073, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [6000/7777], Loss: 0.0605, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [6050/7777], Loss: 0.1153, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [6100/7777], Loss: 0.0806, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [6150/7777], Loss: 0.4148, Accuracy: 94.33%\n",
      "Epoch: [2/3], Step: [6200/7777], Loss: 0.1429, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6250/7777], Loss: 0.1240, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6300/7777], Loss: 0.1786, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6350/7777], Loss: 0.2147, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6400/7777], Loss: 0.0783, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6450/7777], Loss: 0.1572, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6500/7777], Loss: 0.2219, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6550/7777], Loss: 0.1984, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6600/7777], Loss: 0.1065, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [6650/7777], Loss: 0.0895, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [6700/7777], Loss: 0.1339, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [6750/7777], Loss: 0.1093, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [6800/7777], Loss: 0.1302, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [6850/7777], Loss: 0.0944, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [6900/7777], Loss: 0.1370, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [6950/7777], Loss: 0.1898, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [7000/7777], Loss: 0.0907, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7050/7777], Loss: 0.1155, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7100/7777], Loss: 0.1840, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7150/7777], Loss: 0.2641, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7200/7777], Loss: 0.1619, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7250/7777], Loss: 0.1367, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [7300/7777], Loss: 0.1878, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7350/7777], Loss: 0.1692, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7400/7777], Loss: 0.1078, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7450/7777], Loss: 0.1434, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7500/7777], Loss: 0.0753, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7550/7777], Loss: 0.1625, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7600/7777], Loss: 0.1212, Accuracy: 94.31%\n",
      "Epoch: [2/3], Step: [7650/7777], Loss: 0.1472, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7700/7777], Loss: 0.1743, Accuracy: 94.32%\n",
      "Epoch: [2/3], Step: [7750/7777], Loss: 0.1426, Accuracy: 94.32%\n",
      "Epoch: [3/3], Step: [50/7777], Loss: 0.0485, Accuracy: 95.97%\n",
      "Epoch: [3/3], Step: [100/7777], Loss: 0.0484, Accuracy: 96.31%\n",
      "Epoch: [3/3], Step: [150/7777], Loss: 0.0838, Accuracy: 96.34%\n",
      "Epoch: [3/3], Step: [200/7777], Loss: 0.0984, Accuracy: 96.34%\n",
      "Epoch: [3/3], Step: [250/7777], Loss: 0.1053, Accuracy: 96.33%\n",
      "Epoch: [3/3], Step: [300/7777], Loss: 0.1025, Accuracy: 96.26%\n",
      "Epoch: [3/3], Step: [350/7777], Loss: 0.0520, Accuracy: 96.32%\n",
      "Epoch: [3/3], Step: [400/7777], Loss: 0.1147, Accuracy: 96.33%\n",
      "Epoch: [3/3], Step: [450/7777], Loss: 0.0327, Accuracy: 96.43%\n",
      "Epoch: [3/3], Step: [500/7777], Loss: 0.1336, Accuracy: 96.41%\n",
      "Epoch: [3/3], Step: [550/7777], Loss: 0.1159, Accuracy: 96.42%\n",
      "Epoch: [3/3], Step: [600/7777], Loss: 0.0209, Accuracy: 96.42%\n",
      "Epoch: [3/3], Step: [650/7777], Loss: 0.0811, Accuracy: 96.37%\n",
      "Epoch: [3/3], Step: [700/7777], Loss: 0.0985, Accuracy: 96.38%\n",
      "Epoch: [3/3], Step: [750/7777], Loss: 0.0709, Accuracy: 96.38%\n",
      "Epoch: [3/3], Step: [800/7777], Loss: 0.0232, Accuracy: 96.36%\n",
      "Epoch: [3/3], Step: [850/7777], Loss: 0.1285, Accuracy: 96.34%\n",
      "Epoch: [3/3], Step: [900/7777], Loss: 0.1351, Accuracy: 96.29%\n",
      "Epoch: [3/3], Step: [950/7777], Loss: 0.0452, Accuracy: 96.27%\n",
      "Epoch: [3/3], Step: [1000/7777], Loss: 0.1162, Accuracy: 96.27%\n",
      "Epoch: [3/3], Step: [1050/7777], Loss: 0.0625, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [1100/7777], Loss: 0.0285, Accuracy: 96.22%\n",
      "Epoch: [3/3], Step: [1150/7777], Loss: 0.1123, Accuracy: 96.22%\n",
      "Epoch: [3/3], Step: [1200/7777], Loss: 0.1168, Accuracy: 96.24%\n",
      "Epoch: [3/3], Step: [1250/7777], Loss: 0.0871, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [1300/7777], Loss: 0.2548, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [1350/7777], Loss: 0.0482, Accuracy: 96.23%\n",
      "Epoch: [3/3], Step: [1400/7777], Loss: 0.2382, Accuracy: 96.24%\n",
      "Epoch: [3/3], Step: [1450/7777], Loss: 0.0719, Accuracy: 96.23%\n",
      "Epoch: [3/3], Step: [1500/7777], Loss: 0.1040, Accuracy: 96.23%\n",
      "Epoch: [3/3], Step: [1550/7777], Loss: 0.0686, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [1600/7777], Loss: 0.1795, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [1650/7777], Loss: 0.1184, Accuracy: 96.26%\n",
      "Epoch: [3/3], Step: [1700/7777], Loss: 0.0961, Accuracy: 96.24%\n",
      "Epoch: [3/3], Step: [1750/7777], Loss: 0.0337, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [1800/7777], Loss: 0.0855, Accuracy: 96.23%\n",
      "Epoch: [3/3], Step: [1850/7777], Loss: 0.1388, Accuracy: 96.24%\n",
      "Epoch: [3/3], Step: [1900/7777], Loss: 0.0940, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [1950/7777], Loss: 0.2316, Accuracy: 96.26%\n",
      "Epoch: [3/3], Step: [2000/7777], Loss: 0.1509, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [2050/7777], Loss: 0.0926, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [2100/7777], Loss: 0.1221, Accuracy: 96.26%\n",
      "Epoch: [3/3], Step: [2150/7777], Loss: 0.0782, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [2200/7777], Loss: 0.0556, Accuracy: 96.25%\n",
      "Epoch: [3/3], Step: [2250/7777], Loss: 0.1659, Accuracy: 96.24%\n",
      "Epoch: [3/3], Step: [2300/7777], Loss: 0.0873, Accuracy: 96.24%\n",
      "Epoch: [3/3], Step: [2350/7777], Loss: 0.0775, Accuracy: 96.23%\n",
      "Epoch: [3/3], Step: [2400/7777], Loss: 0.1627, Accuracy: 96.21%\n",
      "Epoch: [3/3], Step: [2450/7777], Loss: 0.1176, Accuracy: 96.20%\n",
      "Epoch: [3/3], Step: [2500/7777], Loss: 0.0471, Accuracy: 96.21%\n",
      "Epoch: [3/3], Step: [2550/7777], Loss: 0.1119, Accuracy: 96.21%\n",
      "Epoch: [3/3], Step: [2600/7777], Loss: 0.1425, Accuracy: 96.21%\n",
      "Epoch: [3/3], Step: [2650/7777], Loss: 0.1387, Accuracy: 96.21%\n",
      "Epoch: [3/3], Step: [2700/7777], Loss: 0.1431, Accuracy: 96.20%\n",
      "Epoch: [3/3], Step: [2750/7777], Loss: 0.3058, Accuracy: 96.19%\n",
      "Epoch: [3/3], Step: [2800/7777], Loss: 0.1630, Accuracy: 96.18%\n",
      "Epoch: [3/3], Step: [2850/7777], Loss: 0.0800, Accuracy: 96.19%\n",
      "Epoch: [3/3], Step: [2900/7777], Loss: 0.0796, Accuracy: 96.19%\n",
      "Epoch: [3/3], Step: [2950/7777], Loss: 0.0374, Accuracy: 96.18%\n",
      "Epoch: [3/3], Step: [3000/7777], Loss: 0.1707, Accuracy: 96.17%\n",
      "Epoch: [3/3], Step: [3050/7777], Loss: 0.0299, Accuracy: 96.17%\n",
      "Epoch: [3/3], Step: [3100/7777], Loss: 0.2237, Accuracy: 96.16%\n",
      "Epoch: [3/3], Step: [3150/7777], Loss: 0.0442, Accuracy: 96.17%\n",
      "Epoch: [3/3], Step: [3200/7777], Loss: 0.1242, Accuracy: 96.17%\n",
      "Epoch: [3/3], Step: [3250/7777], Loss: 0.0910, Accuracy: 96.16%\n",
      "Epoch: [3/3], Step: [3300/7777], Loss: 0.1203, Accuracy: 96.15%\n",
      "Epoch: [3/3], Step: [3350/7777], Loss: 0.0772, Accuracy: 96.14%\n",
      "Epoch: [3/3], Step: [3400/7777], Loss: 0.1636, Accuracy: 96.13%\n",
      "Epoch: [3/3], Step: [3450/7777], Loss: 0.1116, Accuracy: 96.14%\n",
      "Epoch: [3/3], Step: [3500/7777], Loss: 0.0463, Accuracy: 96.13%\n",
      "Epoch: [3/3], Step: [3550/7777], Loss: 0.1564, Accuracy: 96.13%\n",
      "Epoch: [3/3], Step: [3600/7777], Loss: 0.0797, Accuracy: 96.12%\n",
      "Epoch: [3/3], Step: [3650/7777], Loss: 0.0989, Accuracy: 96.12%\n",
      "Epoch: [3/3], Step: [3700/7777], Loss: 0.1558, Accuracy: 96.12%\n",
      "Epoch: [3/3], Step: [3750/7777], Loss: 0.0517, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [3800/7777], Loss: 0.1040, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [3850/7777], Loss: 0.1175, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [3900/7777], Loss: 0.0805, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [3950/7777], Loss: 0.0969, Accuracy: 96.12%\n",
      "Epoch: [3/3], Step: [4000/7777], Loss: 0.0444, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4050/7777], Loss: 0.1327, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4100/7777], Loss: 0.1211, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4150/7777], Loss: 0.1671, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4200/7777], Loss: 0.0835, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4250/7777], Loss: 0.1667, Accuracy: 96.10%\n",
      "Epoch: [3/3], Step: [4300/7777], Loss: 0.1620, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4350/7777], Loss: 0.1504, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4400/7777], Loss: 0.1069, Accuracy: 96.12%\n",
      "Epoch: [3/3], Step: [4450/7777], Loss: 0.1966, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4500/7777], Loss: 0.0991, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4550/7777], Loss: 0.0849, Accuracy: 96.11%\n",
      "Epoch: [3/3], Step: [4600/7777], Loss: 0.0788, Accuracy: 96.10%\n",
      "Epoch: [3/3], Step: [4650/7777], Loss: 0.2774, Accuracy: 96.10%\n",
      "Epoch: [3/3], Step: [4700/7777], Loss: 0.0404, Accuracy: 96.09%\n",
      "Epoch: [3/3], Step: [4750/7777], Loss: 0.0698, Accuracy: 96.10%\n",
      "Epoch: [3/3], Step: [4800/7777], Loss: 0.0374, Accuracy: 96.10%\n",
      "Epoch: [3/3], Step: [4850/7777], Loss: 0.1545, Accuracy: 96.09%\n",
      "Epoch: [3/3], Step: [4900/7777], Loss: 0.0708, Accuracy: 96.09%\n",
      "Epoch: [3/3], Step: [4950/7777], Loss: 0.3230, Accuracy: 96.09%\n",
      "Epoch: [3/3], Step: [5000/7777], Loss: 0.0659, Accuracy: 96.09%\n",
      "Epoch: [3/3], Step: [5050/7777], Loss: 0.0805, Accuracy: 96.08%\n",
      "Epoch: [3/3], Step: [5100/7777], Loss: 0.1714, Accuracy: 96.08%\n",
      "Epoch: [3/3], Step: [5150/7777], Loss: 0.1079, Accuracy: 96.08%\n",
      "Epoch: [3/3], Step: [5200/7777], Loss: 0.0694, Accuracy: 96.08%\n",
      "Epoch: [3/3], Step: [5250/7777], Loss: 0.1185, Accuracy: 96.07%\n",
      "Epoch: [3/3], Step: [5300/7777], Loss: 0.0678, Accuracy: 96.07%\n",
      "Epoch: [3/3], Step: [5350/7777], Loss: 0.0814, Accuracy: 96.07%\n",
      "Epoch: [3/3], Step: [5400/7777], Loss: 0.0277, Accuracy: 96.08%\n",
      "Epoch: [3/3], Step: [5450/7777], Loss: 0.1313, Accuracy: 96.08%\n",
      "Epoch: [3/3], Step: [5500/7777], Loss: 0.1829, Accuracy: 96.08%\n",
      "Epoch: [3/3], Step: [5550/7777], Loss: 0.0775, Accuracy: 96.07%\n",
      "Epoch: [3/3], Step: [5600/7777], Loss: 0.2236, Accuracy: 96.07%\n",
      "Epoch: [3/3], Step: [5650/7777], Loss: 0.0957, Accuracy: 96.07%\n",
      "Epoch: [3/3], Step: [5700/7777], Loss: 0.1446, Accuracy: 96.06%\n",
      "Epoch: [3/3], Step: [5750/7777], Loss: 0.0704, Accuracy: 96.05%\n",
      "Epoch: [3/3], Step: [5800/7777], Loss: 0.0886, Accuracy: 96.04%\n",
      "Epoch: [3/3], Step: [5850/7777], Loss: 0.1806, Accuracy: 96.04%\n",
      "Epoch: [3/3], Step: [5900/7777], Loss: 0.1496, Accuracy: 96.04%\n",
      "Epoch: [3/3], Step: [5950/7777], Loss: 0.0759, Accuracy: 96.03%\n",
      "Epoch: [3/3], Step: [6000/7777], Loss: 0.1171, Accuracy: 96.03%\n",
      "Epoch: [3/3], Step: [6050/7777], Loss: 0.1043, Accuracy: 96.03%\n",
      "Epoch: [3/3], Step: [6100/7777], Loss: 0.1024, Accuracy: 96.02%\n",
      "Epoch: [3/3], Step: [6150/7777], Loss: 0.1178, Accuracy: 96.02%\n",
      "Epoch: [3/3], Step: [6200/7777], Loss: 0.1139, Accuracy: 96.02%\n",
      "Epoch: [3/3], Step: [6250/7777], Loss: 0.1679, Accuracy: 96.01%\n",
      "Epoch: [3/3], Step: [6300/7777], Loss: 0.1067, Accuracy: 96.01%\n",
      "Epoch: [3/3], Step: [6350/7777], Loss: 0.1375, Accuracy: 96.01%\n",
      "Epoch: [3/3], Step: [6400/7777], Loss: 0.1561, Accuracy: 96.01%\n",
      "Epoch: [3/3], Step: [6450/7777], Loss: 0.2623, Accuracy: 96.00%\n",
      "Epoch: [3/3], Step: [6500/7777], Loss: 0.0896, Accuracy: 96.00%\n",
      "Epoch: [3/3], Step: [6550/7777], Loss: 0.1944, Accuracy: 96.00%\n",
      "Epoch: [3/3], Step: [6600/7777], Loss: 0.0946, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [6650/7777], Loss: 0.1221, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [6700/7777], Loss: 0.1536, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [6750/7777], Loss: 0.0665, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [6800/7777], Loss: 0.0887, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [6850/7777], Loss: 0.1142, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [6900/7777], Loss: 0.1664, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [6950/7777], Loss: 0.0903, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [7000/7777], Loss: 0.1422, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [7050/7777], Loss: 0.0558, Accuracy: 95.98%\n",
      "Epoch: [3/3], Step: [7100/7777], Loss: 0.1452, Accuracy: 95.99%\n",
      "Epoch: [3/3], Step: [7150/7777], Loss: 0.1057, Accuracy: 95.98%\n",
      "Epoch: [3/3], Step: [7200/7777], Loss: 0.1208, Accuracy: 95.98%\n",
      "Epoch: [3/3], Step: [7250/7777], Loss: 0.0899, Accuracy: 95.98%\n",
      "Epoch: [3/3], Step: [7300/7777], Loss: 0.0671, Accuracy: 95.98%\n",
      "Epoch: [3/3], Step: [7350/7777], Loss: 0.1155, Accuracy: 95.97%\n",
      "Epoch: [3/3], Step: [7400/7777], Loss: 0.2473, Accuracy: 95.98%\n",
      "Epoch: [3/3], Step: [7450/7777], Loss: 0.1961, Accuracy: 95.98%\n",
      "Epoch: [3/3], Step: [7500/7777], Loss: 0.1918, Accuracy: 95.97%\n",
      "Epoch: [3/3], Step: [7550/7777], Loss: 0.1216, Accuracy: 95.97%\n",
      "Epoch: [3/3], Step: [7600/7777], Loss: 0.1926, Accuracy: 95.96%\n",
      "Epoch: [3/3], Step: [7650/7777], Loss: 0.0678, Accuracy: 95.96%\n",
      "Epoch: [3/3], Step: [7700/7777], Loss: 0.1004, Accuracy: 95.96%\n",
      "Epoch: [3/3], Step: [7750/7777], Loss: 0.1282, Accuracy: 95.96%\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model entirely using pkl\n",
    "with open('amazon_model_lstm_pytorch.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model entirely using pkl\n",
    "with open('amazon_model_lstm_pytorch.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict as a .pth file\n",
    "torch.save(model.state_dict(), 'amazon_model_lstm_after_load.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loop with classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for i, (sentences, labels) in enumerate(test_loader):\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(sentences)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "        print('Test Accuracy: {:.2f}%'.format((correct_predictions/total_predictions)*100))\n",
    "        print(classification_report(labels.cpu(), predicted.cpu()))\n",
    "        print(confusion_matrix(labels.cpu(), predicted.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.49%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.88        20\n",
      "           1       0.95      0.84      0.89        25\n",
      "\n",
      "    accuracy                           0.89        45\n",
      "   macro avg       0.89      0.90      0.89        45\n",
      "weighted avg       0.90      0.89      0.89        45\n",
      "\n",
      "[[19  1]\n",
      " [ 4 21]]\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
