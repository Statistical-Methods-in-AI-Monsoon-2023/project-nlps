{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dbpedia_14 (/home/sriteja/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c302a2b06bb94deab13a75eb39fdd550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dbpedia_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('dbpedia.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('dbpedia.pkl', 'rb') as file:\n",
    "#     dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 560000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'content'],\n",
      "        num_rows: 70000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data_title = train_dataset['title']\n",
    "train_data_content = train_dataset['content']\n",
    "train_labels = train_dataset['label']\n",
    "test_data_title = test_dataset['title']\n",
    "test_data_content = test_dataset['content']\n",
    "test_labels = test_dataset['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_title[0])\n",
    "# print(train_data_content[0])\n",
    "# print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data_title + train_data_content\n",
    "# test_data = test_data_title + test_data_content\n",
    "\n",
    "train_data = [None] * len(train_data_title)\n",
    "for i in range(len(train_data_title)):\n",
    "    train_data[i] = train_data_title[i] + \" <mid> \" + train_data_content[i]\n",
    "\n",
    "test_data = [None] * len(test_data_title)\n",
    "for i in range(len(test_data_title)):\n",
    "    test_data[i] = test_data_title[i] + \" <mid> \" + test_data_content[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E. D. Abbott Ltd <mid>  Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.\n",
      "TY KU <mid>  TY KU /taɪkuː/ is an American alcoholic beverage company that specializes in sake and other spirits. The privately-held company was founded in 2004 and is headquartered in New York City New York. While based in New York TY KU's beverages are made in Japan through a joint venture with two sake breweries. Since 2011 TY KU's growth has extended its products into all 50 states.\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sriteja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sriteja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /home/sriteja/anaconda3/lib/python3.10/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/sriteja/anaconda3/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sriteja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# def custom_contractions_fix(text):\n",
    "#     # Define custom contractions to expand\n",
    "#     contractions_dict = {\n",
    "#         \"don't\": \"do not\",\n",
    "#         \"doesn't\": \"does not\",\n",
    "#         \"didn't\": \"did not\",\n",
    "#         # Add more contractions as needed\n",
    "#     }\n",
    "    \n",
    "#     # Use a regular expression to find and replace contractions\n",
    "#     contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "#     def replace(match):\n",
    "#         return contractions_dict[match.group(0)]\n",
    "    \n",
    "#     expanded_text = contractions_re.sub(replace, text)\n",
    "#     return expanded_text\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # if there is a word between () then write it once\n",
    "    text = re.sub(r'\\((.*?)\\)', r'\\1', text)\n",
    "    \n",
    "    # Apply custom contractions expansion\n",
    "    # text = custom_contractions_fix(text)\n",
    "    \n",
    "    expanded_words = []\n",
    "    for word in text.split():\n",
    "        # using contractions.fix to expand the shortened words\n",
    "        # print(word)\n",
    "        try:\n",
    "            expanded_words.append(contractions.fix(word))\n",
    "        except:\n",
    "            expanded_words.append(word)\n",
    "        # expanded_words.append(contractions.fix(word)) \n",
    "        \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    text = expanded_text  \n",
    "    \n",
    "    # Replace hyphens with spaces\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # replace colons with spaces\n",
    "    text = text.replace(':', ' ')\n",
    "    \n",
    "    # replace commas with spaces\n",
    "    text = text.replace(',', ' ')\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "    # Convert to lowercase\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # Remove periods\n",
    "    tokens = [w.replace('.', '') for w in tokens]\n",
    "    \n",
    "    # replace all numbers with <num>\n",
    "    tokens = [re.sub(r'\\d+', 'NUM', w) for w in tokens]\n",
    "    \n",
    "    # replace the word mid everywhere with MID\n",
    "    tokens = [w.replace('mid', 'MID') for w in tokens]\n",
    "    \n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n",
      "420000\n",
      "421000\n",
      "422000\n",
      "423000\n",
      "424000\n",
      "425000\n",
      "426000\n",
      "427000\n",
      "428000\n",
      "429000\n",
      "430000\n",
      "431000\n",
      "432000\n",
      "433000\n",
      "434000\n",
      "435000\n",
      "436000\n",
      "437000\n",
      "438000\n",
      "439000\n",
      "440000\n",
      "441000\n",
      "442000\n",
      "443000\n",
      "444000\n",
      "445000\n",
      "446000\n",
      "447000\n",
      "448000\n",
      "449000\n",
      "450000\n",
      "451000\n",
      "452000\n",
      "453000\n",
      "454000\n",
      "455000\n",
      "456000\n",
      "457000\n",
      "458000\n",
      "459000\n",
      "460000\n",
      "461000\n",
      "462000\n",
      "463000\n",
      "464000\n",
      "465000\n",
      "466000\n",
      "467000\n",
      "468000\n",
      "469000\n",
      "470000\n",
      "471000\n",
      "472000\n",
      "473000\n",
      "474000\n",
      "475000\n",
      "476000\n",
      "477000\n",
      "478000\n",
      "479000\n",
      "480000\n",
      "481000\n",
      "482000\n",
      "483000\n",
      "484000\n",
      "485000\n",
      "486000\n",
      "487000\n",
      "488000\n",
      "489000\n",
      "490000\n",
      "491000\n",
      "492000\n",
      "493000\n",
      "494000\n",
      "495000\n",
      "496000\n",
      "497000\n",
      "498000\n",
      "499000\n",
      "500000\n",
      "501000\n",
      "502000\n",
      "503000\n",
      "504000\n",
      "505000\n",
      "506000\n",
      "507000\n",
      "508000\n",
      "509000\n",
      "510000\n",
      "511000\n",
      "512000\n",
      "513000\n",
      "514000\n",
      "515000\n",
      "516000\n",
      "517000\n",
      "518000\n",
      "519000\n",
      "520000\n",
      "521000\n",
      "522000\n",
      "523000\n",
      "524000\n",
      "525000\n",
      "526000\n",
      "527000\n",
      "528000\n",
      "529000\n",
      "530000\n",
      "531000\n",
      "532000\n",
      "533000\n",
      "534000\n",
      "535000\n",
      "536000\n",
      "537000\n",
      "538000\n",
      "539000\n",
      "540000\n",
      "541000\n",
      "542000\n",
      "543000\n",
      "544000\n",
      "545000\n",
      "546000\n",
      "547000\n",
      "548000\n",
      "549000\n",
      "550000\n",
      "551000\n",
      "552000\n",
      "553000\n",
      "554000\n",
      "555000\n",
      "556000\n",
      "557000\n",
      "558000\n",
      "559000\n"
     ]
    }
   ],
   "source": [
    "# print(train_data[29876])\n",
    "# print(tokenize_text(train_data[29876]))\n",
    "\n",
    "tokens = []\n",
    "print(len(train_data))\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    tokens.append(tokenize_text(train_data[i]))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "# save to pkl file\n",
    "with open('dbpedia_train_tokens.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n"
     ]
    }
   ],
   "source": [
    "tokens_test = []\n",
    "print(len(test_data))\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    tokens_test.append(tokenize_text(test_data[i]))\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "# save to pkl file\n",
    "with open('dbpedia_test_tokens.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from pkl file\n",
    "with open('dbpedia_train_tokens.pkl', 'rb') as file:\n",
    "    tokens = pickle.load(file)\n",
    "    \n",
    "with open('dbpedia_test_tokens.pkl', 'rb') as file:\n",
    "    tokens_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  1473\n",
      "Min length:  3\n",
      "Number of sentences with length > 100:  1207\n",
      "558793\n",
      "558793\n",
      "69839\n",
      "69839\n"
     ]
    }
   ],
   "source": [
    "# find length of longest sentence\n",
    "max_len = 0\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) > max_len:\n",
    "        max_len = len(tokens[i])\n",
    "        \n",
    "# find length of shortest sentence\n",
    "min_len = 1000000\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) < min_len:\n",
    "        min_len = len(tokens[i])\n",
    "        \n",
    "print(\"Max length: \", max_len)\n",
    "print(\"Min length: \", min_len)\n",
    "\n",
    "# number of sentences with length > 100\n",
    "count = 0\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) > 100:\n",
    "        count += 1\n",
    "        \n",
    "print(\"Number of sentences with length > 100: \", count)\n",
    "\n",
    "# remove sentences with length > 100 along with their labels\n",
    "new_tokens = []\n",
    "new_labels = []\n",
    "for i in range(len(tokens)):\n",
    "    if len(tokens[i]) <= 100:\n",
    "        new_tokens.append(tokens[i])\n",
    "        new_labels.append(train_labels[i])\n",
    "        \n",
    "# remove so in test also\n",
    "new_tokens_test = []\n",
    "new_labels_test = []\n",
    "for i in range(len(tokens_test)):\n",
    "    if len(tokens_test[i]) <= 100:\n",
    "        new_tokens_test.append(tokens_test[i])\n",
    "        new_labels_test.append(test_labels[i])\n",
    "        \n",
    "print(len(new_tokens))\n",
    "print(len(new_labels))\n",
    "print(len(new_tokens_test))\n",
    "print(len(new_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  507\n",
      "Min length:  4\n"
     ]
    }
   ],
   "source": [
    "# for tetst data\n",
    "# find length of longest sentence\n",
    "max_len_test = 0\n",
    "for i in range(len(tokens_test)):\n",
    "    if len(tokens_test[i]) > max_len_test:\n",
    "        max_len_test = len(tokens_test[i])\n",
    "        \n",
    "# find length of shortest sentence\n",
    "min_len_test = 1000000\n",
    "for i in range(len(tokens_test)):\n",
    "    if len(tokens_test[i]) < min_len_test:\n",
    "        min_len_test = len(tokens_test[i])\n",
    "        \n",
    "print(\"Max length: \", max_len_test)\n",
    "print(\"Min length: \", min_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding to all sentences and S and EOS tokens\n",
    "def padding(tokens, max_len):\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = ['S'] + tokens[i] + ['EOS']\n",
    "        while len(tokens[i]) < max_len:\n",
    "            tokens[i].append('PAD')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 102\n",
    "tokens = padding(new_tokens, max_len)\n",
    "tokens_test = padding(new_tokens_test, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pritn the 10 tokens and their labels\n",
    "# for i in range(100000):\n",
    "#     # print(new_tokens[i])\n",
    "#     # print(len(new_tokens[i]))\n",
    "#     # print(new_labels[i])\n",
    "#     if len(tokens[i]) != max_len:\n",
    "#         print(tokens[i])\n",
    "#         print(len(tokens[i]))\n",
    "#         # print(new_labels[i])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248156\n",
      "politicianowen\n",
      "105105\n",
      "221468\n",
      "600419\n",
      "578505\n",
      "163757\n",
      "515158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for token in tokens:\n",
    "    vocab.update(token)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(word_to_idx['the'])\n",
    "print(idx_to_word[0])\n",
    "print(word_to_idx['NUM'])\n",
    "print(word_to_idx['UNK'])\n",
    "print(word_to_idx['PAD'])\n",
    "print(word_to_idx['S'])\n",
    "print(word_to_idx['EOS'])\n",
    "print(word_to_idx['MID'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 14\n",
    "vocab_size = len(vocab)\n",
    "num_epochs = 3\n",
    "learning_rate = 0.001\n",
    "embedding_size = 300\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DBpediaDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        # handle unk\n",
    "        for i in range(len(self.tokens)):\n",
    "            for j in range(len(self.tokens[i])):\n",
    "                if self.tokens[i][j] not in word_to_idx:\n",
    "                    self.tokens[i][j] = 'UNK'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in self.tokens[idx]]\n",
    "        return torch.tensor(token_indices), torch.tensor(self.labels[idx])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test datasets\n",
    "train_dataset = DBpediaDataset(tokens, new_labels)\n",
    "test_dataset = DBpediaDataset(tokens_test, new_labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LSTM(vocab_size, embedding_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, train_loader, num_epochs, criterion, optimizer):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    model.train()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in notebook_tqdm.tqdm(range(num_epochs)):\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for i, (tokens, labels) in enumerate(train_loader):\n",
    "            tokens = tokens.long().to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            outputs = model(tokens)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            \n",
    "            optimizer.step()\n",
    "            # calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            if (i+1) % 1 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                      .format(epoch+1, num_epochs, i+1, total_step, loss.item(), (correct_predictions/total_predictions)*100))\n",
    "        \n",
    "        # Save the model after each epoch\n",
    "        torch.save(model.state_dict(), f\"dbpedia_pytorch_model_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/17463], Loss: 2.6503, Accuracy: 0.00%\n",
      "Epoch [1/10], Step [2/17463], Loss: 2.6294, Accuracy: 3.12%\n",
      "Epoch [1/10], Step [3/17463], Loss: 2.6711, Accuracy: 3.12%\n",
      "Epoch [1/10], Step [4/17463], Loss: 2.6771, Accuracy: 3.91%\n",
      "Epoch [1/10], Step [5/17463], Loss: 2.6670, Accuracy: 3.75%\n",
      "Epoch [1/10], Step [6/17463], Loss: 2.6810, Accuracy: 4.69%\n",
      "Epoch [1/10], Step [7/17463], Loss: 2.6631, Accuracy: 4.46%\n",
      "Epoch [1/10], Step [8/17463], Loss: 2.6283, Accuracy: 5.08%\n",
      "Epoch [1/10], Step [9/17463], Loss: 2.6193, Accuracy: 5.90%\n",
      "Epoch [1/10], Step [10/17463], Loss: 2.6676, Accuracy: 5.31%\n",
      "Epoch [1/10], Step [11/17463], Loss: 2.6503, Accuracy: 5.40%\n",
      "Epoch [1/10], Step [12/17463], Loss: 2.6666, Accuracy: 6.25%\n",
      "Epoch [1/10], Step [13/17463], Loss: 2.6628, Accuracy: 6.01%\n",
      "Epoch [1/10], Step [14/17463], Loss: 2.6419, Accuracy: 6.25%\n",
      "Epoch [1/10], Step [15/17463], Loss: 2.6426, Accuracy: 6.46%\n",
      "Epoch [1/10], Step [16/17463], Loss: 2.6621, Accuracy: 6.45%\n",
      "Epoch [1/10], Step [17/17463], Loss: 2.6382, Accuracy: 6.80%\n",
      "Epoch [1/10], Step [18/17463], Loss: 2.6331, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [19/17463], Loss: 2.6511, Accuracy: 6.58%\n",
      "Epoch [1/10], Step [20/17463], Loss: 2.6528, Accuracy: 6.56%\n",
      "Epoch [1/10], Step [21/17463], Loss: 2.5956, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [22/17463], Loss: 2.6530, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [23/17463], Loss: 2.7173, Accuracy: 6.79%\n",
      "Epoch [1/10], Step [24/17463], Loss: 2.6438, Accuracy: 6.77%\n",
      "Epoch [1/10], Step [25/17463], Loss: 2.7084, Accuracy: 6.50%\n",
      "Epoch [1/10], Step [26/17463], Loss: 2.6526, Accuracy: 6.37%\n",
      "Epoch [1/10], Step [27/17463], Loss: 2.6225, Accuracy: 6.13%\n",
      "Epoch [1/10], Step [28/17463], Loss: 2.6834, Accuracy: 6.36%\n",
      "Epoch [1/10], Step [29/17463], Loss: 2.7318, Accuracy: 6.25%\n",
      "Epoch [1/10], Step [30/17463], Loss: 2.6300, Accuracy: 6.25%\n",
      "Epoch [1/10], Step [31/17463], Loss: 2.6847, Accuracy: 6.25%\n",
      "Epoch [1/10], Step [32/17463], Loss: 2.6704, Accuracy: 6.25%\n",
      "Epoch [1/10], Step [33/17463], Loss: 2.6922, Accuracy: 6.16%\n",
      "Epoch [1/10], Step [34/17463], Loss: 2.6003, Accuracy: 6.53%\n",
      "Epoch [1/10], Step [35/17463], Loss: 2.6353, Accuracy: 6.70%\n",
      "Epoch [1/10], Step [36/17463], Loss: 2.6060, Accuracy: 7.12%\n",
      "Epoch [1/10], Step [37/17463], Loss: 2.5997, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [38/17463], Loss: 2.6663, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [39/17463], Loss: 2.6617, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [40/17463], Loss: 2.6539, Accuracy: 7.19%\n",
      "Epoch [1/10], Step [41/17463], Loss: 2.6178, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [42/17463], Loss: 2.6521, Accuracy: 7.14%\n",
      "Epoch [1/10], Step [43/17463], Loss: 2.6493, Accuracy: 7.12%\n",
      "Epoch [1/10], Step [44/17463], Loss: 2.6718, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [45/17463], Loss: 2.6466, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [46/17463], Loss: 2.6217, Accuracy: 7.47%\n",
      "Epoch [1/10], Step [47/17463], Loss: 2.6754, Accuracy: 7.38%\n",
      "Epoch [1/10], Step [48/17463], Loss: 2.6688, Accuracy: 7.42%\n",
      "Epoch [1/10], Step [49/17463], Loss: 2.6521, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [50/17463], Loss: 2.6448, Accuracy: 7.38%\n",
      "Epoch [1/10], Step [51/17463], Loss: 2.6494, Accuracy: 7.41%\n",
      "Epoch [1/10], Step [52/17463], Loss: 2.6559, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [53/17463], Loss: 2.6557, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [54/17463], Loss: 2.6196, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [55/17463], Loss: 2.6041, Accuracy: 7.50%\n",
      "Epoch [1/10], Step [56/17463], Loss: 2.6476, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [57/17463], Loss: 2.6543, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [58/17463], Loss: 2.6700, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [59/17463], Loss: 2.6481, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [60/17463], Loss: 2.6886, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [61/17463], Loss: 2.6776, Accuracy: 7.12%\n",
      "Epoch [1/10], Step [62/17463], Loss: 2.6594, Accuracy: 7.06%\n",
      "Epoch [1/10], Step [63/17463], Loss: 2.6629, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [64/17463], Loss: 2.6743, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [65/17463], Loss: 2.6327, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [66/17463], Loss: 2.6716, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [67/17463], Loss: 2.6945, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [68/17463], Loss: 2.6751, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [69/17463], Loss: 2.6270, Accuracy: 7.11%\n",
      "Epoch [1/10], Step [70/17463], Loss: 2.6395, Accuracy: 7.05%\n",
      "Epoch [1/10], Step [71/17463], Loss: 2.6003, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [72/17463], Loss: 2.6587, Accuracy: 7.20%\n",
      "Epoch [1/10], Step [73/17463], Loss: 2.6353, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [74/17463], Loss: 2.6893, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [75/17463], Loss: 2.5917, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [76/17463], Loss: 2.6467, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [77/17463], Loss: 2.6280, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [78/17463], Loss: 2.6462, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [79/17463], Loss: 2.6360, Accuracy: 7.44%\n",
      "Epoch [1/10], Step [80/17463], Loss: 2.6214, Accuracy: 7.50%\n",
      "Epoch [1/10], Step [81/17463], Loss: 2.6324, Accuracy: 7.52%\n",
      "Epoch [1/10], Step [82/17463], Loss: 2.6785, Accuracy: 7.51%\n",
      "Epoch [1/10], Step [83/17463], Loss: 2.6023, Accuracy: 7.45%\n",
      "Epoch [1/10], Step [84/17463], Loss: 2.6356, Accuracy: 7.44%\n",
      "Epoch [1/10], Step [85/17463], Loss: 2.6714, Accuracy: 7.43%\n",
      "Epoch [1/10], Step [86/17463], Loss: 2.6302, Accuracy: 7.56%\n",
      "Epoch [1/10], Step [87/17463], Loss: 2.7051, Accuracy: 7.58%\n",
      "Epoch [1/10], Step [88/17463], Loss: 2.6178, Accuracy: 7.63%\n",
      "Epoch [1/10], Step [89/17463], Loss: 2.6276, Accuracy: 7.58%\n",
      "Epoch [1/10], Step [90/17463], Loss: 2.6623, Accuracy: 7.60%\n",
      "Epoch [1/10], Step [91/17463], Loss: 2.6413, Accuracy: 7.52%\n",
      "Epoch [1/10], Step [92/17463], Loss: 2.6697, Accuracy: 7.51%\n",
      "Epoch [1/10], Step [93/17463], Loss: 2.6530, Accuracy: 7.56%\n",
      "Epoch [1/10], Step [94/17463], Loss: 2.6955, Accuracy: 7.55%\n",
      "Epoch [1/10], Step [95/17463], Loss: 2.6454, Accuracy: 7.53%\n",
      "Epoch [1/10], Step [96/17463], Loss: 2.6284, Accuracy: 7.62%\n",
      "Epoch [1/10], Step [97/17463], Loss: 2.6696, Accuracy: 7.57%\n",
      "Epoch [1/10], Step [98/17463], Loss: 2.6751, Accuracy: 7.49%\n",
      "Epoch [1/10], Step [99/17463], Loss: 2.6256, Accuracy: 7.51%\n",
      "Epoch [1/10], Step [100/17463], Loss: 2.6428, Accuracy: 7.50%\n",
      "Epoch [1/10], Step [101/17463], Loss: 2.6584, Accuracy: 7.43%\n",
      "Epoch [1/10], Step [102/17463], Loss: 2.6409, Accuracy: 7.38%\n",
      "Epoch [1/10], Step [103/17463], Loss: 2.6462, Accuracy: 7.40%\n",
      "Epoch [1/10], Step [104/17463], Loss: 2.6235, Accuracy: 7.39%\n",
      "Epoch [1/10], Step [105/17463], Loss: 2.6128, Accuracy: 7.44%\n",
      "Epoch [1/10], Step [106/17463], Loss: 2.7072, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [107/17463], Loss: 2.6870, Accuracy: 7.39%\n",
      "Epoch [1/10], Step [108/17463], Loss: 2.6329, Accuracy: 7.41%\n",
      "Epoch [1/10], Step [109/17463], Loss: 2.6314, Accuracy: 7.40%\n",
      "Epoch [1/10], Step [110/17463], Loss: 2.6565, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [111/17463], Loss: 2.6505, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [112/17463], Loss: 2.6717, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [113/17463], Loss: 2.6915, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [114/17463], Loss: 2.6413, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [115/17463], Loss: 2.6311, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [116/17463], Loss: 2.6433, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [117/17463], Loss: 2.6577, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [118/17463], Loss: 2.6308, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [119/17463], Loss: 2.6393, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [120/17463], Loss: 2.6890, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [121/17463], Loss: 2.6088, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [122/17463], Loss: 2.6478, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [123/17463], Loss: 2.6645, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [124/17463], Loss: 2.6513, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [125/17463], Loss: 2.6206, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [126/17463], Loss: 2.6470, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [127/17463], Loss: 2.6296, Accuracy: 7.38%\n",
      "Epoch [1/10], Step [128/17463], Loss: 2.6435, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [129/17463], Loss: 2.6596, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [130/17463], Loss: 2.6756, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [131/17463], Loss: 2.6535, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [132/17463], Loss: 2.6684, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [133/17463], Loss: 2.6596, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [134/17463], Loss: 2.6621, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [135/17463], Loss: 2.6352, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [136/17463], Loss: 2.6270, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [137/17463], Loss: 2.6392, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [138/17463], Loss: 2.6645, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [139/17463], Loss: 2.6228, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [140/17463], Loss: 2.6315, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [141/17463], Loss: 2.6508, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [142/17463], Loss: 2.6423, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [143/17463], Loss: 2.6614, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [144/17463], Loss: 2.6171, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [145/17463], Loss: 2.6596, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [146/17463], Loss: 2.6697, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [147/17463], Loss: 2.6779, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [148/17463], Loss: 2.6623, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [149/17463], Loss: 2.6387, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [150/17463], Loss: 2.6252, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [151/17463], Loss: 2.6385, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [152/17463], Loss: 2.6119, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [153/17463], Loss: 2.6405, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [154/17463], Loss: 2.6563, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [155/17463], Loss: 2.6268, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [156/17463], Loss: 2.6212, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [157/17463], Loss: 2.6308, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [158/17463], Loss: 2.6506, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [159/17463], Loss: 2.6633, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [160/17463], Loss: 2.6126, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [161/17463], Loss: 2.6334, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [162/17463], Loss: 2.6803, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [163/17463], Loss: 2.6441, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [164/17463], Loss: 2.6437, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [165/17463], Loss: 2.6562, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [166/17463], Loss: 2.6485, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [167/17463], Loss: 2.6301, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [168/17463], Loss: 2.6263, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [169/17463], Loss: 2.6330, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [170/17463], Loss: 2.6283, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [171/17463], Loss: 2.6472, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [172/17463], Loss: 2.6367, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [173/17463], Loss: 2.6518, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [174/17463], Loss: 2.6214, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [175/17463], Loss: 2.6350, Accuracy: 7.39%\n",
      "Epoch [1/10], Step [176/17463], Loss: 2.6636, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [177/17463], Loss: 2.6075, Accuracy: 7.40%\n",
      "Epoch [1/10], Step [178/17463], Loss: 2.6668, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [179/17463], Loss: 2.7101, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [180/17463], Loss: 2.6428, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [181/17463], Loss: 2.6415, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [182/17463], Loss: 2.6412, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [183/17463], Loss: 2.6458, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [184/17463], Loss: 2.6272, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [185/17463], Loss: 2.6276, Accuracy: 7.38%\n",
      "Epoch [1/10], Step [186/17463], Loss: 2.6503, Accuracy: 7.38%\n",
      "Epoch [1/10], Step [187/17463], Loss: 2.6501, Accuracy: 7.39%\n",
      "Epoch [1/10], Step [188/17463], Loss: 2.6350, Accuracy: 7.38%\n",
      "Epoch [1/10], Step [189/17463], Loss: 2.6347, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [190/17463], Loss: 2.6589, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [191/17463], Loss: 2.6535, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [192/17463], Loss: 2.6478, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [193/17463], Loss: 2.6665, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [194/17463], Loss: 2.6407, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [195/17463], Loss: 2.6326, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [196/17463], Loss: 2.6274, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [197/17463], Loss: 2.6468, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [198/17463], Loss: 2.6246, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [199/17463], Loss: 2.6055, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [200/17463], Loss: 2.6475, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [201/17463], Loss: 2.6251, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [202/17463], Loss: 2.6185, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [203/17463], Loss: 2.6594, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [204/17463], Loss: 2.6611, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [205/17463], Loss: 2.6459, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [206/17463], Loss: 2.6348, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [207/17463], Loss: 2.6323, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [208/17463], Loss: 2.6429, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [209/17463], Loss: 2.6301, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [210/17463], Loss: 2.6488, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [211/17463], Loss: 2.6398, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [212/17463], Loss: 2.6407, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [213/17463], Loss: 2.6121, Accuracy: 7.42%\n",
      "Epoch [1/10], Step [214/17463], Loss: 2.6367, Accuracy: 7.39%\n",
      "Epoch [1/10], Step [215/17463], Loss: 2.6274, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [216/17463], Loss: 2.6472, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [217/17463], Loss: 2.6484, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [218/17463], Loss: 2.6333, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [219/17463], Loss: 2.6489, Accuracy: 7.41%\n",
      "Epoch [1/10], Step [220/17463], Loss: 2.6454, Accuracy: 7.40%\n",
      "Epoch [1/10], Step [221/17463], Loss: 2.6322, Accuracy: 7.45%\n",
      "Epoch [1/10], Step [222/17463], Loss: 2.6465, Accuracy: 7.42%\n",
      "Epoch [1/10], Step [223/17463], Loss: 2.6416, Accuracy: 7.43%\n",
      "Epoch [1/10], Step [224/17463], Loss: 2.6637, Accuracy: 7.41%\n",
      "Epoch [1/10], Step [225/17463], Loss: 2.6505, Accuracy: 7.39%\n",
      "Epoch [1/10], Step [226/17463], Loss: 2.6321, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [227/17463], Loss: 2.6640, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [228/17463], Loss: 2.6428, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [229/17463], Loss: 2.6323, Accuracy: 7.37%\n",
      "Epoch [1/10], Step [230/17463], Loss: 2.6536, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [231/17463], Loss: 2.6554, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [232/17463], Loss: 2.6474, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [233/17463], Loss: 2.6459, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [234/17463], Loss: 2.6347, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [235/17463], Loss: 2.6728, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [236/17463], Loss: 2.6545, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [237/17463], Loss: 2.6517, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [238/17463], Loss: 2.6452, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [239/17463], Loss: 2.6222, Accuracy: 7.36%\n",
      "Epoch [1/10], Step [240/17463], Loss: 2.6347, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [241/17463], Loss: 2.6407, Accuracy: 7.35%\n",
      "Epoch [1/10], Step [242/17463], Loss: 2.6715, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [243/17463], Loss: 2.6292, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [244/17463], Loss: 2.6375, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [245/17463], Loss: 2.6676, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [246/17463], Loss: 2.6420, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [247/17463], Loss: 2.6336, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [248/17463], Loss: 2.6528, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [249/17463], Loss: 2.6299, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [250/17463], Loss: 2.6427, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [251/17463], Loss: 2.6371, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [252/17463], Loss: 2.6496, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [253/17463], Loss: 2.6410, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [254/17463], Loss: 2.6500, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [255/17463], Loss: 2.6356, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [256/17463], Loss: 2.6434, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [257/17463], Loss: 2.6402, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [258/17463], Loss: 2.6450, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [259/17463], Loss: 2.6170, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [260/17463], Loss: 2.6275, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [261/17463], Loss: 2.6099, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [262/17463], Loss: 2.6406, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [263/17463], Loss: 2.6364, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [264/17463], Loss: 2.6381, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [265/17463], Loss: 2.6354, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [266/17463], Loss: 2.6413, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [267/17463], Loss: 2.6401, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [268/17463], Loss: 2.6441, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [269/17463], Loss: 2.6314, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [270/17463], Loss: 2.6267, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [271/17463], Loss: 2.6307, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [272/17463], Loss: 2.6718, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [273/17463], Loss: 2.6525, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [274/17463], Loss: 2.6297, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [275/17463], Loss: 2.6410, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [276/17463], Loss: 2.6456, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [277/17463], Loss: 2.6277, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [278/17463], Loss: 2.6395, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [279/17463], Loss: 2.6296, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [280/17463], Loss: 2.6463, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [281/17463], Loss: 2.6449, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [282/17463], Loss: 2.6588, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [283/17463], Loss: 2.6520, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [284/17463], Loss: 2.6450, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [285/17463], Loss: 2.6457, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [286/17463], Loss: 2.6371, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [287/17463], Loss: 2.6531, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [288/17463], Loss: 2.6374, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [289/17463], Loss: 2.6335, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [290/17463], Loss: 2.6508, Accuracy: 7.30%\n",
      "Epoch [1/10], Step [291/17463], Loss: 2.6416, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [292/17463], Loss: 2.6313, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [293/17463], Loss: 2.6634, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [294/17463], Loss: 2.6603, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [295/17463], Loss: 2.6381, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [296/17463], Loss: 2.6433, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [297/17463], Loss: 2.6309, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [298/17463], Loss: 2.6511, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [299/17463], Loss: 2.6400, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [300/17463], Loss: 2.6465, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [301/17463], Loss: 2.6328, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [302/17463], Loss: 2.6318, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [303/17463], Loss: 2.6337, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [304/17463], Loss: 2.6230, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [305/17463], Loss: 2.6465, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [306/17463], Loss: 2.6283, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [307/17463], Loss: 2.6334, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [308/17463], Loss: 2.6382, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [309/17463], Loss: 2.6371, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [310/17463], Loss: 2.6651, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [311/17463], Loss: 2.6413, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [312/17463], Loss: 2.6414, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [313/17463], Loss: 2.6324, Accuracy: 7.34%\n",
      "Epoch [1/10], Step [314/17463], Loss: 2.6549, Accuracy: 7.33%\n",
      "Epoch [1/10], Step [315/17463], Loss: 2.6678, Accuracy: 7.32%\n",
      "Epoch [1/10], Step [316/17463], Loss: 2.6463, Accuracy: 7.31%\n",
      "Epoch [1/10], Step [317/17463], Loss: 2.6438, Accuracy: 7.29%\n",
      "Epoch [1/10], Step [318/17463], Loss: 2.6496, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [319/17463], Loss: 2.6539, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [320/17463], Loss: 2.6400, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [321/17463], Loss: 2.6382, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [322/17463], Loss: 2.6358, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [323/17463], Loss: 2.6543, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [324/17463], Loss: 2.6512, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [325/17463], Loss: 2.6366, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [326/17463], Loss: 2.6594, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [327/17463], Loss: 2.6332, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [328/17463], Loss: 2.6462, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [329/17463], Loss: 2.6433, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [330/17463], Loss: 2.6404, Accuracy: 7.20%\n",
      "Epoch [1/10], Step [331/17463], Loss: 2.6543, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [332/17463], Loss: 2.6523, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [333/17463], Loss: 2.6417, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [334/17463], Loss: 2.6485, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [335/17463], Loss: 2.6269, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [336/17463], Loss: 2.6424, Accuracy: 7.14%\n",
      "Epoch [1/10], Step [337/17463], Loss: 2.6412, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [338/17463], Loss: 2.6564, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [339/17463], Loss: 2.6326, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [340/17463], Loss: 2.6454, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [341/17463], Loss: 2.6362, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [342/17463], Loss: 2.6505, Accuracy: 7.14%\n",
      "Epoch [1/10], Step [343/17463], Loss: 2.6413, Accuracy: 7.12%\n",
      "Epoch [1/10], Step [344/17463], Loss: 2.6279, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [345/17463], Loss: 2.6457, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [346/17463], Loss: 2.6282, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [347/17463], Loss: 2.6443, Accuracy: 7.11%\n",
      "Epoch [1/10], Step [348/17463], Loss: 2.6398, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [349/17463], Loss: 2.6630, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [350/17463], Loss: 2.6382, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [351/17463], Loss: 2.6374, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [352/17463], Loss: 2.6520, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [353/17463], Loss: 2.6239, Accuracy: 7.11%\n",
      "Epoch [1/10], Step [354/17463], Loss: 2.6159, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [355/17463], Loss: 2.6406, Accuracy: 7.12%\n",
      "Epoch [1/10], Step [356/17463], Loss: 2.6355, Accuracy: 7.14%\n",
      "Epoch [1/10], Step [357/17463], Loss: 2.6373, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [358/17463], Loss: 2.6398, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [359/17463], Loss: 2.6366, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [360/17463], Loss: 2.6461, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [361/17463], Loss: 2.6446, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [362/17463], Loss: 2.6315, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [363/17463], Loss: 2.6421, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [364/17463], Loss: 2.6352, Accuracy: 7.14%\n",
      "Epoch [1/10], Step [365/17463], Loss: 2.6477, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [366/17463], Loss: 2.6243, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [367/17463], Loss: 2.6401, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [368/17463], Loss: 2.6266, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [369/17463], Loss: 2.6447, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [370/17463], Loss: 2.6366, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [371/17463], Loss: 2.6422, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [372/17463], Loss: 2.6361, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [373/17463], Loss: 2.6300, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [374/17463], Loss: 2.6402, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [375/17463], Loss: 2.6540, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [376/17463], Loss: 2.6411, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [377/17463], Loss: 2.6339, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [378/17463], Loss: 2.6502, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [379/17463], Loss: 2.6298, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [380/17463], Loss: 2.6199, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [381/17463], Loss: 2.6432, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [382/17463], Loss: 2.6442, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [383/17463], Loss: 2.6474, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [384/17463], Loss: 2.6401, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [385/17463], Loss: 2.6325, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [386/17463], Loss: 2.6373, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [387/17463], Loss: 2.6367, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [388/17463], Loss: 2.6379, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [389/17463], Loss: 2.6246, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [390/17463], Loss: 2.6504, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [391/17463], Loss: 2.6494, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [392/17463], Loss: 2.6392, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [393/17463], Loss: 2.6293, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [394/17463], Loss: 2.6555, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [395/17463], Loss: 2.6262, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [396/17463], Loss: 2.6556, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [397/17463], Loss: 2.6530, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [398/17463], Loss: 2.6258, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [399/17463], Loss: 2.6272, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [400/17463], Loss: 2.6515, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [401/17463], Loss: 2.6352, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [402/17463], Loss: 2.6375, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [403/17463], Loss: 2.6482, Accuracy: 7.20%\n",
      "Epoch [1/10], Step [404/17463], Loss: 2.6398, Accuracy: 7.19%\n",
      "Epoch [1/10], Step [405/17463], Loss: 2.6640, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [406/17463], Loss: 2.6361, Accuracy: 7.19%\n",
      "Epoch [1/10], Step [407/17463], Loss: 2.6395, Accuracy: 7.19%\n",
      "Epoch [1/10], Step [408/17463], Loss: 2.6362, Accuracy: 7.20%\n",
      "Epoch [1/10], Step [409/17463], Loss: 2.6298, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [410/17463], Loss: 2.6306, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [411/17463], Loss: 2.6431, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [412/17463], Loss: 2.6486, Accuracy: 7.19%\n",
      "Epoch [1/10], Step [413/17463], Loss: 2.6364, Accuracy: 7.20%\n",
      "Epoch [1/10], Step [414/17463], Loss: 2.6294, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [415/17463], Loss: 2.6240, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [416/17463], Loss: 2.6398, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [417/17463], Loss: 2.6434, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [418/17463], Loss: 2.6375, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [419/17463], Loss: 2.6408, Accuracy: 7.20%\n",
      "Epoch [1/10], Step [420/17463], Loss: 2.6454, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [421/17463], Loss: 2.6523, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [422/17463], Loss: 2.6430, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [423/17463], Loss: 2.6389, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [424/17463], Loss: 2.6357, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [425/17463], Loss: 2.6499, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [426/17463], Loss: 2.6306, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [427/17463], Loss: 2.6362, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [428/17463], Loss: 2.6363, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [429/17463], Loss: 2.6397, Accuracy: 7.26%\n",
      "Epoch [1/10], Step [430/17463], Loss: 2.6386, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [431/17463], Loss: 2.6274, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [432/17463], Loss: 2.6419, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [433/17463], Loss: 2.6537, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [434/17463], Loss: 2.6366, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [435/17463], Loss: 2.6270, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [436/17463], Loss: 2.6466, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [437/17463], Loss: 2.6274, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [438/17463], Loss: 2.6354, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [439/17463], Loss: 2.6401, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [440/17463], Loss: 2.6520, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [441/17463], Loss: 2.6284, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [442/17463], Loss: 2.6439, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [443/17463], Loss: 2.6356, Accuracy: 7.28%\n",
      "Epoch [1/10], Step [444/17463], Loss: 2.6436, Accuracy: 7.27%\n",
      "Epoch [1/10], Step [445/17463], Loss: 2.6535, Accuracy: 7.25%\n",
      "Epoch [1/10], Step [446/17463], Loss: 2.6379, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [447/17463], Loss: 2.6461, Accuracy: 7.24%\n",
      "Epoch [1/10], Step [448/17463], Loss: 2.6373, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [449/17463], Loss: 2.6386, Accuracy: 7.23%\n",
      "Epoch [1/10], Step [450/17463], Loss: 2.6512, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [451/17463], Loss: 2.6358, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [452/17463], Loss: 2.6383, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [453/17463], Loss: 2.6459, Accuracy: 7.22%\n",
      "Epoch [1/10], Step [454/17463], Loss: 2.6439, Accuracy: 7.21%\n",
      "Epoch [1/10], Step [455/17463], Loss: 2.6432, Accuracy: 7.20%\n",
      "Epoch [1/10], Step [456/17463], Loss: 2.6426, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [457/17463], Loss: 2.6462, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [458/17463], Loss: 2.6402, Accuracy: 7.18%\n",
      "Epoch [1/10], Step [459/17463], Loss: 2.6367, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [460/17463], Loss: 2.6404, Accuracy: 7.17%\n",
      "Epoch [1/10], Step [461/17463], Loss: 2.6451, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [462/17463], Loss: 2.6473, Accuracy: 7.16%\n",
      "Epoch [1/10], Step [463/17463], Loss: 2.6433, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [464/17463], Loss: 2.6431, Accuracy: 7.15%\n",
      "Epoch [1/10], Step [465/17463], Loss: 2.6466, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [466/17463], Loss: 2.6446, Accuracy: 7.13%\n",
      "Epoch [1/10], Step [467/17463], Loss: 2.6533, Accuracy: 7.12%\n",
      "Epoch [1/10], Step [468/17463], Loss: 2.6426, Accuracy: 7.11%\n",
      "Epoch [1/10], Step [469/17463], Loss: 2.6332, Accuracy: 7.12%\n",
      "Epoch [1/10], Step [470/17463], Loss: 2.6533, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [471/17463], Loss: 2.6322, Accuracy: 7.11%\n",
      "Epoch [1/10], Step [472/17463], Loss: 2.6478, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [473/17463], Loss: 2.6451, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [474/17463], Loss: 2.6487, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [475/17463], Loss: 2.6478, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [476/17463], Loss: 2.6264, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [477/17463], Loss: 2.6337, Accuracy: 7.10%\n",
      "Epoch [1/10], Step [478/17463], Loss: 2.6439, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [479/17463], Loss: 2.6427, Accuracy: 7.09%\n",
      "Epoch [1/10], Step [480/17463], Loss: 2.6319, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [481/17463], Loss: 2.6493, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [482/17463], Loss: 2.6366, Accuracy: 7.08%\n",
      "Epoch [1/10], Step [483/17463], Loss: 2.6498, Accuracy: 7.07%\n",
      "Epoch [1/10], Step [484/17463], Loss: 2.6366, Accuracy: 7.07%\n",
      "Epoch [1/10], Step [485/17463], Loss: 2.6395, Accuracy: 7.07%\n",
      "Epoch [1/10], Step [486/17463], Loss: 2.6407, Accuracy: 7.07%\n",
      "Epoch [1/10], Step [487/17463], Loss: 2.6474, Accuracy: 7.06%\n",
      "Epoch [1/10], Step [488/17463], Loss: 2.6443, Accuracy: 7.05%\n",
      "Epoch [1/10], Step [489/17463], Loss: 2.6445, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [490/17463], Loss: 2.6405, Accuracy: 7.05%\n",
      "Epoch [1/10], Step [491/17463], Loss: 2.6367, Accuracy: 7.06%\n",
      "Epoch [1/10], Step [492/17463], Loss: 2.6349, Accuracy: 7.06%\n",
      "Epoch [1/10], Step [493/17463], Loss: 2.6375, Accuracy: 7.06%\n",
      "Epoch [1/10], Step [494/17463], Loss: 2.6438, Accuracy: 7.05%\n",
      "Epoch [1/10], Step [495/17463], Loss: 2.6337, Accuracy: 7.06%\n",
      "Epoch [1/10], Step [496/17463], Loss: 2.6470, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [497/17463], Loss: 2.6459, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [498/17463], Loss: 2.6462, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [499/17463], Loss: 2.6359, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [500/17463], Loss: 2.6445, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [501/17463], Loss: 2.6487, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [502/17463], Loss: 2.6502, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [503/17463], Loss: 2.6449, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [504/17463], Loss: 2.6338, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [505/17463], Loss: 2.6505, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [506/17463], Loss: 2.6334, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [507/17463], Loss: 2.6383, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [508/17463], Loss: 2.6370, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [509/17463], Loss: 2.6370, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [510/17463], Loss: 2.6453, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [511/17463], Loss: 2.6461, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [512/17463], Loss: 2.6396, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [513/17463], Loss: 2.6464, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [514/17463], Loss: 2.6327, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [515/17463], Loss: 2.6388, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [516/17463], Loss: 2.6331, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [517/17463], Loss: 2.6347, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [518/17463], Loss: 2.6376, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [519/17463], Loss: 2.6358, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [520/17463], Loss: 2.6435, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [521/17463], Loss: 2.6441, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [522/17463], Loss: 2.6381, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [523/17463], Loss: 2.6331, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [524/17463], Loss: 2.6387, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [525/17463], Loss: 2.6491, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [526/17463], Loss: 2.6452, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [527/17463], Loss: 2.6412, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [528/17463], Loss: 2.6380, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [529/17463], Loss: 2.6395, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [530/17463], Loss: 2.6401, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [531/17463], Loss: 2.6455, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [532/17463], Loss: 2.6364, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [533/17463], Loss: 2.6373, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [534/17463], Loss: 2.6318, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [535/17463], Loss: 2.6480, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [536/17463], Loss: 2.6355, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [537/17463], Loss: 2.6378, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [538/17463], Loss: 2.6333, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [539/17463], Loss: 2.6416, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [540/17463], Loss: 2.6389, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [541/17463], Loss: 2.6342, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [542/17463], Loss: 2.6405, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [543/17463], Loss: 2.6356, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [544/17463], Loss: 2.6362, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [545/17463], Loss: 2.6373, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [546/17463], Loss: 2.6382, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [547/17463], Loss: 2.6283, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [548/17463], Loss: 2.6434, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [549/17463], Loss: 2.6352, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [550/17463], Loss: 2.6362, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [551/17463], Loss: 2.6380, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [552/17463], Loss: 2.6395, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [553/17463], Loss: 2.6365, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [554/17463], Loss: 2.6347, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [555/17463], Loss: 2.6344, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [556/17463], Loss: 2.6399, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [557/17463], Loss: 2.6317, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [558/17463], Loss: 2.6474, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [559/17463], Loss: 2.6380, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [560/17463], Loss: 2.6555, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [561/17463], Loss: 2.6487, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [562/17463], Loss: 2.6325, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [563/17463], Loss: 2.6366, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [564/17463], Loss: 2.6325, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [565/17463], Loss: 2.6302, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [566/17463], Loss: 2.6363, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [567/17463], Loss: 2.6370, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [568/17463], Loss: 2.6536, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [569/17463], Loss: 2.6347, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [570/17463], Loss: 2.6467, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [571/17463], Loss: 2.6420, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [572/17463], Loss: 2.6362, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [573/17463], Loss: 2.6351, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [574/17463], Loss: 2.6391, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [575/17463], Loss: 2.6353, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [576/17463], Loss: 2.6435, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [577/17463], Loss: 2.6428, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [578/17463], Loss: 2.6434, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [579/17463], Loss: 2.6487, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [580/17463], Loss: 2.6292, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [581/17463], Loss: 2.6343, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [582/17463], Loss: 2.6362, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [583/17463], Loss: 2.6377, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [584/17463], Loss: 2.6386, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [585/17463], Loss: 2.6422, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [586/17463], Loss: 2.6414, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [587/17463], Loss: 2.6396, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [588/17463], Loss: 2.6432, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [589/17463], Loss: 2.6385, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [590/17463], Loss: 2.6442, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [591/17463], Loss: 2.6414, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [592/17463], Loss: 2.6383, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [593/17463], Loss: 2.6425, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [594/17463], Loss: 2.6466, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [595/17463], Loss: 2.6314, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [596/17463], Loss: 2.6408, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [597/17463], Loss: 2.6417, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [598/17463], Loss: 2.6328, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [599/17463], Loss: 2.6354, Accuracy: 6.83%\n",
      "Epoch [1/10], Step [600/17463], Loss: 2.6446, Accuracy: 6.83%\n",
      "Epoch [1/10], Step [601/17463], Loss: 2.6557, Accuracy: 6.83%\n",
      "Epoch [1/10], Step [602/17463], Loss: 2.6456, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [603/17463], Loss: 2.6393, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [604/17463], Loss: 2.6364, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [605/17463], Loss: 2.6330, Accuracy: 6.83%\n",
      "Epoch [1/10], Step [606/17463], Loss: 2.6385, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [607/17463], Loss: 2.6394, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [608/17463], Loss: 2.6426, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [609/17463], Loss: 2.6408, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [610/17463], Loss: 2.6394, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [611/17463], Loss: 2.6414, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [612/17463], Loss: 2.6411, Accuracy: 6.82%\n",
      "Epoch [1/10], Step [613/17463], Loss: 2.6405, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [614/17463], Loss: 2.6452, Accuracy: 6.80%\n",
      "Epoch [1/10], Step [615/17463], Loss: 2.6327, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [616/17463], Loss: 2.6300, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [617/17463], Loss: 2.6429, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [618/17463], Loss: 2.6369, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [619/17463], Loss: 2.6284, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [620/17463], Loss: 2.6389, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [621/17463], Loss: 2.6406, Accuracy: 6.81%\n",
      "Epoch [1/10], Step [622/17463], Loss: 2.6345, Accuracy: 6.83%\n",
      "Epoch [1/10], Step [623/17463], Loss: 2.6280, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [624/17463], Loss: 2.6285, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [625/17463], Loss: 2.6340, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [626/17463], Loss: 2.6359, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [627/17463], Loss: 2.6381, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [628/17463], Loss: 2.6456, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [629/17463], Loss: 2.6337, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [630/17463], Loss: 2.6349, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [631/17463], Loss: 2.6322, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [632/17463], Loss: 2.6259, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [633/17463], Loss: 2.6422, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [634/17463], Loss: 2.6415, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [635/17463], Loss: 2.6438, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [636/17463], Loss: 2.6465, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [637/17463], Loss: 2.6395, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [638/17463], Loss: 2.6455, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [639/17463], Loss: 2.6411, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [640/17463], Loss: 2.6416, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [641/17463], Loss: 2.6414, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [642/17463], Loss: 2.6434, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [643/17463], Loss: 2.6409, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [644/17463], Loss: 2.6366, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [645/17463], Loss: 2.6327, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [646/17463], Loss: 2.6337, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [647/17463], Loss: 2.6420, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [648/17463], Loss: 2.6367, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [649/17463], Loss: 2.6311, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [650/17463], Loss: 2.6467, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [651/17463], Loss: 2.6359, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [652/17463], Loss: 2.6397, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [653/17463], Loss: 2.6416, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [654/17463], Loss: 2.6326, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [655/17463], Loss: 2.6434, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [656/17463], Loss: 2.6365, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [657/17463], Loss: 2.6427, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [658/17463], Loss: 2.6456, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [659/17463], Loss: 2.6436, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [660/17463], Loss: 2.6460, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [661/17463], Loss: 2.6428, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [662/17463], Loss: 2.6397, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [663/17463], Loss: 2.6391, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [664/17463], Loss: 2.6424, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [665/17463], Loss: 2.6362, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [666/17463], Loss: 2.6394, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [667/17463], Loss: 2.6476, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [668/17463], Loss: 2.6393, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [669/17463], Loss: 2.6409, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [670/17463], Loss: 2.6494, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [671/17463], Loss: 2.6363, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [672/17463], Loss: 2.6381, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [673/17463], Loss: 2.6433, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [674/17463], Loss: 2.6466, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [675/17463], Loss: 2.6392, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [676/17463], Loss: 2.6370, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [677/17463], Loss: 2.6524, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [678/17463], Loss: 2.6359, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [679/17463], Loss: 2.6463, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [680/17463], Loss: 2.6497, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [681/17463], Loss: 2.6370, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [682/17463], Loss: 2.6308, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [683/17463], Loss: 2.6421, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [684/17463], Loss: 2.6301, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [685/17463], Loss: 2.6351, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [686/17463], Loss: 2.6387, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [687/17463], Loss: 2.6392, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [688/17463], Loss: 2.6441, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [689/17463], Loss: 2.6316, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [690/17463], Loss: 2.6428, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [691/17463], Loss: 2.6383, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [692/17463], Loss: 2.6360, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [693/17463], Loss: 2.6389, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [694/17463], Loss: 2.6354, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [695/17463], Loss: 2.6430, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [696/17463], Loss: 2.6394, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [697/17463], Loss: 2.6321, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [698/17463], Loss: 2.6468, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [699/17463], Loss: 2.6362, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [700/17463], Loss: 2.6352, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [701/17463], Loss: 2.6362, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [702/17463], Loss: 2.6416, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [703/17463], Loss: 2.6412, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [704/17463], Loss: 2.6401, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [705/17463], Loss: 2.6381, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [706/17463], Loss: 2.6441, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [707/17463], Loss: 2.6382, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [708/17463], Loss: 2.6414, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [709/17463], Loss: 2.6422, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [710/17463], Loss: 2.6427, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [711/17463], Loss: 2.6463, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [712/17463], Loss: 2.6430, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [713/17463], Loss: 2.6494, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [714/17463], Loss: 2.6444, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [715/17463], Loss: 2.6480, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [716/17463], Loss: 2.6503, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [717/17463], Loss: 2.6452, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [718/17463], Loss: 2.6346, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [719/17463], Loss: 2.6362, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [720/17463], Loss: 2.6371, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [721/17463], Loss: 2.6321, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [722/17463], Loss: 2.6374, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [723/17463], Loss: 2.6342, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [724/17463], Loss: 2.6286, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [725/17463], Loss: 2.6351, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [726/17463], Loss: 2.6371, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [727/17463], Loss: 2.6384, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [728/17463], Loss: 2.6372, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [729/17463], Loss: 2.6409, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [730/17463], Loss: 2.6418, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [731/17463], Loss: 2.6334, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [732/17463], Loss: 2.6248, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [733/17463], Loss: 2.6428, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [734/17463], Loss: 2.6348, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [735/17463], Loss: 2.6417, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [736/17463], Loss: 2.6299, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [737/17463], Loss: 2.6490, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [738/17463], Loss: 2.6451, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [739/17463], Loss: 2.6463, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [740/17463], Loss: 2.6412, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [741/17463], Loss: 2.6367, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [742/17463], Loss: 2.6399, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [743/17463], Loss: 2.6397, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [744/17463], Loss: 2.6414, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [745/17463], Loss: 2.6438, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [746/17463], Loss: 2.6584, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [747/17463], Loss: 2.6377, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [748/17463], Loss: 2.6380, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [749/17463], Loss: 2.6402, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [750/17463], Loss: 2.6431, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [751/17463], Loss: 2.6435, Accuracy: 6.83%\n",
      "Epoch [1/10], Step [752/17463], Loss: 2.6441, Accuracy: 6.83%\n",
      "Epoch [1/10], Step [753/17463], Loss: 2.6430, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [754/17463], Loss: 2.6400, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [755/17463], Loss: 2.6336, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [756/17463], Loss: 2.6414, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [757/17463], Loss: 2.6366, Accuracy: 6.84%\n",
      "Epoch [1/10], Step [758/17463], Loss: 2.6350, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [759/17463], Loss: 2.6378, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [760/17463], Loss: 2.6416, Accuracy: 6.85%\n",
      "Epoch [1/10], Step [761/17463], Loss: 2.6291, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [762/17463], Loss: 2.6266, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [763/17463], Loss: 2.6497, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [764/17463], Loss: 2.6384, Accuracy: 6.86%\n",
      "Epoch [1/10], Step [765/17463], Loss: 2.6353, Accuracy: 6.87%\n",
      "Epoch [1/10], Step [766/17463], Loss: 2.6358, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [767/17463], Loss: 2.6509, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [768/17463], Loss: 2.6397, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [769/17463], Loss: 2.6431, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [770/17463], Loss: 2.6503, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [771/17463], Loss: 2.6351, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [772/17463], Loss: 2.6410, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [773/17463], Loss: 2.6392, Accuracy: 6.88%\n",
      "Epoch [1/10], Step [774/17463], Loss: 2.6443, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [775/17463], Loss: 2.6422, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [776/17463], Loss: 2.6406, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [777/17463], Loss: 2.6252, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [778/17463], Loss: 2.6357, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [779/17463], Loss: 2.6488, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [780/17463], Loss: 2.6408, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [781/17463], Loss: 2.6360, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [782/17463], Loss: 2.6426, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [783/17463], Loss: 2.6388, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [784/17463], Loss: 2.6371, Accuracy: 6.89%\n",
      "Epoch [1/10], Step [785/17463], Loss: 2.6366, Accuracy: 6.90%\n",
      "Epoch [1/10], Step [786/17463], Loss: 2.6401, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [787/17463], Loss: 2.6352, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [788/17463], Loss: 2.6347, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [789/17463], Loss: 2.6397, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [790/17463], Loss: 2.6387, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [791/17463], Loss: 2.6350, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [792/17463], Loss: 2.6339, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [793/17463], Loss: 2.6501, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [794/17463], Loss: 2.6343, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [795/17463], Loss: 2.6330, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [796/17463], Loss: 2.6423, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [797/17463], Loss: 2.6449, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [798/17463], Loss: 2.6351, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [799/17463], Loss: 2.6399, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [800/17463], Loss: 2.6453, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [801/17463], Loss: 2.6327, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [802/17463], Loss: 2.6452, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [803/17463], Loss: 2.6446, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [804/17463], Loss: 2.6341, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [805/17463], Loss: 2.6460, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [806/17463], Loss: 2.6336, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [807/17463], Loss: 2.6360, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [808/17463], Loss: 2.6352, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [809/17463], Loss: 2.6356, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [810/17463], Loss: 2.6381, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [811/17463], Loss: 2.6423, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [812/17463], Loss: 2.6368, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [813/17463], Loss: 2.6401, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [814/17463], Loss: 2.6360, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [815/17463], Loss: 2.6479, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [816/17463], Loss: 2.6425, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [817/17463], Loss: 2.6382, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [818/17463], Loss: 2.6442, Accuracy: 6.91%\n",
      "Epoch [1/10], Step [819/17463], Loss: 2.6369, Accuracy: 6.92%\n",
      "Epoch [1/10], Step [820/17463], Loss: 2.6354, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [821/17463], Loss: 2.6394, Accuracy: 6.93%\n",
      "Epoch [1/10], Step [822/17463], Loss: 2.6333, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [823/17463], Loss: 2.6436, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [824/17463], Loss: 2.6347, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [825/17463], Loss: 2.6349, Accuracy: 6.94%\n",
      "Epoch [1/10], Step [826/17463], Loss: 2.6361, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [827/17463], Loss: 2.6412, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [828/17463], Loss: 2.6357, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [829/17463], Loss: 2.6329, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [830/17463], Loss: 2.6424, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [831/17463], Loss: 2.6477, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [832/17463], Loss: 2.6428, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [833/17463], Loss: 2.6458, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [834/17463], Loss: 2.6357, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [835/17463], Loss: 2.6356, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [836/17463], Loss: 2.6428, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [837/17463], Loss: 2.6382, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [838/17463], Loss: 2.6410, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [839/17463], Loss: 2.6264, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [840/17463], Loss: 2.6335, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [841/17463], Loss: 2.6391, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [842/17463], Loss: 2.6429, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [843/17463], Loss: 2.6379, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [844/17463], Loss: 2.6370, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [845/17463], Loss: 2.6409, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [846/17463], Loss: 2.6313, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [847/17463], Loss: 2.6360, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [848/17463], Loss: 2.6320, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [849/17463], Loss: 2.6459, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [850/17463], Loss: 2.6506, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [851/17463], Loss: 2.6336, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [852/17463], Loss: 2.6278, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [853/17463], Loss: 2.6473, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [854/17463], Loss: 2.6503, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [855/17463], Loss: 2.6437, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [856/17463], Loss: 2.6385, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [857/17463], Loss: 2.6439, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [858/17463], Loss: 2.6354, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [859/17463], Loss: 2.6332, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [860/17463], Loss: 2.6529, Accuracy: 6.95%\n",
      "Epoch [1/10], Step [861/17463], Loss: 2.6331, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [862/17463], Loss: 2.6488, Accuracy: 6.96%\n",
      "Epoch [1/10], Step [863/17463], Loss: 2.6298, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [864/17463], Loss: 2.6280, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [865/17463], Loss: 2.6373, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [866/17463], Loss: 2.6392, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [867/17463], Loss: 2.6377, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [868/17463], Loss: 2.6384, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [869/17463], Loss: 2.6414, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [870/17463], Loss: 2.6369, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [871/17463], Loss: 2.6509, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [872/17463], Loss: 2.6356, Accuracy: 6.97%\n",
      "Epoch [1/10], Step [873/17463], Loss: 2.6366, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [874/17463], Loss: 2.6347, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [875/17463], Loss: 2.6335, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [876/17463], Loss: 2.6386, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [877/17463], Loss: 2.6331, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [878/17463], Loss: 2.6407, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [879/17463], Loss: 2.6234, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [880/17463], Loss: 2.6300, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [881/17463], Loss: 2.6486, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [882/17463], Loss: 2.6522, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [883/17463], Loss: 2.6298, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [884/17463], Loss: 2.6434, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [885/17463], Loss: 2.6412, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [886/17463], Loss: 2.6425, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [887/17463], Loss: 2.6434, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [888/17463], Loss: 2.6490, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [889/17463], Loss: 2.6343, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [890/17463], Loss: 2.6419, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [891/17463], Loss: 2.6423, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [892/17463], Loss: 2.6425, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [893/17463], Loss: 2.6415, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [894/17463], Loss: 2.6361, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [895/17463], Loss: 2.6424, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [896/17463], Loss: 2.6350, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [897/17463], Loss: 2.6275, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [898/17463], Loss: 2.6463, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [899/17463], Loss: 2.6390, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [900/17463], Loss: 2.6422, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [901/17463], Loss: 2.6241, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [902/17463], Loss: 2.6410, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [903/17463], Loss: 2.6538, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [904/17463], Loss: 2.6419, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [905/17463], Loss: 2.6401, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [906/17463], Loss: 2.6242, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [907/17463], Loss: 2.6413, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [908/17463], Loss: 2.6403, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [909/17463], Loss: 2.6411, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [910/17463], Loss: 2.6397, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [911/17463], Loss: 2.6362, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [912/17463], Loss: 2.6333, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [913/17463], Loss: 2.6374, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [914/17463], Loss: 2.6305, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [915/17463], Loss: 2.6422, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [916/17463], Loss: 2.6461, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [917/17463], Loss: 2.6452, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [918/17463], Loss: 2.6492, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [919/17463], Loss: 2.6409, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [920/17463], Loss: 2.6548, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [921/17463], Loss: 2.6368, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [922/17463], Loss: 2.6356, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [923/17463], Loss: 2.6243, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [924/17463], Loss: 2.6376, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [925/17463], Loss: 2.6376, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [926/17463], Loss: 2.6337, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [927/17463], Loss: 2.6423, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [928/17463], Loss: 2.6361, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [929/17463], Loss: 2.6405, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [930/17463], Loss: 2.6462, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [931/17463], Loss: 2.6306, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [932/17463], Loss: 2.6416, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [933/17463], Loss: 2.6377, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [934/17463], Loss: 2.6452, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [935/17463], Loss: 2.6374, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [936/17463], Loss: 2.6443, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [937/17463], Loss: 2.6389, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [938/17463], Loss: 2.6461, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [939/17463], Loss: 2.6277, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [940/17463], Loss: 2.6534, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [941/17463], Loss: 2.6445, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [942/17463], Loss: 2.6372, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [943/17463], Loss: 2.6351, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [944/17463], Loss: 2.6468, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [945/17463], Loss: 2.6323, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [946/17463], Loss: 2.6506, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [947/17463], Loss: 2.6448, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [948/17463], Loss: 2.6468, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [949/17463], Loss: 2.6395, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [950/17463], Loss: 2.6445, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [951/17463], Loss: 2.6391, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [952/17463], Loss: 2.6470, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [953/17463], Loss: 2.6374, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [954/17463], Loss: 2.6370, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [955/17463], Loss: 2.6442, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [956/17463], Loss: 2.6435, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [957/17463], Loss: 2.6473, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [958/17463], Loss: 2.6360, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [959/17463], Loss: 2.6421, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [960/17463], Loss: 2.6507, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [961/17463], Loss: 2.6408, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [962/17463], Loss: 2.6279, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [963/17463], Loss: 2.6415, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [964/17463], Loss: 2.6384, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [965/17463], Loss: 2.6385, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [966/17463], Loss: 2.6350, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [967/17463], Loss: 2.6322, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [968/17463], Loss: 2.6500, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [969/17463], Loss: 2.6400, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [970/17463], Loss: 2.6440, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [971/17463], Loss: 2.6366, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [972/17463], Loss: 2.6431, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [973/17463], Loss: 2.6484, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [974/17463], Loss: 2.6273, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [975/17463], Loss: 2.6300, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [976/17463], Loss: 2.6386, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [977/17463], Loss: 2.6413, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [978/17463], Loss: 2.6435, Accuracy: 7.04%\n",
      "Epoch [1/10], Step [979/17463], Loss: 2.6477, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [980/17463], Loss: 2.6401, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [981/17463], Loss: 2.6395, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [982/17463], Loss: 2.6378, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [983/17463], Loss: 2.6410, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [984/17463], Loss: 2.6398, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [985/17463], Loss: 2.6442, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [986/17463], Loss: 2.6449, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [987/17463], Loss: 2.6455, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [988/17463], Loss: 2.6432, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [989/17463], Loss: 2.6308, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [990/17463], Loss: 2.6373, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [991/17463], Loss: 2.6395, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [992/17463], Loss: 2.6413, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [993/17463], Loss: 2.6424, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [994/17463], Loss: 2.6319, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [995/17463], Loss: 2.6355, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [996/17463], Loss: 2.6414, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [997/17463], Loss: 2.6391, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [998/17463], Loss: 2.6359, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [999/17463], Loss: 2.6407, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1000/17463], Loss: 2.6386, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1001/17463], Loss: 2.6428, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1002/17463], Loss: 2.6409, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1003/17463], Loss: 2.6406, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1004/17463], Loss: 2.6406, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1005/17463], Loss: 2.6431, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1006/17463], Loss: 2.6458, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1007/17463], Loss: 2.6360, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1008/17463], Loss: 2.6238, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1009/17463], Loss: 2.6344, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1010/17463], Loss: 2.6502, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1011/17463], Loss: 2.6321, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1012/17463], Loss: 2.6432, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1013/17463], Loss: 2.6388, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1014/17463], Loss: 2.6530, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1015/17463], Loss: 2.6541, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1016/17463], Loss: 2.6383, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1017/17463], Loss: 2.6462, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1018/17463], Loss: 2.6370, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1019/17463], Loss: 2.6365, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1020/17463], Loss: 2.6273, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1021/17463], Loss: 2.6388, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1022/17463], Loss: 2.6462, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1023/17463], Loss: 2.6421, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1024/17463], Loss: 2.6452, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1025/17463], Loss: 2.6336, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1026/17463], Loss: 2.6362, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1027/17463], Loss: 2.6363, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1028/17463], Loss: 2.6320, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1029/17463], Loss: 2.6442, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1030/17463], Loss: 2.6355, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1031/17463], Loss: 2.6410, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1032/17463], Loss: 2.6455, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1033/17463], Loss: 2.6364, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1034/17463], Loss: 2.6372, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1035/17463], Loss: 2.6398, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1036/17463], Loss: 2.6353, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1037/17463], Loss: 2.6431, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1038/17463], Loss: 2.6381, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1039/17463], Loss: 2.6471, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1040/17463], Loss: 2.6374, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1041/17463], Loss: 2.6349, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1042/17463], Loss: 2.6436, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1043/17463], Loss: 2.6376, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1044/17463], Loss: 2.6294, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1045/17463], Loss: 2.6454, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1046/17463], Loss: 2.6326, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1047/17463], Loss: 2.6375, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1048/17463], Loss: 2.6333, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1049/17463], Loss: 2.6280, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1050/17463], Loss: 2.6467, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1051/17463], Loss: 2.6421, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1052/17463], Loss: 2.6395, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1053/17463], Loss: 2.6376, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1054/17463], Loss: 2.6408, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1055/17463], Loss: 2.6511, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1056/17463], Loss: 2.6345, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1057/17463], Loss: 2.6416, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1058/17463], Loss: 2.6481, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1059/17463], Loss: 2.6407, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1060/17463], Loss: 2.6363, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1061/17463], Loss: 2.6455, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1062/17463], Loss: 2.6384, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1063/17463], Loss: 2.6447, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1064/17463], Loss: 2.6469, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1065/17463], Loss: 2.6473, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1066/17463], Loss: 2.6354, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1067/17463], Loss: 2.6362, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1068/17463], Loss: 2.6397, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1069/17463], Loss: 2.6337, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1070/17463], Loss: 2.6302, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1071/17463], Loss: 2.6331, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1072/17463], Loss: 2.6343, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1073/17463], Loss: 2.6366, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1074/17463], Loss: 2.6314, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1075/17463], Loss: 2.6345, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1076/17463], Loss: 2.6323, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1077/17463], Loss: 2.6501, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1078/17463], Loss: 2.6347, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1079/17463], Loss: 2.6361, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1080/17463], Loss: 2.6357, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1081/17463], Loss: 2.6407, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1082/17463], Loss: 2.6437, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1083/17463], Loss: 2.6410, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1084/17463], Loss: 2.6422, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1085/17463], Loss: 2.6421, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1086/17463], Loss: 2.6255, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1087/17463], Loss: 2.6493, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1088/17463], Loss: 2.6414, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1089/17463], Loss: 2.6442, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1090/17463], Loss: 2.6444, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1091/17463], Loss: 2.6439, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1092/17463], Loss: 2.6323, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1093/17463], Loss: 2.6357, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1094/17463], Loss: 2.6367, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1095/17463], Loss: 2.6394, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1096/17463], Loss: 2.6394, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1097/17463], Loss: 2.6384, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1098/17463], Loss: 2.6401, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1099/17463], Loss: 2.6405, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1100/17463], Loss: 2.6410, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1101/17463], Loss: 2.6386, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1102/17463], Loss: 2.6463, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1103/17463], Loss: 2.6512, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1104/17463], Loss: 2.6302, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1105/17463], Loss: 2.6363, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1106/17463], Loss: 2.6397, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1107/17463], Loss: 2.6365, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1108/17463], Loss: 2.6447, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1109/17463], Loss: 2.6445, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1110/17463], Loss: 2.6325, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1111/17463], Loss: 2.6453, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1112/17463], Loss: 2.6406, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1113/17463], Loss: 2.6299, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1114/17463], Loss: 2.6305, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1115/17463], Loss: 2.6422, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1116/17463], Loss: 2.6443, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1117/17463], Loss: 2.6423, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1118/17463], Loss: 2.6463, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1119/17463], Loss: 2.6473, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1120/17463], Loss: 2.6378, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1121/17463], Loss: 2.6305, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1122/17463], Loss: 2.6404, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1123/17463], Loss: 2.6476, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1124/17463], Loss: 2.6376, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1125/17463], Loss: 2.6364, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1126/17463], Loss: 2.6396, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1127/17463], Loss: 2.6450, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1128/17463], Loss: 2.6404, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1129/17463], Loss: 2.6404, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1130/17463], Loss: 2.6334, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1131/17463], Loss: 2.6370, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1132/17463], Loss: 2.6402, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1133/17463], Loss: 2.6389, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1134/17463], Loss: 2.6519, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1135/17463], Loss: 2.6388, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1136/17463], Loss: 2.6295, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1137/17463], Loss: 2.6397, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1138/17463], Loss: 2.6385, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1139/17463], Loss: 2.6343, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1140/17463], Loss: 2.6394, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1141/17463], Loss: 2.6379, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1142/17463], Loss: 2.6425, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1143/17463], Loss: 2.6460, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1144/17463], Loss: 2.6374, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1145/17463], Loss: 2.6377, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1146/17463], Loss: 2.6374, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1147/17463], Loss: 2.6429, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1148/17463], Loss: 2.6357, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1149/17463], Loss: 2.6454, Accuracy: 7.03%\n",
      "Epoch [1/10], Step [1150/17463], Loss: 2.6504, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1151/17463], Loss: 2.6378, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1152/17463], Loss: 2.6482, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1153/17463], Loss: 2.6469, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1154/17463], Loss: 2.6361, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1155/17463], Loss: 2.6459, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1156/17463], Loss: 2.6328, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1157/17463], Loss: 2.6428, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1158/17463], Loss: 2.6350, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1159/17463], Loss: 2.6482, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1160/17463], Loss: 2.6377, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1161/17463], Loss: 2.6334, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1162/17463], Loss: 2.6391, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1163/17463], Loss: 2.6322, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1164/17463], Loss: 2.6430, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1165/17463], Loss: 2.6399, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1166/17463], Loss: 2.6418, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1167/17463], Loss: 2.6438, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1168/17463], Loss: 2.6433, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1169/17463], Loss: 2.6414, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1170/17463], Loss: 2.6354, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1171/17463], Loss: 2.6379, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1172/17463], Loss: 2.6405, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1173/17463], Loss: 2.6401, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1174/17463], Loss: 2.6388, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1175/17463], Loss: 2.6443, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1176/17463], Loss: 2.6324, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1177/17463], Loss: 2.6443, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1178/17463], Loss: 2.6391, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1179/17463], Loss: 2.6397, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1180/17463], Loss: 2.6316, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1181/17463], Loss: 2.6388, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1182/17463], Loss: 2.6463, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1183/17463], Loss: 2.6340, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1184/17463], Loss: 2.6389, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1185/17463], Loss: 2.6375, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1186/17463], Loss: 2.6359, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1187/17463], Loss: 2.6370, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1188/17463], Loss: 2.6397, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1189/17463], Loss: 2.6390, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1190/17463], Loss: 2.6444, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1191/17463], Loss: 2.6467, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1192/17463], Loss: 2.6352, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1193/17463], Loss: 2.6368, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1194/17463], Loss: 2.6436, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1195/17463], Loss: 2.6390, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1196/17463], Loss: 2.6403, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1197/17463], Loss: 2.6481, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1198/17463], Loss: 2.6387, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1199/17463], Loss: 2.6379, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1200/17463], Loss: 2.6451, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1201/17463], Loss: 2.6347, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1202/17463], Loss: 2.6368, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1203/17463], Loss: 2.6392, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1204/17463], Loss: 2.6364, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1205/17463], Loss: 2.6353, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1206/17463], Loss: 2.6362, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1207/17463], Loss: 2.6339, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1208/17463], Loss: 2.6370, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1209/17463], Loss: 2.6373, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1210/17463], Loss: 2.6413, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1211/17463], Loss: 2.6353, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1212/17463], Loss: 2.6342, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1213/17463], Loss: 2.6525, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1214/17463], Loss: 2.6373, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1215/17463], Loss: 2.6421, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1216/17463], Loss: 2.6371, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1217/17463], Loss: 2.6428, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1218/17463], Loss: 2.6392, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1219/17463], Loss: 2.6433, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1220/17463], Loss: 2.6403, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1221/17463], Loss: 2.6371, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1222/17463], Loss: 2.6429, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1223/17463], Loss: 2.6377, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1224/17463], Loss: 2.6365, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1225/17463], Loss: 2.6480, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1226/17463], Loss: 2.6325, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1227/17463], Loss: 2.6404, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1228/17463], Loss: 2.6407, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1229/17463], Loss: 2.6334, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1230/17463], Loss: 2.6420, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1231/17463], Loss: 2.6336, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1232/17463], Loss: 2.6310, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1233/17463], Loss: 2.6388, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1234/17463], Loss: 2.6340, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1235/17463], Loss: 2.6294, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1236/17463], Loss: 2.6378, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1237/17463], Loss: 2.6458, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1238/17463], Loss: 2.6349, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1239/17463], Loss: 2.6301, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1240/17463], Loss: 2.6476, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1241/17463], Loss: 2.6384, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1242/17463], Loss: 2.6436, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1243/17463], Loss: 2.6356, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1244/17463], Loss: 2.6377, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1245/17463], Loss: 2.6351, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1246/17463], Loss: 2.6478, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1247/17463], Loss: 2.6371, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1248/17463], Loss: 2.6479, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1249/17463], Loss: 2.6453, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1250/17463], Loss: 2.6438, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1251/17463], Loss: 2.6394, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1252/17463], Loss: 2.6394, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1253/17463], Loss: 2.6478, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1254/17463], Loss: 2.6392, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1255/17463], Loss: 2.6329, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1256/17463], Loss: 2.6458, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1257/17463], Loss: 2.6383, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1258/17463], Loss: 2.6372, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1259/17463], Loss: 2.6412, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1260/17463], Loss: 2.6457, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1261/17463], Loss: 2.6409, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1262/17463], Loss: 2.6427, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1263/17463], Loss: 2.6363, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1264/17463], Loss: 2.6437, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1265/17463], Loss: 2.6454, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1266/17463], Loss: 2.6340, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1267/17463], Loss: 2.6370, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1268/17463], Loss: 2.6373, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1269/17463], Loss: 2.6402, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1270/17463], Loss: 2.6416, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1271/17463], Loss: 2.6411, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1272/17463], Loss: 2.6397, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1273/17463], Loss: 2.6413, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1274/17463], Loss: 2.6474, Accuracy: 6.98%\n",
      "Epoch [1/10], Step [1275/17463], Loss: 2.6399, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1276/17463], Loss: 2.6427, Accuracy: 6.99%\n",
      "Epoch [1/10], Step [1277/17463], Loss: 2.6339, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1278/17463], Loss: 2.6436, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1279/17463], Loss: 2.6422, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1280/17463], Loss: 2.6329, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1281/17463], Loss: 2.6420, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1282/17463], Loss: 2.6427, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1283/17463], Loss: 2.6460, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1284/17463], Loss: 2.6387, Accuracy: 7.00%\n",
      "Epoch [1/10], Step [1285/17463], Loss: 2.6409, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1286/17463], Loss: 2.6294, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1287/17463], Loss: 2.6344, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1288/17463], Loss: 2.6335, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1289/17463], Loss: 2.6430, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1290/17463], Loss: 2.6483, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1291/17463], Loss: 2.6474, Accuracy: 7.01%\n",
      "Epoch [1/10], Step [1292/17463], Loss: 2.6440, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1293/17463], Loss: 2.6337, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1294/17463], Loss: 2.6346, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1295/17463], Loss: 2.6408, Accuracy: 7.02%\n",
      "Epoch [1/10], Step [1296/17463], Loss: 2.6348, Accuracy: 7.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [25:55<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, train_loader, num_epochs, criterion, optimizer)\n",
      "\u001b[1;32m/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# calculate accuracy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/LSTM/LSTM_dbpedia/lstm_pytorch.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 12:33:48.093003: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-31 12:33:48.124120: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-31 12:33:48.276354: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-31 12:33:48.278808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 12:33:49.254671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-31 12:34:06.958857: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTMCell, RNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert tokens and labels to Keras format\n",
    "# Note: Ensure tokens and new_labels are numpy arrays\n",
    "X_train = [[word_to_idx.get(token, word_to_idx['UNK']) for token in sentence] for sentence in tokens]\n",
    "X_test = [[word_to_idx.get(token, word_to_idx['UNK']) for token in sentence] for sentence in tokens_test]\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Convert labels to one-hot format\n",
    "y_train = to_categorical(new_labels, num_classes=num_classes)\n",
    "y_test = to_categorical(new_labels_test, num_classes=num_classes)\n",
    "\n",
    "# GPU Configuration\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, mask_zero=True))\n",
    "model.add(RNN(LSTMCell(hidden_size)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 12:36:35.657519: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [32,14]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/17462 [..............................] - ETA: 20:50:08 - loss: 2.6268 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 12:36:40.061046: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [32,14]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2/17462 [..............................] - ETA: 18:20:26 - loss: 2.6260 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 12:36:43.994149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [32,14]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    # Initialize Progbar with the total number of batches\n",
    "    progbar = Progbar(len(X_train) // batch_size)\n",
    "    \n",
    "    # Training\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i + batch_size]\n",
    "        y_batch = y_train[i:i + batch_size]\n",
    "        \n",
    "        # Check batch size; skip if it's not 32\n",
    "        if len(X_batch) != 32:\n",
    "            continue\n",
    "        \n",
    "        batch_loss, batch_acc = model.train_on_batch(X_batch, y_batch)\n",
    "        \n",
    "        # Update progbar with the batch's metrics\n",
    "        progbar.add(1, values=[('loss', batch_loss), ('accuracy', batch_acc)])\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model.save(f\"dbpedia_keras_model_epoch_{epoch+1}.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
