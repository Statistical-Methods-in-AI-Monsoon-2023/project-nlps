# Notes LSTM dbpedia

## Baseline Pytorch

- 2 epochs

```txt
            precision    recall  f1-score   support

           0       0.78      0.72      0.75      4987
           1       0.86      0.93      0.89      4981
           2       0.89      0.80      0.84      4979
           3       0.88      0.98      0.93      4990
           4       0.91      0.90      0.90      4989
           5       0.97      0.90      0.93      4989
           6       0.88      0.87      0.87      4997
           7       0.97      0.96      0.97      4987
           8       1.00      0.98      0.99      5000
           9       0.94      0.92      0.93      4984
          10       0.94      0.94      0.94      4999
          11       0.98      0.94      0.96      4985
          12       0.92      0.97      0.94      4991
          13       0.71      0.81      0.76      4981

    accuracy                           0.90     69839
   macro avg       0.90      0.90      0.90     69839
weighted avg       0.90      0.90      0.90     69839

[[3574   74  102   14   58   12   77    0    0    4    2    9   47 1014]
 [  19 4612    1    1    3    3  147    0    0    0    1    0   31  163]
 [ 183    2 3962  374  324    0    1    5    1   58   12    6    7   44]
 [   9    1   57 4880   24    0    1   10    0    3    4    0    0    1]
 [ 110    1  222  127 4497    0    4    0    0    0    0    0    0   28]
 [  21   38    2    3    5 4489  295    3    0    2    0   37   21   73]
 [  35  302    4   13   13   74 4346   10    2    1    0    0   10  187]
 [  22    5   12   80   12    2   11 4785    2    7   19    1    1   28]
 [   2    4   13    7    2    0    1   19 4885   55    3    0    0    9]
 [  24    4   48   15    0    1    3   14    4 4587  277    1    0    6]
 [  11    2   26   11    3    2    0   80    0  179 4677    0    2    6]
 [  23   12    4    0    4   24   11    0    0    0    0 4686  201   20]
 [  27   30    2    0    1    3   18    0    0    1    1   13 4824   71]
 [ 495  270   15    3   16    8   36    3    1    1    1    6  100 4026]]
```

## Baseline Keras

- 2 epochs

```txt
Test Accuracy: 96.86%
Classification Report:
               precision    recall  f1-score   support

           0       0.95      0.93      0.94      4987
           1       0.97      0.98      0.97      4981
           2       0.96      0.92      0.94      4979
           3       0.99      0.97      0.98      4990
           4       0.95      0.98      0.96      4989
           5       0.98      0.98      0.98      4989
           6       0.96      0.94      0.95      4997
           7       0.97      0.98      0.98      4987
           8       0.98      1.00      0.99      5000
           9       0.98      0.98      0.98      4984
          10       0.98      0.98      0.98      4999
          11       0.97      0.99      0.98      4985
          12       0.96      0.97      0.97      4991
          13       0.94      0.96      0.95      4981

    accuracy                           0.97     69839
   macro avg       0.97      0.97      0.97     69839
weighted avg       0.97      0.97      0.97     69839

Confusion Matrix:
 [[4628  104   29    2   11   32   64    9    9    6   21    7   32   33]
 [  31 4881    0    0    2    0   22    2    5    0    3    0   34    1]
 [  17    2 4585   21  130    3    3    6    0    0    2   49   13  148]
 [   5    1   44 4846   85    2    0    3    0    0    1    0    1    2]
 [   2    2   47    8 4868    1    6    9    0    1    1   26    7   11]
 [  49    0    1    1    6 4868   59    3    0    0    0    0    1    1]
 [  71   41    0    1    7   35 4720   76   33    2    2    0    7    2]
 [   3    0    0    0    1    1   15 4911   35    7    7    2    2    3]
 [   3    0    0    0    1    0    3    7 4976    8    1    0    0    1]
 [   3    0    0    0    0    0    0   15    9 4903   54    0    0    0]
 [  15    0    1    0    0    0    1    2    0   73 4896    1    2    8]
 [   0    0    9    1   12    1    0    2    0    0    0 4923   10   27]
 [  11    4    4    0    4    0    0    3    0    0    5   16 4849   95]
 [  20    6   34    2    8    0    4    6    0    1   10   26   72 4792]]
```