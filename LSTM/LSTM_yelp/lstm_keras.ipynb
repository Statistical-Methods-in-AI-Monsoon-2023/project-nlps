{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:12:24.225096Z","iopub.status.busy":"2023-10-31T21:12:24.224826Z","iopub.status.idle":"2023-10-31T21:13:05.523768Z","shell.execute_reply":"2023-10-31T21:13:05.522769Z","shell.execute_reply.started":"2023-10-31T21:12:24.225073Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1a15b2374a5432a87b1de5917a7719b","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1dbaa08cfe54d4f907be1a90e0c5187","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/979 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset yelp_review_full/yelp_review_full (download: 187.06 MiB, generated: 496.94 MiB, post-processed: Unknown size, total: 684.00 MiB) to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8943388447941f2b4f9afff6cd45bc8","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/196M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset yelp_review_full downloaded and prepared to /root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e30ca96f56ed48fda8bc8f5e2bd15e0e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"yelp_review_full\")\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:13:05.532326Z","iopub.status.busy":"2023-10-31T21:13:05.532027Z","iopub.status.idle":"2023-10-31T21:13:07.895500Z","shell.execute_reply":"2023-10-31T21:13:07.894499Z","shell.execute_reply.started":"2023-10-31T21:13:05.532295Z"},"trusted":true},"outputs":[],"source":["import pickle\n","# with open('yelp_dataset.pkl', 'wb') as file:\n","#     pickle.dump(dataset, file)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# with open('yelp_dataset.pkl', 'rb') as file:\n","#     dataset = pickle.load(file)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","\n","    train: Dataset({\n","\n","        features: ['label', 'text'],\n","\n","        num_rows: 650000\n","\n","    })\n","\n","    test: Dataset({\n","\n","        features: ['label', 'text'],\n","\n","        num_rows: 50000\n","\n","    })\n","\n","})\n"]}],"source":["print(dataset)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:13:07.898581Z","iopub.status.busy":"2023-10-31T21:13:07.898168Z","iopub.status.idle":"2023-10-31T21:13:10.092434Z","shell.execute_reply":"2023-10-31T21:13:10.091269Z","shell.execute_reply.started":"2023-10-31T21:13:07.898546Z"},"trusted":true},"outputs":[],"source":["train_dataset = dataset['train']\n","test_dataset = dataset['test']\n","\n","# Convert train and test datasets to arrays\n","train_data = train_dataset['text']\n","train_labels = train_dataset['label']\n","test_data = test_dataset['text']\n","test_labels = test_dataset['label']\n","\n","# Convert labels to lists (optional)\n","# train_labels = train_labels.tolist()\n","# test_labels = test_labels.tolist()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n","\n","4\n"]}],"source":["print(train_data[0])\n","print(train_labels[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","\n","[nltk_data]     /home/sriteja/nltk_data...\n","\n","[nltk_data]   Package stopwords is already up-to-date!\n","\n","[nltk_data] Downloading package punkt to /home/sriteja/nltk_data...\n","\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: contractions in /home/sriteja/anaconda3/lib/python3.10/site-packages (0.1.73)\n","\n","Requirement already satisfied: textsearch>=0.0.21 in /home/sriteja/anaconda3/lib/python3.10/site-packages (from contractions) (0.0.24)\n","\n","Requirement already satisfied: pyahocorasick in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n","\n","Requirement already satisfied: anyascii in /home/sriteja/anaconda3/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"]}],"source":["import sys  \n","!{sys.executable} -m pip install contractions\n","import contractions"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# import re  # Import the regular expressions module\n","\n","# # use tokenizer to remove punctuation\n","\n","# def tokenize_text(text):\n","#     # Replace \"!\" with \"exm\" using regular expressions\n","#     text = re.sub(r'!', ' exm', text)\n","    \n","#     expanded_words = []\n","#     for word in text.split():\n","#         # using contractions.fix to expand the shortened words\n","#         expanded_words.append(contractions.fix(word))   \n","    \n","#     expanded_text = ' '.join(expanded_words)\n","#     # print(expanded_text)\n","    \n","#     text = expanded_text    \n","    \n","#     # replace a-b with a and b\n","#     text = text.replace('-', ' ')\n","    \n","#     tokens = nltk.word_tokenize(text)\n","    \n","#     # Add an extra occurrence for all-uppercase words with more than one letter\n","#     # temp = [word if (len(word) > 1 and word.isupper()) else None for word in tokens]\n","#     temp = []\n","#     for word in tokens:\n","#         if len(word) > 1 and word.isupper():\n","#             temp.append(word)\n","#     tokens.extend(x for x in temp if x)\n","    \n","#     # convert to lower case\n","#     tokens = [w.lower() for w in tokens]\n","#     # dr. = dr and st. = st and so on\n","#     tokens = [w.replace('.', '') for w in tokens]\n","    \n","#     # remove punctuation\n","#     # tokens = [word for word in tokens if word.isalpha()]\n","    \n","#     # Remove stop words\n","#     stop_words = set(stopwords.words(\"english\"))\n","#     tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n","    \n","#     return tokens\n","\n","# # print(train_data[4])\n","# # tokenize_text(train_data[4])\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# tokens = []\n","# print(len(train_data))\n","\n","# for i in range(len(train_data)):\n","#     tokens.append(tokenize_text(train_data[i]))\n","#     if i % 1000 == 0:\n","#         print(i)\n","\n","# # save to pkl file\n","# with open('yelp_train_tokens_no_stop.pkl', 'wb') as file:\n","#     pickle.dump(tokens, file)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# import pickle\n","# from collections import Counter\n","\n","# # Load the tokenized data\n","# with open('yelp_train_tokens_no_stop.pkl', 'rb') as file:\n","#     tokens = pickle.load(file)\n","\n","# # Define the frequency cutoff threshold\n","# frequency_cutoff = 10\n","\n","# # Count word frequencies\n","# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n","\n","# # Filter out words with counts less than the threshold\n","# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n","\n","# # Save the filtered tokens to a new file\n","# with open('yelp_train_tokens_filtered.pkl', 'wb') as file:\n","#     pickle.dump(filtered_tokens, file)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# tokens_test = []\n","# print(len(test_data))\n","\n","# for i in range(len(test_data)):\n","#     tokens_test.append(tokenize_text(test_data[i]))\n","#     if i % 1000 == 0:\n","#         print(i)\n","\n","# with open('yelp_test_tokens_no_stop.pkl', 'wb') as file:\n","#     pickle.dump(tokens_test, file)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# import pickle\n","# from collections import Counter\n","\n","# # Load the tokenized data\n","# with open('yelp_test_tokens_no_stop.pkl', 'rb') as file:\n","#     tokens = pickle.load(file)\n","\n","# # Define the frequency cutoff threshold\n","# frequency_cutoff = 10\n","\n","# # Count word frequencies\n","# word_counts = Counter(word for tokens_list in tokens for word in tokens_list)\n","\n","# # Filter out words with counts less than the threshold\n","# filtered_tokens = [[word for word in tokens_list if word_counts[word] >= frequency_cutoff] for tokens_list in tokens]\n","\n","# # Save the filtered tokens to a new file\n","# with open('yelp_test_tokens_filtered.pkl', 'wb') as file:\n","#     pickle.dump(filtered_tokens, file)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:28:12.066808Z","iopub.status.busy":"2023-10-31T21:28:12.066429Z","iopub.status.idle":"2023-10-31T21:28:23.109502Z","shell.execute_reply":"2023-10-31T21:28:23.108379Z","shell.execute_reply.started":"2023-10-31T21:28:12.066772Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded tokens\n"]}],"source":["import numpy as np\n","\n","train_tokens = []\n","with open('yelp_train_tokens_filtered.pkl', 'rb') as file:\n","    train_tokens = pickle.load(file)\n","\n","test_tokens = []\n","with open('yelp_test_tokens_filtered.pkl', 'rb') as file:\n","    test_tokens = pickle.load(file)\n","    \n","print(\"Loaded tokens\")\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:28:23.111490Z","iopub.status.busy":"2023-10-31T21:28:23.111100Z","iopub.status.idle":"2023-10-31T21:29:33.994635Z","shell.execute_reply":"2023-10-31T21:29:33.993716Z","shell.execute_reply.started":"2023-10-31T21:28:23.111463Z"},"trusted":true},"outputs":[],"source":["# in train data replace every 1000th word with UNK randomly\n","\n","import random\n","\n","for i in range(len(train_tokens)):\n","    for j in range(len(train_tokens[i])):\n","        if random.randint(1, 1000) == 1:\n","            train_tokens[i][j] = 'UNK'\n","        "]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:29:33.996247Z","iopub.status.busy":"2023-10-31T21:29:33.995916Z","iopub.status.idle":"2023-10-31T21:29:34.002276Z","shell.execute_reply":"2023-10-31T21:29:34.001149Z","shell.execute_reply.started":"2023-10-31T21:29:33.996208Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["650000\n","50000\n"]}],"source":["tokens = train_tokens\n","tokens_test = test_tokens\n","print(len(tokens))\n","print(len(tokens_test))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:29:34.004274Z","iopub.status.busy":"2023-10-31T21:29:34.003963Z","iopub.status.idle":"2023-10-31T21:29:34.700050Z","shell.execute_reply":"2023-10-31T21:29:34.699004Z","shell.execute_reply.started":"2023-10-31T21:29:34.004240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Max length:  626\n","Min length:  0\n","Number of sentences with length > 250:  11094\n"]}],"source":["# find length of longest sentence\n","max_len = 0\n","for i in range(len(tokens)):\n","    if len(tokens[i]) > max_len:\n","        max_len = len(tokens[i])\n","        \n","# find length of shortest sentence\n","min_len = 1000000\n","for i in range(len(tokens)):\n","    if len(tokens[i]) < min_len:\n","        min_len = len(tokens[i])\n","        \n","print(\"Max length: \", max_len)\n","print(\"Min length: \", min_len)\n","\n","# number of sentences with length > 100\n","count = 0\n","for i in range(len(tokens)):\n","    if len(tokens[i]) > 250:\n","        count += 1\n","        \n","print(\"Number of sentences with length > 250: \", count)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:30:40.043150Z","iopub.status.busy":"2023-10-31T21:30:40.042757Z","iopub.status.idle":"2023-10-31T21:30:40.542779Z","shell.execute_reply":"2023-10-31T21:30:40.541730Z","shell.execute_reply.started":"2023-10-31T21:30:40.043118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["638906\n","638906\n","49225\n","49225\n"]}],"source":["# remove sentences with length > 100 along with their labels\n","new_tokens = []\n","new_labels = []\n","for i in range(len(tokens)):\n","    if len(tokens[i]) <= 250:\n","        new_tokens.append(tokens[i])\n","        new_labels.append(train_labels[i])\n","        \n","# remove so in test also\n","new_tokens_test = []\n","new_labels_test = []\n","for i in range(len(tokens_test)):\n","    if len(tokens_test[i]) <= 250:\n","        new_tokens_test.append(tokens_test[i])\n","        new_labels_test.append(test_labels[i])\n","        \n","print(len(new_tokens))\n","print(len(new_labels))\n","print(len(new_tokens_test))\n","print(len(new_labels_test))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:31:17.518746Z","iopub.status.busy":"2023-10-31T21:31:17.518371Z","iopub.status.idle":"2023-10-31T21:31:17.563390Z","shell.execute_reply":"2023-10-31T21:31:17.562475Z","shell.execute_reply.started":"2023-10-31T21:31:17.518716Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Max length:  487\n","Min length:  0\n"]}],"source":["# for tetst data\n","# find length of longest sentence\n","max_len_test = 0\n","for i in range(len(tokens_test)):\n","    if len(tokens_test[i]) > max_len_test:\n","        max_len_test = len(tokens_test[i])\n","        \n","# find length of shortest sentence\n","min_len_test = 1000000\n","for i in range(len(tokens_test)):\n","    if len(tokens_test[i]) < min_len_test:\n","        min_len_test = len(tokens_test[i])\n","        \n","print(\"Max length: \", max_len_test)\n","print(\"Min length: \", min_len_test)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:31:21.056703Z","iopub.status.busy":"2023-10-31T21:31:21.056350Z","iopub.status.idle":"2023-10-31T21:31:21.062304Z","shell.execute_reply":"2023-10-31T21:31:21.061409Z","shell.execute_reply.started":"2023-10-31T21:31:21.056678Z"},"trusted":true},"outputs":[],"source":["# add padding to all sentences and S and EOS tokens\n","def padding(tokens_list, max_len):\n","    for i in range(len(tokens_list)):\n","        tokens_list[i] = ['S'] + tokens_list[i] + ['EOS']\n","        while len(tokens_list[i]) < max_len:\n","            tokens_list[i].append('PAD')\n","    return tokens_list"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:31:26.284038Z","iopub.status.busy":"2023-10-31T21:31:26.283654Z","iopub.status.idle":"2023-10-31T21:32:04.532136Z","shell.execute_reply":"2023-10-31T21:32:04.531244Z","shell.execute_reply.started":"2023-10-31T21:31:26.284008Z"},"trusted":true},"outputs":[],"source":["max_len = 252\n","tokens = padding(new_tokens, max_len)\n","tokens_test = padding(new_tokens_test, max_len)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["644366\n","\n","49603\n"]}],"source":["print(len(tokens))\n","print(len(tokens_test))"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:32:04.534169Z","iopub.status.busy":"2023-10-31T21:32:04.533860Z","iopub.status.idle":"2023-10-31T21:32:09.140085Z","shell.execute_reply":"2023-10-31T21:32:09.138943Z","shell.execute_reply.started":"2023-10-31T21:32:04.534142Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mallet\n","125\n","2208\n","9873\n","33346\n"]}],"source":["# Create a vocabulary by collecting unique words from the training data\n","vocab = set()\n","for token in tokens:\n","    vocab.update(token)\n","\n","# Create a dictionary to map words to indices in the vocabulary\n","word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n","idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n","\n","print(idx_to_word[0])\n","print(word_to_idx['UNK'])\n","print(word_to_idx['PAD'])\n","print(word_to_idx['S'])\n","print(word_to_idx['EOS'])\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:32:09.142197Z","iopub.status.busy":"2023-10-31T21:32:09.141537Z","iopub.status.idle":"2023-10-31T21:32:09.147312Z","shell.execute_reply":"2023-10-31T21:32:09.146296Z","shell.execute_reply.started":"2023-10-31T21:32:09.142159Z"},"trusted":true},"outputs":[],"source":["batch_size = 256\n","num_classes = 5\n","vocab_size = len(vocab)\n","num_epochs = 3\n","learning_rate = 0.001\n","embedding_size = 300\n","hidden_size = 128"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:44:44.935114Z","iopub.status.busy":"2023-10-31T21:44:44.934702Z","iopub.status.idle":"2023-10-31T21:45:52.868178Z","shell.execute_reply":"2023-10-31T21:45:52.867078Z","shell.execute_reply.started":"2023-10-31T21:44:44.935082Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTMCell, RNN, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","\n","# Convert tokens and labels to Keras format\n","# Note: Ensure tokens and new_labels are numpy arrays\n","X_train = [[word_to_idx.get(token, word_to_idx['UNK']) for token in sentence] for sentence in tokens]\n","X_test = [[word_to_idx.get(token, word_to_idx['UNK']) for token in sentence] for sentence in tokens_test]\n","X_train = np.array(X_train)\n","X_test = np.array(X_test)\n","\n","# Convert labels to one-hot format\n","y_train = to_categorical(new_labels, num_classes=num_classes)\n","y_test = to_categorical(new_labels_test, num_classes=num_classes)\n","\n","# GPU Configuration\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if len(physical_devices) > 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:46:28.723960Z","iopub.status.busy":"2023-10-31T21:46:28.722982Z","iopub.status.idle":"2023-10-31T21:46:28.865105Z","shell.execute_reply":"2023-10-31T21:46:28.863973Z","shell.execute_reply.started":"2023-10-31T21:46:28.723922Z"},"trusted":true},"outputs":[],"source":["# Model\n","model = Sequential()\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, mask_zero=True))\n","model.add(RNN(LSTMCell(hidden_size)))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(optimizer=Adam(learning_rate=learning_rate),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T21:46:30.841406Z","iopub.status.busy":"2023-10-31T21:46:30.841016Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  33/2496 [..............................] - ETA: 36:25 - loss: 1.6113 - accuracy: 0.2073"]}],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","for _ in range(num_epochs):\n","    \n","    history = model.fit(X_train, y_train, epochs=1, batch_size=256, shuffle=True, validation_data=(X_test, y_test))\n","    \n","    # Get the predictions\n","    y_pred = model.predict(X_test)\n","    y_pred_classes = np.argmax(y_pred, axis=1)\n","    \n","    # Default accuracy\n","    print('Default Test Accuracy:', np.mean(y_pred_classes == y_test) * 100)\n","    \n","    # Custom accuracy calculation\n","    total = len(y_test)\n","    correct = 0\n","\n","    for i in range(total):\n","        diff = abs(y_pred_classes[i] - y_test[i])\n","        if diff == 0:\n","            correct += 1\n","        elif diff == 1:\n","            correct += 0.75\n","        elif diff == 2:\n","            correct += 0.5\n","        elif diff == 3:\n","            correct += 0.25\n","\n","    # Calculate and print custom accuracy\n","    accuracy_ordinal = 100 * correct / total\n","    print(f'Ordinal Accuracy of the network on the {total} test inputs: {accuracy_ordinal} %')\n","\n","    # Classification report and confusion matrix\n","    print(classification_report(y_test, y_pred_classes))\n","    print(confusion_matrix(y_test, y_pred_classes))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":4}
