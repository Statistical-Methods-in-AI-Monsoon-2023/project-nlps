{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### developing end-to-end model, where given an input and choosing model which is either a CNN or CANINE transfomer, it runs the model and returns the output of model for given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from transformers import CanineTokenizer, CanineForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input from user\n",
    "# text_input = input(\"Enter the input text: \")\n",
    "model_input = input(\"Enter the model name, either CNN or CANINE: \")\n",
    "dataset_input = input(\"Enter the dataset name, either yelp, dbpedia, or amazon: \")\n",
    "\n",
    "if model_input == \"CNN\":\n",
    "    # run CNN model\n",
    "    model_flag = 0\n",
    "else:\n",
    "    # run CANINE model\n",
    "    model_flag = 1\n",
    "\n",
    "if dataset_input == \"yelp\":\n",
    "    # run yelp dataset\n",
    "    dataset_flag = 0\n",
    "    text_input = input(\"Enter the input text: \")\n",
    "elif dataset_input == \"dbpedia\":\n",
    "    # run dbpedia dataset\n",
    "    dataset_flag = 1\n",
    "    title_input = input(\"Enter the title: \")\n",
    "    content_input = input(\"Enter the content: \")\n",
    "else:\n",
    "    # run amazon dataset\n",
    "    dataset_flag = 2\n",
    "    title_input = input(\"Enter the title: \")\n",
    "    content_input = input(\"Enter the content: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "if model_flag == 0:\n",
    "    if dataset_flag == 0:\n",
    "        dataset = load_dataset(\"yelp_review_full\")\n",
    "        train_dataset = dataset['train']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "        # Convert train and test datasets to arrays\n",
    "        train_data = train_dataset['text']\n",
    "        train_labels = train_dataset['label']\n",
    "        test_data = test_dataset['text']\n",
    "        test_labels = test_dataset['label']\n",
    "        \n",
    "        # train val split\n",
    "        train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "    elif dataset_flag == 1:\n",
    "        dataset = load_dataset(\"dbpedia_14\")\n",
    "        train_dataset = dataset['train']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "        # Convert train and test datasets to arrays\n",
    "        train_data_title = train_dataset['title']\n",
    "        train_data_content = train_dataset['content']\n",
    "        train_labels = train_dataset['label']\n",
    "        test_data_title = test_dataset['title']\n",
    "        test_data_content = test_dataset['content']\n",
    "        test_labels = test_dataset['label']\n",
    "\n",
    "        # train_data = train_data_title + train_data_content\n",
    "        # test_data = test_data_title + test_data_content\n",
    "\n",
    "        train_data = [None] * len(train_data_title)\n",
    "        for i in range(len(train_data_title)):\n",
    "            train_data[i] = train_data_title[i] + \"mid\" + train_data_content[i]\n",
    "\n",
    "        test_data = [None] * len(test_data_title)\n",
    "        for i in range(len(test_data_title)):\n",
    "            test_data[i] = test_data_title[i] + \"mid\" + test_data_content[i]\n",
    "\n",
    "        # train val split\n",
    "        train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "    else:\n",
    "        dataset = load_dataset(\"amazon_polarity\")\n",
    "        train_dataset = dataset['train']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "        # Convert train and test datasets to arrays\n",
    "        train_data_title = train_dataset['title']\n",
    "        train_data_content = train_dataset['content']\n",
    "        train_labels = train_dataset['label']\n",
    "        test_data_title = test_dataset['title']\n",
    "        test_data_content = test_dataset['content']\n",
    "        test_labels = test_dataset['label']\n",
    "\n",
    "        # train_data = train_data_title + train_data_content\n",
    "        # test_data = test_data_title + test_data_content\n",
    "\n",
    "        train_data = [None] * len(train_data_title)\n",
    "        for i in range(len(train_data_title)):\n",
    "            train_data[i] = train_data_title[i] + \"mid\" + train_data_content[i]\n",
    "\n",
    "        test_data = [None] * len(test_data_title)\n",
    "        for i in range(len(test_data_title)):\n",
    "            test_data[i] = test_data_title[i] + \"mid\" + test_data_content[i]\n",
    "\n",
    "        # train val split\n",
    "        train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_flag == 1:\n",
    "    # define text_input as title_input + mid + content_input\n",
    "    text_input = title_input + \"mid \" + content_input\n",
    "if dataset_flag == 2:\n",
    "    # define text_input as title_input + mid + content_input\n",
    "    text_input = title_input + \"mid \" + content_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x3367d0860> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "The predicted label is:  [1]\n"
     ]
    }
   ],
   "source": [
    "# pre-process and run inference the input text based on which dataset is chosen and which model is chosen\n",
    "if model_flag == 1:\n",
    "    tokenizer = CanineTokenizer.from_pretrained(\"google/canine-c\")\n",
    "    encoded_input = tokenizer(text_input, padding=True, truncation=True,return_tensors=\"pt\").to(device)\n",
    "\n",
    "    if dataset_flag == 0:\n",
    "        # load the yelp model\n",
    "        loaded_model = CanineForSequenceClassification.from_pretrained(\"canine_yelp_model_2\").to(device)    \n",
    "    elif dataset_flag == 1:\n",
    "        # load the dbpedia model\n",
    "        loaded_model = CanineForSequenceClassification.from_pretrained(\"canine_dbpedia_model_2\").to(device)\n",
    "    else:\n",
    "        # load the amazon model\n",
    "        loaded_model = CanineForSequenceClassification.from_pretrained(\"canine_amazon_model_2\").to(device)\n",
    "\n",
    "    #perform inference\n",
    "    with torch.no_grad():\n",
    "        loaded_model.eval()\n",
    "        output = loaded_model(**encoded_input)\n",
    "        # scores = output[0][0].cpu().numpy()\n",
    "        # print(\"The output scores are: \", scores)\n",
    "        predicted_label = torch.argmax(output.logits, dim=1).item()\n",
    "\n",
    "    # print the predicted label\n",
    "    print(\"The predicted label is: \", predicted_label)\n",
    "\n",
    "else:\n",
    "    tokenizer_cnn  = Tokenizer(char_level=True)\n",
    "    tokenizer_cnn.fit_on_texts(train_data)\n",
    "    sequences = tokenizer_cnn.texts_to_sequences([text_input])\n",
    "    # padded_sequences = pad_sequences(sequences, maxlen=1495)\n",
    "\n",
    "\n",
    "    if dataset_flag == 0:\n",
    "        # load the yelp model\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=1100)\n",
    "        loaded_model = keras.models.load_model('yelp_cnn_model.h5')\n",
    "    elif dataset_flag == 1:\n",
    "        # load the dbpedia model\n",
    "        # print(\"The sequence is: \", sequences)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=1495)\n",
    "        # print(\"The padded sequence is: \", padded_sequences)\n",
    "        loaded_model = keras.models.load_model('dbpedia_cnn.h5')\n",
    "    else:\n",
    "        # load the amazon model\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=266)\n",
    "        loaded_model = keras.models.load_model('amazon_cnn.h5')\n",
    "\n",
    "    #perform inference\n",
    "    output = loaded_model.predict(padded_sequences)\n",
    "    predicted_label = np.argmax(output, axis=1)\n",
    "\n",
    "    # print the predicted label\n",
    "    print(\"The predicted label is: \", predicted_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example cases:\n",
    "\n",
    "- dbpeida:\n",
    "    - input titel: TY KU\n",
    "    - input content: TY KU /taɪkuː/ is an American alcoholic beverage company that specializes in sake and other spirits. The privately-held company was founded in 2004 and is headquartered in New York City New York. While based in New York TY KU's beverages are made in Japan through a joint venture with two sake breweries. Since 2011 TY KU's growth has extended its products into all 50 states.\n",
    "    - output class: 0\n",
    "\n",
    "    - input title: Weybridge Man Powered Aircraft\n",
    "    - input content: The Weybridge Man Powered Aircraft (also known as Dumbo and later Mercury) is a British single-seat man-powered aircraft built and flown by members of the Weybridge Man Powered Aircraft Group.\n",
    "    - output class: 5\n",
    "\n",
    "- yelp:\n",
    "    - input: This cafe's espresso drinks are on par with the best artisinal coffee houses anywhere. I always go out of my way to get coffee from here when I'm in Pittsburgh.\n",
    "    - output class: 4\n",
    "\n",
    "    - input: Maybe I am just the most jaded vegan in town; Quiet Storm's food menu bores me to tears, that which I will use to salt my own mediocre cooking at home! Their coffee isn't all that great either, as given the choice, local La Prima is in my favor. Tears. This is the only vegetarian restaurant on the East side of town, and oh so close to me, but I cannot stand to go there. Desperate times now call for toast and tea served by the sad server that resides within my depressed little dining room.\n",
    "    - output class: 0\n",
    "\n",
    "- amazon:\n",
    "    - input title: Hunting the Hard Way\n",
    "    - input content: Thia was a gift for my Husband, who loved the book. It arrived on the date we were told it would.\n",
    "    - output class: 1\n",
    "\n",
    "    - input title: Disappointing\n",
    "    - input content: The text is the same as ever (I think), but the plates are markedly inferior to all previous editions. Had I known this ahead of time, I would not have purchased this volume.\n",
    "    - output class: 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
