{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_review_full (/home/sriteja/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded3ec6ff841493c878fa5a07ace734a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('yelp_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriteja/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Convert train and test datasets to arrays\n",
    "train_data = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "test_data = test_dataset['text']\n",
    "test_labels = test_dataset['label']\n",
    "\n",
    "# Convert labels to lists (optional)\n",
    "# train_labels = train_labels.tolist()\n",
    "# test_labels = test_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_tokens = []\n",
    "with open('yelp_train_tokens_filtered.pkl', 'rb') as file:\n",
    "    train_tokens = pickle.load(file)\n",
    "\n",
    "test_tokens = []\n",
    "with open('yelp_test_tokens_filtered.pkl', 'rb') as file:\n",
    "    test_tokens = pickle.load(file)\n",
    "    \n",
    "print(\"Loaded tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train data replace every 1000th word with UNK randomly\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(len(train_tokens)):\n",
    "    for j in range(len(train_tokens[i])):\n",
    "        if random.randint(1, 1000) == 1:\n",
    "            train_tokens[i][j] = 'UNK'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45287\n"
     ]
    }
   ],
   "source": [
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab = set()\n",
    "for tokens in train_tokens:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    vocab_dict[word] = i\n",
    "\n",
    "print(len(vocab_dict))\n",
    "# Initialize BoW matrices for training and testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27982\n",
      "bow train\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "reduced_train_tokens = train_tokens[:20000]\n",
    "\n",
    "# Build the BoW representation manually\n",
    "# Create a vocabulary by collecting unique words from the training data\n",
    "vocab_reduced = set()\n",
    "for tokens in reduced_train_tokens:\n",
    "    vocab_reduced.update(tokens)\n",
    "\n",
    "# Create a dictionary to map words to indices in the vocabulary\n",
    "vocab_dict_reduced = {}\n",
    "for i, word in enumerate(vocab_reduced):\n",
    "    vocab_dict_reduced[word] = i\n",
    "\n",
    "print(len(vocab_dict_reduced))\n",
    "\n",
    "train_bow = np.zeros((len(reduced_train_tokens), len(vocab_reduced)))\n",
    "\n",
    "print(\"bow train\")\n",
    "# Convert text to BoW vectors\n",
    "for i, tokens in enumerate(reduced_train_tokens):\n",
    "    if i % 5000 == 0:\n",
    "        print(i)\n",
    "    for token in tokens:\n",
    "        train_bow[i][vocab_dict_reduced[token]] += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow test\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "reduced_test_tokens = test_tokens[:5000]\n",
    "test_bow = np.zeros((len(reduced_test_tokens), len(vocab_reduced)))\n",
    "\n",
    "print(\"bow test\")\n",
    "\n",
    "for i, tokens in enumerate(reduced_test_tokens):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    for token in tokens:\n",
    "        if token in vocab_dict_reduced:\n",
    "            test_bow[i][vocab_dict_reduced[token]] += 1\n",
    "        else:    # if there is an unknown word, add it to the UNK column \n",
    "            test_bow[i][vocab_dict_reduced['UNK']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriteja/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      1141\n",
      "           1       0.41      0.42      0.41      1009\n",
      "           2       0.40      0.40      0.40      1003\n",
      "           3       0.43      0.40      0.42       986\n",
      "           4       0.56      0.59      0.57       861\n",
      "\n",
      "    accuracy                           0.50      5000\n",
      "   macro avg       0.49      0.49      0.49      5000\n",
      "weighted avg       0.50      0.50      0.50      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "classifier = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Fit the classifier to the training data with a progress bar\n",
    "# with tqdm(total=len(train_bow[:20000])) as pbar:\n",
    "# classifier.fit(train_bow[:20000], train_labels[:20000])\n",
    "    # pbar.update(len(train_bow[:20000]))\n",
    "\n",
    "# Predict the labels of the test data with a progress bar\n",
    "# with tqdm(total=len(test_bow[:5000])) as pbar:\n",
    "# y_pred = classifier.predict(test_bow[:5000])\n",
    "    # pbar.update(len(test_bow[:5000]))\n",
    "\n",
    "# Calculate accuracy and other classification metrics\n",
    "# print('Accuracy: {}'.format(accuracy_score(test_labels[:5000], y_pred)))\n",
    "# print(classification_report(test_labels[:5000], y_pred))\n",
    "\n",
    "# save the classifier\n",
    "with open('yelp_classifier_log_reg_06_200itr.pkl', 'wb') as file:\n",
    "    pickle.dump(classifier, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_bow[:17000]\n",
    "y_train = train_labels[:17000]\n",
    "X_val = train_bow[17000:20000, :]\n",
    "y_val = train_labels[17000:20000]\n",
    "X_test = test_bow[:5000]\n",
    "y_test = test_labels[:5000]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "y_train = y_train.astype('int')\n",
    "y_val = y_val.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilvertongue1729\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val accuracy',\n",
    "        'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'max_iter': {\n",
    "            'values': [500]\n",
    "        },\n",
    "    },\n",
    "    \"name\": \"bow_yelp_sweep\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff22752d6644ff48617abe5e05725bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112576422217697, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sriteja/College/SMAI/Project/BoW/BoW_yelp/Log Reg 6 handling UNK incr itr/wandb/run-20231130_164826-x57ywkrg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/silvertongue1729/SMAI_project_bow/runs/x57ywkrg' target=\"_blank\">distinctive-resonance-17</a></strong> to <a href='https://wandb.ai/silvertongue1729/SMAI_project_bow' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/silvertongue1729/SMAI_project_bow' target=\"_blank\">https://wandb.ai/silvertongue1729/SMAI_project_bow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/silvertongue1729/SMAI_project_bow/runs/x57ywkrg' target=\"_blank\">https://wandb.ai/silvertongue1729/SMAI_project_bow/runs/x57ywkrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/silvertongue1729/SMAI_project_bow/runs/x57ywkrg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0cc00ea4d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "wandb.init(project=\"SMAI_project_bow\", config={\n",
    "    \"model\": \"BoW\", \"dataset\": \"yelp review\", \"max_iter\": 500\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 27982)) while a minimum of 1 is required by LogisticRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sriteja/College/SMAI/Project/BoW/BoW_yelp/Log Reg 6 handling UNK incr itr/bow_logreg_wandb.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/BoW/BoW_yelp/Log%20Reg%206%20handling%20UNK%20incr%20itr/bow_logreg_wandb.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m classifier\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/BoW/BoW_yelp/Log%20Reg%206%20handling%20UNK%20incr%20itr/bow_logreg_wandb.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/BoW/BoW_yelp/Log%20Reg%206%20handling%20UNK%20incr%20itr/bow_logreg_wandb.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_pred_val \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(X_val)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/BoW/BoW_yelp/Log%20Reg%206%20handling%20UNK%20incr%20itr/bow_logreg_wandb.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_accuracy \u001b[39m=\u001b[39m accuracy_score(y_train, y_pred)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sriteja/College/SMAI/Project/BoW/BoW_yelp/Log%20Reg%206%20handling%20UNK%20incr%20itr/bow_logreg_wandb.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m val_accuracy \u001b[39m=\u001b[39m accuracy_score(y_val, y_pred_val)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 27982)) while a minimum of 1 is required by LogisticRegression."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "classifier = LogisticRegression(max_iter=500)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_train)\n",
    "y_pred_val = classifier.predict(X_val)\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y_pred_val.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Calculate accuracy and other classification metrics\n",
    "print('Val Accuracy: {}'.format(val_accuracy))\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "wandb.log({\"val accuracy\": val_accuracy, \"train accuracy\": train_accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17000, 27982)\n",
      "(3000, 27982)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17000,)\n",
      "(3000,)\n",
      "(17000,)\n",
      "(3000,)\n",
      "Val Accuracy: 0.9606666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       621\n",
      "           1       0.98      0.96      0.97       552\n",
      "           2       0.95      0.95      0.95       602\n",
      "           3       0.95      0.93      0.94       612\n",
      "           4       0.95      0.97      0.96       613\n",
      "\n",
      "    accuracy                           0.96      3000\n",
      "   macro avg       0.96      0.96      0.96      3000\n",
      "weighted avg       0.96      0.96      0.96      3000\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Initialize the Logistic Regression classifier\n",
    "        classifier = LogisticRegression(C=config.C, solver=config.solver, max_iter=config.max_iter)\n",
    "\n",
    "        # Fit the classifier to the training data with a progress bar\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_val)\n",
    "\n",
    "        # Calculate accuracy and other classification metrics\n",
    "        print('Accuracy: {}'.format(accuracy_score(y_val, y_pred)))\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        \n",
    "        wandb.log({\"val accuracy\": accuracy_score(y_val, y_pred), \"train accuracy\": accuracy_score(y_train, classifier.predict(X_train))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.agent(sweep_id, train, count=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
